{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Markov Processes\n",
    "\n",
    "A **Markov Process** is a tuple $ \\langle \\mathcal{S}, \\mathcal{P} \\rangle $ where $\\mathcal{S}$ is a set of **states** called the **observation space** or **state space** that the agent can be in, and $ \\mathcal{P} : \\mathcal{S}^2 \\to \\left[ 0, 1 \\right]$ is a function describing the probability of transitioning from one state to another.\n",
    "$$\n",
    "\\mathcal{P}(s_t, s_{t+1}) = \\mathbb{P} \\left[s_{t+1} \\vert s_t \\right]\n",
    "$$\n",
    "A Markov Processes are used to model stochastic sequences of states $s_0, s_1, \\dots$ satisfying the **Markov property**:\n",
    "$$\n",
    "\\mathbb{P} \\left[ s_{t+1} \\vert s_t \\right] = \\mathbb{P} \\left[ s_{t+1} \\vert s_0, s_1, \\dots, s_t \\right]\n",
    "$$\n",
    "that is, the probability of transitioning from state $s_t$ to state $s_{t+1}$ is independent of previous transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # library for fast array, matrix, and tensor-based operations\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_state_transition_matrix(num_states):\n",
    "    P = np.random.rand(num_states, num_states)\n",
    "    for i in range(num_states): # convert each row to a probability distribution by dividing by the total\n",
    "        P[i] /= sum(P[i])\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 4 # the number of states in our Markov Process. Our current state is just an integer from 0 to 9.\n",
    "\n",
    "P = generate_state_transition_matrix(num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2275677  0.29655611 0.24993822 0.22593797]\n",
      " [0.1766031  0.26924493 0.18241092 0.37174105]\n",
      " [0.36123028 0.14373357 0.29677919 0.19825697]\n",
      " [0.34389291 0.56035414 0.04300507 0.05274788]]\n"
     ]
    }
   ],
   "source": [
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trajectories\n",
    "\n",
    "A **trajectory** (denoted $\\tau$) for a Markov process is a (potentially infinite) sequence of states\n",
    "$$\n",
    "\\tau = \\langle s_0, s_1, \\dots, s_T \\rangle\n",
    "$$\n",
    "The probability of generating particular trajectories depends on the underlying dynamics of the process, which is determined by $\\mathcal{P}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(P, T=100, s_0 = 0):\n",
    "    num_states = len(P) # the number of next states we may go to\n",
    "    s_t = s_0 # the current state is the starting state\n",
    "    tau = np.zeros(T, dtype=np.int32) # the trajectory, a fixed-length array of length T\n",
    "    tau[0] = s_t # the first state is the starting state\n",
    "    \n",
    "    for t in range(1, T): # the length of the rest of the trajectory\n",
    "        s_t = np.random.choice(np.arange(num_states), p=P[s_t]) # randomly select a state using P, where every entry j in P[s_t] is the probability of transitioning from state s_t ot state j\n",
    "        tau[t] = s_t # add this state to our trajectory.\n",
    "        \n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "s_0 = 0\n",
    "tau = generate_trajectory(P, T, s_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 1 3 3 1 2 2 0 2 0 3 1 1 1 3 1 2 0 2 2 2 3 1 1 1 3 0 2 2 0 0 1 1 2 1\n",
      " 3 0 0 0 2 0 1 1 0 0 2 0 0 1 3 0 3 0 3 1 3 1 3 0 1 0 1 0 1 1 0 2 2 0 1 0 2\n",
      " 3 0 2 0 2 0 0 2 0 3 0 2 0 2 3 0 2 2 2 0 3 1 3 1 1 3]\n"
     ]
    }
   ],
   "source": [
    "print(tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Markov Reward Processes\n",
    "\n",
    "A **Markov Reward Process**  is an extension of a Markov Process that allows us to associate rewards with states. Formally, it is a tuple $\\langle \\mathcal{S}, \\mathcal{P}, \\mathcal{R} \\rangle$ that allows us to associate with each state transition $\\langle s_t, s_{t+1} \\rangle$ some reward\n",
    "$$\n",
    "\\mathcal{R}(s_t, s_{t+1}) = \\mathbb{E}\\left[r_t \\vert s_t, s_{t+1} \\right]\n",
    "$$\n",
    "which is often simplified to being $\\mathcal{R}(s_t)$, the reward of being in a particular state $s_t$. For the purpose of this lesson and essentially all implementations, we make this simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reward_matrix(num_states, mu=0, sigma=1):\n",
    "    return mu + np.random.randn(num_states)*sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 4\n",
    "mu = 0\n",
    "sigma = 4\n",
    "P = generate_state_transition_matrix(num_states)\n",
    "R = generate_reward_matrix(num_states, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14467754 0.32149809 0.21209561 0.32172876]\n",
      " [0.24086431 0.25224535 0.17436194 0.3325284 ]\n",
      " [0.38028709 0.25029357 0.35808544 0.01133391]\n",
      " [0.16128364 0.35306392 0.15513487 0.33051757]]\n"
     ]
    }
   ],
   "source": [
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.28086001 -0.83651447  2.41828911 -6.97223292]\n"
     ]
    }
   ],
   "source": [
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(P, R, T=100, s_0 = 0):\n",
    "    num_states = len(P) # the number of next states we may go to\n",
    "    s_t = s_0 # the current state is the starting state\n",
    "    tau = np.zeros(T, dtype=np.int32) # the trajectory, a fixed-length array of length T\n",
    "    tau[0] = s_t # the first state is the starting state\n",
    "    rewards = np.zeros(T) # the rewards experiences, a fixed-length array of length T\n",
    "    rewards[0] = R[s_0] # the first reward is the reward for being in the first state\n",
    "    \n",
    "    for t in range(1, T): # the length of the rest of the trajectory\n",
    "        s_t = np.random.choice(np.arange(num_states), p=P[s_t]) # randomly select a state using P, where every entry j in P[s_t] is the probability of transitioning from state s_t ot state j\n",
    "        tau[t] = s_t # add this state to our trajectory.\n",
    "        rewards[t] = R[s_t]\n",
    "        \n",
    "    return tau, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau, rewards = generate_trajectory(P, R, 100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 2 2 1 3 1 1 3 3 3 0 3 3 3 0 3 1 2 0 3 3 2 1 0 3 1 3 3 3 3 0 1 3 1 2\n",
      " 0 1 0 3 1 1 3 3 0 1 2 1 0 3 2 1 2 2 0 1 0 1 3 3 1 0 1 0 1 1 3 1 0 0 0 3 1\n",
      " 2 2 3 1 2 0 0 3 1 1 2 2 1 3 1 3 1 3 3 1 3 3 1 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.28086001  2.41828911 -0.83651447  2.41828911  2.41828911 -0.83651447\n",
      " -6.97223292 -0.83651447 -0.83651447 -6.97223292 -6.97223292 -6.97223292\n",
      " -4.28086001 -6.97223292 -6.97223292 -6.97223292 -4.28086001 -6.97223292\n",
      " -0.83651447  2.41828911 -4.28086001 -6.97223292 -6.97223292  2.41828911\n",
      " -0.83651447 -4.28086001 -6.97223292 -0.83651447 -6.97223292 -6.97223292\n",
      " -6.97223292 -6.97223292 -4.28086001 -0.83651447 -6.97223292 -0.83651447\n",
      "  2.41828911 -4.28086001 -0.83651447 -4.28086001 -6.97223292 -0.83651447\n",
      " -0.83651447 -6.97223292 -6.97223292 -4.28086001 -0.83651447  2.41828911\n",
      " -0.83651447 -4.28086001 -6.97223292  2.41828911 -0.83651447  2.41828911\n",
      "  2.41828911 -4.28086001 -0.83651447 -4.28086001 -0.83651447 -6.97223292\n",
      " -6.97223292 -0.83651447 -4.28086001 -0.83651447 -4.28086001 -0.83651447\n",
      " -0.83651447 -6.97223292 -0.83651447 -4.28086001 -4.28086001 -4.28086001\n",
      " -6.97223292 -0.83651447  2.41828911  2.41828911 -6.97223292 -0.83651447\n",
      "  2.41828911 -4.28086001 -4.28086001 -6.97223292 -0.83651447 -0.83651447\n",
      "  2.41828911  2.41828911 -0.83651447 -6.97223292 -0.83651447 -6.97223292\n",
      " -0.83651447 -6.97223292 -6.97223292 -0.83651447 -6.97223292 -6.97223292\n",
      " -0.83651447 -4.28086001  2.41828911 -4.28086001]\n"
     ]
    }
   ],
   "source": [
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Return and Discounted Return\n",
    "\n",
    "We are interested in trajectories that maximize the **return** $R_t$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_t &= r_t + r_{t+1} +  r_{t+2}+  \\dots + r_T  \\\\\n",
    "    &= \\sum_{k=t}^T r_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When $T$ is finite, we say that the trajectory has a **finite time horizon** and that the environment is **episodic** (happens in episodes).\n",
    "\n",
    "For infinite time horizons, we cannot guarantee that $R_t$ converges. As a result, we might consider discounting rewards exponentially over time in order to guarantee convergence. This line of reasoning leads us to the **discounted return** $G_t$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    G_t &= r_{t} + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots \\\\\n",
    "    &= \\sum_{k=t}^\\infty \\gamma^{k-t} r_k \n",
    "\\end{align}\n",
    "$$\n",
    "where $\\gamma$ is a discount factor between $0$ and $1$ (often close to $1$).\n",
    "\n",
    "We sometimes refer to both the discounted and undiscounted return as just \"return\" for brevity, and write $G_t$ where for some episodic environments it may be more appropriate to use $R_t$. In fact, it should not be hard to see that $R_t$ is just $G_t$ with $r_t = 0$ for $t > T$ and $\\gamma = 1$.\n",
    "\n",
    "***\n",
    "#### Value Function\n",
    "\n",
    "We can use the expected value of $G_t$ to determine the **value** of being a certain state $s_t$:\n",
    "\n",
    "$$\n",
    "V(s_t) = \\mathbb{E}\\left[ G_t \\big\\vert s_t \\right]\n",
    "$$\n",
    "\n",
    "We can decompose $V(s_t)$ into two parts: the immediate reward $r_t$ and the discounted value of being in the next state $s_{t+1}$:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    \\begin{split}\n",
    "        V(s_t) &= \\mathbb{E} \\left[ G_t \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E} \\left[ r_{t} + \\gamma r_{t+1} + \\dots + \\gamma^2 r_{t+2} \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E}\\left[r_{t} + \\gamma (r_{t+1} + \\gamma r_{t+2} + \\dots) \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E}\\left[ r_{t} + \\gamma G_{t+1} \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E} \\left[ r_{t} + \\gamma V(s_{t+1}) \\big\\vert s_t \\right] \\\\\n",
    "    \\end{split}\n",
    "\\end{align}\n",
    "\n",
    "This last form of $V(s_t)$ is known as the **Bellman Equation**.\n",
    "\n",
    "***\n",
    "#### TD(0)\n",
    "\n",
    "Say we are interesting in learning to predict $V(s_t)$. We can use a simple method called **TD(0)** to approximate $V(s_t)$ when our observation space $\\mathcal{S}$ is **discrete** (i.e., finite). We begin by initializing a vector $V$ with random values, which will serve as our initial predictions for what $V(s_t)$ is.\n",
    "\n",
    "According to the Bellman Equation, we can use $r_t + \\gamma V(s_{t+1})$ (which we call the **TD-target**) as an unbiased estimator for $V(s_t)$. Then we can modify our estimate of $V(s_t)$ to more closely match $r_t + \\gamma V(s_{t+1})$ according to a learning rate $\\alpha$ as follows:\n",
    "$$\n",
    "V(s_t) \\gets V(s_t) + \\alpha \\left( r_t + V(s_{t+1}) - V(s_t) \\right)\n",
    "$$\n",
    "\n",
    "Let's use the TD(0) algorithm to attempt to learn the value function of this Markov reward process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_value_predictions(num_states):\n",
    "    return np.random.rand(num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_0(V, P, R, T=100, s_0=0, gamma=0.99, alpha=0.1):\n",
    "    num_states = len(P) # the number of next states we may go to\n",
    "    s_t = s_0 # the current state is the starting state\n",
    "    r_t = R[s_t]\n",
    "    \n",
    "    for t in range(1, T): # the length of the rest of the trajectory\n",
    "        s_t_next = np.random.choice(np.arange(num_states), p=P[s_t]) # randomly select a state using P, where every entry j in P[s_t] is the probability of transitioning from state s_t ot state j\n",
    "#         print('current state: {}\\t next state: {}'.format(s_t, s_t_next))\n",
    "#         print('current V: {} \\t better V: {}'.format( V[s_t], (r_t + gamma*V[s_t_next])))\n",
    "        V[s_t] = V[s_t] + alpha*(r_t + gamma*V[s_t_next] - V[s_t]) # update our value function according to the bellman equation\n",
    "        s_t = s_t_next # update our reference to s_t to point to the new state\n",
    "        r_t = R[s_t] # update our reference to r_t to point to the new reward\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = generate_value_predictions(num_states)\n",
    "V = TD_0(V, P, R, 10000, 0, 0.99, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-235.32581674 -232.18687133 -227.29607587 -239.75810401]\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Markov Decision Processes\n",
    "A **Markov Decision Process** (MDP) is an extension of a Markov Reward Process that allows state transitions to be conditional upon some action. Formally, it is a tuple $\\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R} \\rangle $ where $\\mathcal{A}$ is a set of actions available to an agent in a state $s_t$. Our reward conditional now upon both the state $s_t$ we are in and the action $a_t$ that we took:\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(s_t, a_t) = \\mathbb{E} \\left[ r_t \\vert s_t, a_t \\right]\n",
    "$$\n",
    "\n",
    "Also, $\\mathcal{P}$ is the probability of transitioning to state $s_{t+1}$ given that the current state is $s_t$ and the current action is $a_t$:\n",
    "\n",
    "$$\n",
    "\\mathcal{P}(s_t, a_t, s_{t+1}) = \\mathbb{P} \\left[ s_{t+1} \\vert s_t, a_t \\right]\n",
    "$$\n",
    "\n",
    "Whereas in an MRP the probability of generating trajectories is dependant upon only the dynamics of the underlying Markov process, in an MDP trajectories also depend on the actions of an agent.\n",
    "\n",
    "This is an MDP with four states and two actions.\n",
    "\n",
    "![MDP](../images/mdp.png)\n",
    "\n",
    "In this case, $\\mathcal{P}$ is a **tensor** of shape $4 \\times 2 \\times 4$ where $\\mathcal{P}(i, j, k)$ is the probability of transitioning from state $i$ to state $k$ given action $j$. Furthermore, $\\mathcal{R}$ is a matrix of shape $4 \\times 2$ where $\\mathcal{R}(i, j)$ is the reward associated with choosing action $j$ in state $i$.\n",
    "\n",
    "***\n",
    "### Policies\n",
    "\n",
    "Our goal is to define a **policy** $\\pi$, which can be thought of as a set of rules for choosing actions based on the state. Typically, we represent $\\pi$ as a probability distribution:\n",
    "$$\n",
    "\\pi: \\mathcal{S} \\times \\mathcal{A} \\to \\left[ 0, 1 \\right]\n",
    "$$\n",
    "\n",
    "where we sample $a_t$ from the distribution\n",
    "$$\n",
    "a_t \\sim \\pi(\\cdot \\vert s_t)\n",
    "$$\n",
    "For discrete action spaces and observation spaces, we can think of this as a table of size $\\lvert \\mathcal{S} \\rvert \\times \\lvert \\mathcal{A} \\rvert$ where $\\pi_{i,j}$ is the probability of choosing action $j$ in state $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_state_transition_matrix(num_states, num_actions):\n",
    "    P = np.random.rand(num_states, num_actions, num_states)\n",
    "    for i in range(num_states):\n",
    "        for j in range(num_actions):\n",
    "            P[i, j] /= sum(P[i, j]) # convert to a probability distribution\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reward_matrix(num_states, num_actions, mu=0, sigma=1):\n",
    "    return mu + np.random.randn(num_states, num_actions)*sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_policy(num_states, num_actions):\n",
    "    pi = np.random.rand(num_states, num_actions)\n",
    "    for i in range(num_states):\n",
    "        pi[i] /= sum(pi[i])\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 4 # the number of states in our Markov Decision Process. Our current state is just an integer from 0 to 9.\n",
    "num_actions = 2 \n",
    "\n",
    "P = generate_state_transition_matrix(num_states, num_actions)\n",
    "R = generate_reward_matrix(num_states, num_actions)\n",
    "pi = generate_policy(num_states, num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[3.35773294e-01 3.22717622e-01 2.40582527e-01 1.00926557e-01]\n",
      "  [5.35278470e-04 2.34064783e-01 2.86547645e-01 4.78852294e-01]]\n",
      "\n",
      " [[9.50065093e-02 6.34878311e-02 3.04728701e-01 5.36776958e-01]\n",
      "  [1.26654156e-01 2.46686263e-01 3.31258149e-01 2.95401432e-01]]\n",
      "\n",
      " [[1.34688542e-01 5.91374847e-01 5.03679509e-02 2.23568660e-01]\n",
      "  [3.63783739e-01 8.21737957e-02 2.63766129e-01 2.90276336e-01]]\n",
      "\n",
      " [[7.64379875e-02 4.46430032e-01 2.85486980e-02 4.48583282e-01]\n",
      "  [5.23841513e-02 4.14688608e-01 1.69347608e-01 3.63579633e-01]]]\n"
     ]
    }
   ],
   "source": [
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.4901262  -0.67593776]\n",
      " [ 0.08486087  1.17553087]\n",
      " [-1.63591592 -1.51936463]\n",
      " [-0.53401975  1.53747598]]\n"
     ]
    }
   ],
   "source": [
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14774924 0.85225076]\n",
      " [0.57339332 0.42660668]\n",
      " [0.50872034 0.49127966]\n",
      " [0.2952526  0.7047474 ]]\n"
     ]
    }
   ],
   "source": [
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(P, R, pi, T=100, s_0 = 0, a_0 = 0):\n",
    "    num_states = len(P) # the number of next states we may go to\n",
    "    num_actions = len(P[0]) # the number of actions we may take\n",
    "    \n",
    "    s_t = s_0 # the current state is the starting state\n",
    "    tau = np.zeros(T, dtype=np.int32) # the trajectory, a fixed-length array of length T\n",
    "    tau[0] = s_t # the first state is the starting state\n",
    "    \n",
    "    a_t = np.random.choice(np.arange(num_actions), p=pi[s_t]) # select our first action\n",
    "    actions = np.zeros(T, dtype=np.int32) # the actions we choose, a fixed-length array of length T\n",
    "    actions[0] = a_t \n",
    "    \n",
    "    rewards = np.zeros(T) # the rewards experiences, a fixed-length array of length T\n",
    "    rewards[0] = R[s_t, a_t] # the first reward is the reward for being in the first state and choosing the first action\n",
    "    \n",
    "    for t in range(1, T): # the length of the rest of the trajectory\n",
    "        s_t = np.random.choice(np.arange(num_states), p=P[s_t, a_t]) # randomly select a state using P, where every entry j in P[s_t] is the probability of transitioning from state s_t ot state j\n",
    "        a_t = np.random.choice(np.arange(num_actions), p=pi[s_t]) # randomly select an action given the state.\n",
    "        tau[t] = s_t # add this state to our trajectory.\n",
    "        actions[t] = a_t\n",
    "        rewards[t] = R[s_t, a_t]\n",
    "\n",
    "    return tau, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau, rewards = generate_trajectory(P, R, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 1 0 1 3 3 3 0 1 2 1 3 2 0 2 1 2 3 1 0 2 3 1 3 1 3 1 0 3 3 1 3 3 0\n",
      " 3 2 1 2 0 1 2 1 3 3 1 3 0 2 0 2 1 2 1 2 3 2 1 3 3 1 1 0 3 3 2 2 0 0 2 2 1\n",
      " 2 2 0 1 2 2 0 3 2 0 3 1 1 1 3 3 1 2 3 2 2 3 1 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.67593776  0.08486087 -1.63591592  1.53747598  0.08486087 -0.67593776\n",
      "  0.08486087  1.53747598  1.53747598 -0.53401975 -0.67593776  1.17553087\n",
      " -1.51936463  0.08486087  1.53747598 -1.51936463 -0.67593776 -1.63591592\n",
      "  1.17553087 -1.51936463  1.53747598  1.17553087 -0.67593776 -1.51936463\n",
      "  1.53747598  0.08486087 -0.53401975  0.08486087 -0.53401975  0.08486087\n",
      " -0.67593776  1.53747598  1.53747598  0.08486087 -0.53401975 -0.53401975\n",
      " -0.67593776 -0.53401975 -1.63591592  0.08486087 -1.51936463  1.4901262\n",
      "  1.17553087 -1.63591592  1.17553087  1.53747598  1.53747598  0.08486087\n",
      " -0.53401975 -0.67593776 -1.51936463 -0.67593776 -1.63591592  0.08486087\n",
      " -1.63591592  0.08486087 -1.51936463  1.53747598 -1.63591592  0.08486087\n",
      "  1.53747598 -0.53401975  1.17553087  0.08486087 -0.67593776  1.53747598\n",
      "  1.53747598 -1.51936463 -1.51936463  1.4901262  -0.67593776 -1.51936463\n",
      " -1.51936463  1.17553087 -1.51936463 -1.63591592  1.4901262   0.08486087\n",
      " -1.51936463 -1.51936463 -0.67593776  1.53747598 -1.51936463  1.4901262\n",
      "  1.53747598  1.17553087  0.08486087  1.17553087  1.53747598  1.53747598\n",
      "  0.08486087 -1.63591592  1.53747598 -1.51936463 -1.63591592 -0.53401975\n",
      "  0.08486087 -1.51936463 -1.51936463  1.53747598]\n"
     ]
    }
   ],
   "source": [
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we learned about:\n",
    "\n",
    "- Markov processes and how trajectories $\\tau$ depend on the underlying state dynamics  $\\mathcal{P}$. \n",
    "- Markov reward processes and how we can generate a stream of rewards given our trajectory.\n",
    "- Returns, the value function $V(s_t)$ and how we can learn to approximate it using the TD(0) algorithm.\n",
    "- Markov decision processes and how we can use a policy $\\pi$ to guide our trajectories by choosing actions $a_t$ conditioned on the state $s_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
