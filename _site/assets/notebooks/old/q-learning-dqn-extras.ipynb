{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep $Q$ networks (for real)\n",
    "In the previous notebook, we used a neural network to learn to play `FrozenLake` with the same performance as our other tabular approaches (using `numpy` and `tensorflow`). In this notebook, we add additional elements to our agent that improve stability.\n",
    "\n",
    "### Target Network\n",
    "The $Q$-learning update rule involves a maximization over outputs of the same network:\n",
    "$$\n",
    "Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha \\left( r_t + (1-d_t)\\gamma \\max_{a_{t+1}} \\left( Q(s_{t+1}, a_{t+1}) \\right) - Q(s_t, a_t) \\right)\n",
    "$$\n",
    "(Note that we write $Q_\\theta$ to mean the $Q$ function parametrized by $\\theta$).\n",
    "\n",
    "When learning this function using a differentiable function approximator like a neural network, this causes a problem. Generally, this update *increases* the prediction for $Q(s_t, a_t)$. Since the function approximator is differentiable, it also increases predictions for states that are similar to $s_t$. You can imagine this like picking up a fishing net; the area around where you grab the net also gets pulled up around it. Frequently, $s_{t+1}$ is similar to $s_t$ due to a small temporal separation between them! Since we are boostrapping our predictions for $Q(s_t, a_t)$ (that is, we use the output of the network in the prediction target itself), our predictions tend to increase quickly and can diverge from what is reasonable.\n",
    "\n",
    "To combat this, we have two networks: $\\theta$ and $\\theta_\\text{targ}$ that share the same architecture. $\\theta_\\text{targ}$ initially has the same parameters as $\\theta$, but we freeze it so that it stays the same for some period of time. We use $\\theta_\\text{targ}$ in our target $ \\max_{a_{t+1}} \\left( Q(s_{t+1}, a_{t+1}) \\right)$ so that the maximization step doesn't influence $\\theta$, which updates at every time step.\n",
    "\n",
    "We implement this in `tensorflow` by making use of `scope`s (which allow us to describe hierarchies of `tensorflow` variables using a naming scheme similar to directories).\n",
    "\n",
    "### Experience Replay\n",
    "\n",
    "As a neural network, our DQN relies on data to perform well. If we only feed the agent the most recent data (updating it 'online'), then our agent is only good at making predictions about the most recent data. It may become less accurate for older or less common states, a problem known as **catastrophic forgetting**. Thus, we make use of a **buffer** of state transitions $\\langle s_t, a_t, r_t, s_{t+1}, d_t \\rangle$ that store the agent's experiences, and update the network by randomly sampling batches of transitions from the buffer. This improves stability of training.\n",
    "\n",
    "Our implementation will assume that the environments are vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_shape, num_envs, max_size=1000000):\n",
    "        self.max_size = max_size\n",
    "        self.num_envs = num_envs\n",
    "        self.s_t_buf = np.zeros((self.max_size, *state_shape))\n",
    "        self.a_t_buf = np.zeros(self.max_size)\n",
    "        self.r_t_buf = np.zeros(self.max_size)\n",
    "        self.s_t_next_buf =  np.zeros((self.max_size, *state_shape))\n",
    "        self.d_t_buf = np.zeros(self.max_size)\n",
    "        self.pointer = 0\n",
    "        self.filled = 0\n",
    "    \n",
    "    def store(self, s_t, a_t, r_t, s_t_next, d_t):\n",
    "        indices = slice(self.pointer, self.pointer+self.num_envs)\n",
    "        self.s_t_buf[indices] = s_t\n",
    "        self.a_t_buf[indices] = a_t\n",
    "        self.r_t_buf[indices] = r_t\n",
    "        self.s_t_next_buf[indices] = s_t_next\n",
    "        self.d_t_buf[indices] = d_t\n",
    "        self.pointer = (self.pointer + num_envs) % self.max_size\n",
    "        self.filled = min(self.max_size, self.filled+self.num_envs)\n",
    "        \n",
    "    def get(self, number=32):\n",
    "        number = min(number, self.filled)\n",
    "        indices = np.random.choice(np.arange(self.filled), number, replace=False)\n",
    "        return self.s_t_buf[indices], self.a_t_buf[indices], self.r_t_buf[indices], self.s_t_next_buf[indices], self.d_t_buf[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, hidden_units):\n",
    "        self.layers = [tf.keras.layers.Dense(units, \n",
    "                                    activation='relu', \n",
    "                                    kernel_initializer='he_uniform', \n",
    "                                    use_bias=False) for units in hidden_units[:-1]]\n",
    "        self.layers.append(tf.keras.layers.Dense(hidden_units[-1],\n",
    "                                                activation='linear',\n",
    "                                                kernel_initializer='zeros',\n",
    "                                                use_bias=False))\n",
    "    \n",
    "    def call(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return [layer.get_weights() for layer in self.layers]\n",
    "    \n",
    "    def set_weights(self, weights_list):\n",
    "        for layer, weights in zip(self.layers, weights_list):\n",
    "            layer.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_shape, num_actions, num_envs,\n",
    "                 epsilon_i=1.0, \n",
    "                 epsilon_f=0.0, \n",
    "                 n_epsilon=0.1, \n",
    "                 alpha=0.5, \n",
    "                 gamma = 0.95,\n",
    "                 hidden_units = []\n",
    "                ):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.epsilon_i = epsilon_i\n",
    "        self.epsilon_f = epsilon_f\n",
    "        self.epsilon = tf.get_variable(\"epsilon\", initializer=tf.constant(self.epsilon_i))\n",
    "        self.n_epsilon = n_epsilon\n",
    "        self.state_shape = state_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.num_envs = num_envs\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.s_t_ph = tf.placeholder(shape=(None, *state_shape), dtype=tf.float32, name=\"state\")\n",
    "        self.a_t_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name=\"action\")\n",
    "        self.r_t_ph = tf.placeholder(shape=(None, ), dtype=tf.float32, name=\"reward\")\n",
    "        self.s_t_next_ph = tf.placeholder(shape=(None, *state_shape), dtype=tf.float32, name=\"next_state\")\n",
    "        self.d_t_ph = tf.placeholder(shape=(None, ), dtype=tf.float32, name=\"done\")\n",
    "        self.n_ph = tf.placeholder(shape=(), dtype=tf.float32, name=\"n\")\n",
    "        \n",
    "        self.Q = Network(hidden_units+[self.num_actions])\n",
    "        self.Q_targ = Network(hidden_units+[self.num_actions])\n",
    "\n",
    "        self.decay_epsilon = self.decay_epsilon_tf(self.n_ph)\n",
    "        self.act = self.act_tf(self.s_t_ph)\n",
    "        self.update = self.update_tf(self.s_t_ph, self.a_t_ph, self.r_t_ph, self.s_t_next_ph, self.d_t_ph)\n",
    "        self.sync_params = self.sync_params_tf()\n",
    "        \n",
    "        self.test1 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Q')\n",
    "        self.test2 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Q_targ')\n",
    "        \n",
    "    def sync_params_tf(self):\n",
    "        self.Q_targ.set_weights(self.Q.get_weights())\n",
    "        \n",
    "    def decay_epsilon_tf(self, n):\n",
    "        return tf.assign(self.epsilon, tf.maximum(\n",
    "            self.epsilon_f, \n",
    "            self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f)))\n",
    "    \n",
    "    def act_tf(self, s_t):\n",
    "        return tf.where(tf.random_uniform(shape=(self.num_envs, ), minval=0, maxval=1, dtype=tf.float32) < self.epsilon,\n",
    "                      tf.random_uniform(shape=(self.num_envs, ), minval=0, maxval=self.num_actions, dtype=tf.int32) ,\n",
    "                      tf.argmax(self.Q.call(s_t), output_type=tf.int32, axis=1))\n",
    "    \n",
    "    def update_tf(self, s_t, a_t, r_t, s_t_next, d_t):\n",
    "        Q_next = tf.reduce_max(self.Q.call(s_t_next), axis=1)\n",
    "#         Q_next = tf.reduce_max(self.Q_targ.call(s_t_next), axis=1)\n",
    "        TD = r_t + (1-d_t)*self.gamma*Q_next\n",
    "        Q_pred = tf.reduce_sum(self.Q.call(s_t) * tf.one_hot(a_t, self.num_actions), axis=1)\n",
    "        loss = tf.reduce_mean(0.5*(Q_pred - tf.stop_gradient(TD))**2)\n",
    "        return tf.train.GradientDescentOptimizer(self.alpha).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, T=100000, sync_every=1):\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    t1, t2 = sess.run([agent.test1, agent.test2])\n",
    "    \n",
    "    print(t1)\n",
    "    print(t2)\n",
    "    \n",
    "    sess.run(agent.sync_params)\n",
    "    \n",
    "    t_11, t_12 = sess.run([agent.test1, agent.test2])\n",
    "    print(t_11)\n",
    "    print(t_12)\n",
    "    \n",
    "    print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n",
    "    \n",
    "    buffer = ReplayBuffer(env.observation_space.shape, env.num_envs)\n",
    "    \n",
    "    rewards = []\n",
    "    episode_rewards = 0\n",
    "    \n",
    "    s_t = env.reset()\n",
    "    \n",
    "    for t in range(T):\n",
    "        if t%1000 == 0:\n",
    "            print(f'{100*t/T}%', end='\\r')\n",
    "        if t%sync_every == 0:\n",
    "            sess.run(agent.sync_params)\n",
    "            \n",
    "        a_t = sess.run(agent.act, \n",
    "                       feed_dict={\n",
    "                           agent.s_t_ph:s_t\n",
    "                       }\n",
    "                      )   \n",
    "\n",
    "        s_t_next, r_t, d_t, info = env.step(a_t)\n",
    "        \n",
    "        buffer.store(s_t, a_t, r_t, s_t_next, d_t)\n",
    "        \n",
    "        s_t_train, a_t_train, r_t_train, s_t_next_train, d_t_train = buffer.get()\n",
    "        \n",
    "        sess.run([agent.update, agent.decay_epsilon], \n",
    "                 feed_dict={\n",
    "                     agent.s_t_ph:s_t_train,\n",
    "                     agent.a_t_ph:a_t_train,\n",
    "                     agent.r_t_ph:r_t_train,\n",
    "                     agent.s_t_next_ph:s_t_next_train,\n",
    "                     agent.d_t_ph:d_t_train,\n",
    "                     agent.n_ph:t/T,\n",
    "                 }\n",
    "                )\n",
    "        \n",
    "        s_t = s_t_next\n",
    "        episode_rewards += r_t\n",
    "        \n",
    "        for env_index in range(env.num_envs):\n",
    "            if d_t[env_index]:\n",
    "                rewards.append(episode_rewards[env_index])\n",
    "                episode_rewards[env_index] = 0\n",
    "                s_t[env_index] = env.reset_at(env_index)\n",
    "            \n",
    "    sess.close()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, window=100):\n",
    "    sns.lineplot(\n",
    "        data=data.rolling(window=window).mean()[window-1::window]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteToBoxWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete), \\\n",
    "            \"Should only be used to wrap Discrete envs.\"\n",
    "        self.n = self.observation_space.n\n",
    "        self.observation_space = gym.spaces.Box(0, 1, (self.n,))\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        new_obs = np.zeros(self.n)\n",
    "        new_obs[obs] = 1\n",
    "        return new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, make_env, num_envs=1):\n",
    "        super().__init__(make_env())\n",
    "        self.num_envs = num_envs\n",
    "        self.envs = [make_env() for env_index in range(num_envs)]\n",
    "    \n",
    "    def reset(self):\n",
    "        return np.asarray([env.reset() for env in self.envs])\n",
    "    \n",
    "    def reset_at(self, env_index):\n",
    "        return self.envs[env_index].reset()\n",
    "    \n",
    "    def step(self, actions):\n",
    "        next_states, rewards, dones, infos = [], [], [], []\n",
    "        for env, action in zip(self.envs, actions):\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            infos.append(info)\n",
    "        return np.asarray(next_states), np.asarray(rewards), \\\n",
    "            np.asarray(dones), np.asarray(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/rl-lessons/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/envs/rl-lessons/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"dense_1\" with a  weight list of length 1, but the layer was expecting 0 weights. Provided weights: [array([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fcd2f4f6db68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstate_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-943334c5aa81>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_shape, num_actions, num_envs, epsilon_i, epsilon_f, n_epsilon, alpha, gamma, hidden_units)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_t_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_t_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_t_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_t_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_t_next_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_t_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_params_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_VARIABLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-943334c5aa81>\u001b[0m in \u001b[0;36msync_params_tf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msync_params_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_targ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecay_epsilon_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-51ac35af9a72>\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights_list)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/rl-lessons/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    823\u001b[0m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', but the layer was expecting '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' weights. Provided weights: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                        str(weights)[:50] + '...')\n\u001b[0m\u001b[1;32m    826\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"dense_1\" with a  weight list of length 1, but the layer was expecting 0 weights. Provided weights: [array([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],..."
     ]
    }
   ],
   "source": [
    "num_envs = 10\n",
    "make_env = lambda: DiscreteToBoxWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "env = VectorizedEnvWrapper(make_env, num_envs=num_envs)\n",
    "state_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "agent = Agent(state_shape, num_actions, num_envs, alpha=0.8, gamma=0.95, epsilon_i=1.0, epsilon_f=0.0)\n",
    "\n",
    "rewards = train(env, agent, T=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(pd.DataFrame(rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
