<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" type="text/css" media="screen" href="/assets/css/style.css?v=e88ea2ad56e95913e172df6f31c1ff82889ac891">
    <!-- Mathjax Support -->
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Bandit Environments | alexandervandekleut.github.io</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Bandit Environments" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/assets/notebooks/old/gym-bandits/" />
<meta property="og:url" content="http://localhost:4000/assets/notebooks/old/gym-bandits/" />
<meta property="og:site_name" content="alexandervandekleut.github.io" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/assets/notebooks/old/gym-bandits/","headline":"Bandit Environments","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <h1 id="project_title"> A Complete Guide to Reinforcement Learning with Tensorflow </h1>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        
        <h4>
          <a href="http://localhost:4000">Home</a>
        </h4>
        
        
        <h1 id="bandit-environments">Bandit Environments</h1>

<p>Series of n-armed bandit environments for the OpenAI Gym</p>

<p>Each env uses a different set of:</p>
<ul>
  <li>Probability Distributions - A list of probabilities of the likelihood that a particular bandit will pay out</li>
  <li>Reward Distributions - A list of either rewards (if number) or means and standard deviations (if list) of the payout that bandit has</li>
</ul>

<p>E.g. BanditTwoArmedHighLowFixed-v0 has <code class="highlighter-rouge">p_dist=[0.8, 0.2]</code>, <code class="highlighter-rouge">r_dist=[1, 1]</code>, meaning 80% of the time that action 0 is
selected it will payout 1, and 20% of the time action 2 is selected it will payout 1</p>

<p>You can access the distributions through the p_dist and r_dist variables using <code class="highlighter-rouge">env.p_dist</code> or <code class="highlighter-rouge">env.r_dist</code> if you want to match
your weights against the true values for plotting results of various algorithms</p>

<h3 id="environments">Environments</h3>
<ul>
  <li><code class="highlighter-rouge">BanditTwoArmedDeterministicFixed-v0</code>: Simplest case where one bandit always pays, and the other always doesnâ€™t</li>
  <li><code class="highlighter-rouge">BanditTwoArmedHighLowFixed-v0</code>: Stochastic version with a large difference between which bandit pays out of two choices</li>
  <li><code class="highlighter-rouge">BanditTwoArmedHighHighFixed-v0</code>: Stochastic version with a small difference between which bandit pays where both are good</li>
  <li><code class="highlighter-rouge">BanditTwoArmedLowLowFixed-v0</code>: Stochastic version with a small difference between which bandit pays where both are bad</li>
  <li><code class="highlighter-rouge">BanditTenArmedRandomFixed-v0</code>: 10 armed bandit with random probabilities assigned to payouts</li>
  <li><code class="highlighter-rouge">BanditTenArmedRandomRandom-v0</code>: 10 armed bandit with random probabilities assigned to both payouts and rewards</li>
  <li><code class="highlighter-rouge">BanditTenArmedUniformDistributedReward-v0</code>: 10 armed bandit with that always pays out with a reward selected from a uniform distribution</li>
  <li><code class="highlighter-rouge">BanditTenArmedGaussian-v0</code>: 10 armed bandit mentioned on page 30 of <a href="https://www.dropbox.com/s/b3psxv2r0ccmf80/book2015oct.pdf?dl=0">Reinforcement Learning: An Introduction</a> (Sutton and Barto)</li>
</ul>

<h3 id="installation">Installation</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:JKCooper2/gym-bandits.git
<span class="nb">cd </span>gym-bandits
pip install <span class="nb">.</span>
</code></pre></div></div>

<p>To install using <code class="highlighter-rouge">requirements.txt</code> or <code class="highlighter-rouge">environment.yml</code> call:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git+https://github.com/JKCooper2/gym-bandits#egg=gym-bandits
</code></pre></div></div>

<p>In your gym environment</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym_bandits</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">"BanditTenArmedGaussian-v0"</span><span class="p">)</span> <span class="c"># Replace with relevant env</span>
</code></pre></div></div>

        
        <h4>
          <a href="http://localhost:4000">Home</a>
        </h4>
        
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">alexandervandekleut.github.io maintained by <a href="https://github.com/alexandervandekleut">alexandervandekleut</a></p>
        
      </footer>
    </div>

    
  </body>
</html>
