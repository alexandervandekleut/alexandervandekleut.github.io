{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Introduction to the OpenAI Gym Interface\n",
    "\n",
    "OpenAI has been developing the `gym` library to help reinforcement learning researchers get started with pre-implemented environments. In the lesson on Markov decision processes, we explicitly implemented $\\mathcal{S}, \\mathcal{A}, \\mathcal{P}$ and $\\mathcal{R}$ using matrices and tensors in `numpy`.\n",
    "\n",
    "Recall the **environment** and **agent** that we discussed in the introduction. When we specified an MDP and a policy, we were abstractly representing the environment and the agent. `gym` gives us access to predefined environments that are more meaningful than our essentially random environments.\n",
    "\n",
    "Thus far we have been using **discrete** observation spaces (i.e., our state is representable by an integer). In keeping with this, we will start by considering a `gym` environment that also has a discrete observation space: `FrozenLake-v0`.\n",
    "\n",
    "From [the documentation](https://gym.openai.com/envs/FrozenLake-v0/):\n",
    "> The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "The grid is $4 \\times 4$, giving us a total of $16$ states.\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect information about `gym` environments. Every environment has an `observation_space` (corresponding to $\\mathcal{S}$) and an `action_space` (corresponding to $\\mathcal{A}$). There are many categories of $spaces$ available, but the two that are most common and most important are:\n",
    "\n",
    "1. `Discrete`: When observation spaces or action spaces are discrete, they expect integers. This is what we used in our notebook on Markov decision processes.\n",
    "    - Every `Discrete` space has an attribute `n` corresponding to the number of discrete elements in the space (i.e., number of states or number of actions).\n",
    "2. `Box`: Some observations or action spaces instead take on real values, and can be of various shapes.\n",
    "    - Every `Box` space has an attribute `shape` corresponding to the dimensions of the space. For example, an observation space with shape $(84, 84, 3)$ might correspond to an $84 \\times 84 \\times 3$ RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: \n",
      "Discrete(16) \n",
      "action space: \n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(f'observation space: \\n{env.observation_space} \\naction space: \\n{env.action_space}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most environments have a finite time limit corresponding to the time horizon $T$. An environment can become **terminal** if we exceed this time horizon, or if something else happens that causes the environment to end. In the context of an MDP, this is a state where any action we take returns us to the same state. This often happens when our agent \"dies\".\n",
    "\n",
    "Environments, when created via `gym.make`, start out terminal. Whenever an environment is terminal, we need to `reset` it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this returned something. Resetting the environment is used to provide the initial state $s_0$ without requiring some initial action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate the next state and rewards, we call the `env`'s `step` function. The `step` function accepts an action, and returns four things:\n",
    "\n",
    "1. The next state\n",
    "2. The reward for the transition\n",
    "3. A boolean indicating whether or not the environment is terminal\n",
    "4. Additional info that may be relevant for logging but is not intended to be used by the agent.\n",
    "\n",
    "Since we do not have a way of generating actions yet, we can call the `sample` method of the `env`'s `action_space`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: \n",
      "0 \n",
      "reward: \n",
      "0.0 \n",
      "done: \n",
      "False \n",
      "info: \n",
      "{'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "observation_next, reward, done, info = env.step(env.action_space.sample())\n",
    "print(f'observation: \\n{observation} \\nreward: \\n{reward} \\ndone: \\n{done} \\ninfo: \\n{info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can log trajectories, we can get a better idea on how the environment is changing by calling its `render` function. Unfortunately, routing the output of this function to a jupyter notebook is not trivial, so for most environments it will pop up in a new window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider an agent who's policy is to choose actions uniformly randomly. We can emulate this by using `env.action_space.sample()` as our action.\n",
    "\n",
    "Then the basic agent-environment interaction loop using `gym` looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "observation = env.reset()\n",
    "for t in range(T):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        observation = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using `gym` for its wide range of available environments. As we will see in later lessons, we can also extend the functionality of `gym` environments by using `Wrappers`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
