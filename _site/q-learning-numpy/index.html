<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" type="text/css" media="screen" href="/assets/css/style.css?v=4347d9b846f2cebdc47e657f7c3c9e451d3ad96a">
    <!-- Mathjax Support -->
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Q Learning Numpy | alexandervandekleut.github.io</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Q Learning Numpy" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In&nbsp;[1]: import numpy as np import gym import seaborn as sns import pandas as pd sns.set() Q-learning&#182; Value Function&#182;Recall the definition of the value function $V(s_t)$: $$ V(s_t) = \mathbb{E} \left[ G_t \vert s_t \right] $$In an MDP, our expected discounted return $G_t$ depends on the trajectories $\tau$ that we generate. Whereas in a Markov process or Markov reward process this depends only on the underlying state dynamics, in an MDP, trajectories also depend on the actions that we choose. Since these actions are determined by our policy $\pi$, we write the following: $$ V^\pi (s_t) = \mathbb{E}_\pi \left[ G_t \vert s_t \right] $$that is, given that we are currently in state $s_t$, what value should we expect our discounted return $G_t$ to be, given that we generate trajectories according to our policy $\pi$? Action-Value function&#182;We can define an analogous definition for the action-value function, which extends the notion of the value function to account for choosing a specific action $a_t$ in a state $s_t$, rather than just choosing the one suggested by the policy: $$ Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ G_t \vert s_t, a_t \right] $$that is, given that we are currently in state $s_t$, taking action $a_t$, what value should we expect our discounted return $G_t$ to be? We can see the relationship between $V^\pi (s_t)$ and $Q^\pi (s_t, a_t)$: $$ V^\pi (s_t) = \mathbb{E}_{a_t \sim \pi} \left[ Q^\pi (s_t, a_t) \vert s_t \right] $$where $a_t \sim \pi$ means &#39;over actions $a_t$ selected according to a probability distribution $\pi$&#39;. We also have a Bellman equation for the state-value function: $$ Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ r_t + \gamma Q^\pi (s_{t+1}, a_{t+1}) \vert s_t, a_t \right] $$ $Q$-learning&#182;$Q$-learning is an algorithm analogous to the TD(0) algorithm we&#39;ve described before. In TD(0), we have a table $V$ containing predictions for $V^\pi (s_t)$ for each state $s_t$, updating our predictions as follows: $$ V (s_t) \gets V (s_t) + \alpha \left( r_t + \gamma V (s_{t+1}) - V (s_t) \right) $$In $Q$-learning, we have a similar learning rule, except that our table now contains values for any state-action pair $(s_t, a_t)$. $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right) $$The advantage of $Q$-learning is that we have an estimate, for each action, of the expected discounted return we will get from taking that action. Recall the reward hypothesis: Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal. Consider being in some state $s_t$, and for each action $a_t$ we compute $Q(s_t, a_t)$. These values are our guesses for the discounted return we will get as a result of choosing each action. If we choose the action that maximizes $Q(s_t, a_t)$, then our agent is maximizing the discounted return. This defines a greedy policy $$ \pi(a_t \vert s_t) = \begin{cases} 1, &amp; a_t = \arg \max_{a_t} Q(s_t, a_t) \\ 0, &amp; \text{otherwise} \end{cases} $$Knowing this, we can estimate the $Q$ value for the next state-action pair $(s_{t+1}, a_{t+1})$ by assuming the agent chooses the action that maximizes that $Q$ value. This gives us the following update rule: $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + \gamma \max_{a_{t+1}} \left( Q(s_{t+1}, a_{t+1}) \right) - Q(s_t, a_t) \right) $$ Modified Bellman Equation and Terminal States&#182;Recall that some environments are episodic. This means that there must be some states $s_T$ where, regardless of what action is chosen, the next state is always $s_T$ and the reward is always $0$. The gym library defines an environment api with a step function that returns $s_{t+1}, r_t, d_t,$ info given an action $a_t$. Here, $d_t$ is a boolean flag denoting whether or not $s_{t+1}$ is terminal. By the definition of $Q(s_t, a_t)$, we can see that $r_t + \gamma Q(s_{t+1}, a_{t+1})$ is an unbiased estimate for $Q(s_t, a_t)$. This is what gives us the Bellman equation update rule. If we know that a state is terminal, then we have an even better estimate: if $s_t$ is terminal, then $G_t = r_t$, and our update rule is just $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t - Q(s_t, a_t) \right) $$If we interpret the truthiness of $d_t$ as an integer (i.e., $d_t = 1$ if True and $0$ if False), then we can define a modified update rule that takes terminal states into account: $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q(s_{t+1}, a_{t+1}) \right) - Q(s_t, a_t) \right) $$ Exploration-Exploitation&#182;Our agent can quickly become stuck in a suboptimal policy by always choosing the same action in the same state. Recall that we use the greedy policy to choose actions. If we choose a certain action in a certain state, it&#39;s because that action has the maximal $Q$ value associated with it. If we always follow the greedy policy, we will generally tend to continue to choose this action since we don&#39;t get the opportunity to explore (and thus update our predictions for) the other actions. This problem is known as the exploration-exploitation tradeoff: should we try to learn more about the $Q$ function by exploring (and thus finding a better policy) or should we exploit what we know about the environment to try to maximize the returns that we know about? $\epsilon$-greedy policies&#182;The simplest approach to solving the exploration-exploitation tradeoff is to randomly choose an action $a_t \sim \mathcal{A}$ rather than from the policy $a_t \sim \pi$. We choose a random action with a probability $\epsilon$. At the beginning of training, the agent should not follow its policy, since its estimates for $Q(s_t, a_t)$ are poor. Instead, the agent should focus on gathering as much data as possible to improve its guesses, and should do this by exploring. This means that initially, $\epsilon$ should be high (close to 1). Over time, as our predictions for $Q(s_t, a_t)$ improve, we can trust our prediction and thus our greedy policy more, and thus $\epsilon$ should be low (close to 0). We define $\epsilon_i$ to be the initial value for $\epsilon$ and $\epsilon_f$ to be the final value. We decay $\epsilon$ using either linear or exponential decay. Linear decay means we subtract a constant from $\epsilon$ at every time step, and exponential decay means we multiply $\epsilon$ by some decay factor less than 1. Here we use linear decay. In&nbsp;[2]: class Agent: def __init__(self, num_states, num_actions, epsilon_i=1.0, epsilon_f=0.0, n_epsilon=0.1, alpha=0.5, gamma = 0.95): &quot;&quot;&quot; num_states: the number of states in the discrete state space num_actions: the number of actions in the discrete action space epsilon_i: the initial value for epsilon epsilon_f: the final value for epsilon n_epsilon: a float between 0 and 1 determining at which point during training epsilon should have decayed from epsilon_i to epsilon_f alpha: the learning rate gamma: the decay rate &quot;&quot;&quot; self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.epsilon = epsilon_i self.n_epsilon = n_epsilon self.num_states = num_states self.num_actions = num_actions self.alpha = alpha self.gamma = gamma self.Q = np.zeros((num_states, num_actions)) def decay_epsilon(self, n): &quot;&quot;&quot; Decays the agent&#39;s exploration rate according to n, which is a float between 0 and 1 describing how far along training is, with 0 meaning &#39;just started&#39; and 1 meaning &#39;done&#39;. &quot;&quot;&quot; self.epsilon = max( self.epsilon_f, self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f)) def act(self, s_t): &quot;&quot;&quot; Epsilon-greedy policy. &quot;&quot;&quot; if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions) return np.argmax(self.Q[s_t]) def update(self, s_t, a_t, r_t, s_t_next, d_t): &quot;&quot;&quot; Uses the q-learning update rule to update the agent&#39;s predictions for Q(s_t, a_t). &quot;&quot;&quot; Q_next = np.max(self.Q[s_t_next]) self.Q[s_t, a_t] = self.Q[s_t, a_t] + \ self.alpha*(r_t + (1-d_t)*self.gamma*Q_next - \ self.Q[s_t, a_t]) In&nbsp;[3]: def plot(data, window=100): &quot;&quot;&quot; Given a pandas dataframe &#39;data&#39;, plots a rolling mean of the data using a window size of &#39;window&#39;. &quot;&quot;&quot; sns.lineplot( data=data.rolling(window=window).mean()[window-1::window] ) In&nbsp;[4]: def train(env_name, T=100000, alpha=0.8, gamma=0.95, epsilon_i = 1.0, epsilon_f = 0.0, n_epsilon = 0.1): env = gym.make(env_name) num_states = env.observation_space.n num_actions = env.action_space.n agent = Agent(num_states, num_actions, alpha=alpha, gamma=gamma, epsilon_i=epsilon_i, epsilon_f=epsilon_f, n_epsilon = n_epsilon) rewards = [] episode_rewards = 0 s_t = env.reset() for t in range(T): a_t = agent.act(s_t) s_t_next, r_t, d_t, info = env.step(a_t) agent.update(s_t, a_t, r_t, s_t_next, d_t) agent.decay_epsilon(t/T) s_t = s_t_next episode_rewards += r_t if d_t: rewards.append(episode_rewards) episode_rewards = 0 s_t = env.reset() plot(pd.DataFrame(rewards)) return agent In&nbsp;[7]: train(&quot;FrozenLake-v0&quot;, T=100000) Out[7]: &lt;__main__.Agent at 0x1a18692320&gt; In&nbsp;[&nbsp;]:" />
<meta property="og:description" content="In&nbsp;[1]: import numpy as np import gym import seaborn as sns import pandas as pd sns.set() Q-learning&#182; Value Function&#182;Recall the definition of the value function $V(s_t)$: $$ V(s_t) = \mathbb{E} \left[ G_t \vert s_t \right] $$In an MDP, our expected discounted return $G_t$ depends on the trajectories $\tau$ that we generate. Whereas in a Markov process or Markov reward process this depends only on the underlying state dynamics, in an MDP, trajectories also depend on the actions that we choose. Since these actions are determined by our policy $\pi$, we write the following: $$ V^\pi (s_t) = \mathbb{E}_\pi \left[ G_t \vert s_t \right] $$that is, given that we are currently in state $s_t$, what value should we expect our discounted return $G_t$ to be, given that we generate trajectories according to our policy $\pi$? Action-Value function&#182;We can define an analogous definition for the action-value function, which extends the notion of the value function to account for choosing a specific action $a_t$ in a state $s_t$, rather than just choosing the one suggested by the policy: $$ Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ G_t \vert s_t, a_t \right] $$that is, given that we are currently in state $s_t$, taking action $a_t$, what value should we expect our discounted return $G_t$ to be? We can see the relationship between $V^\pi (s_t)$ and $Q^\pi (s_t, a_t)$: $$ V^\pi (s_t) = \mathbb{E}_{a_t \sim \pi} \left[ Q^\pi (s_t, a_t) \vert s_t \right] $$where $a_t \sim \pi$ means &#39;over actions $a_t$ selected according to a probability distribution $\pi$&#39;. We also have a Bellman equation for the state-value function: $$ Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ r_t + \gamma Q^\pi (s_{t+1}, a_{t+1}) \vert s_t, a_t \right] $$ $Q$-learning&#182;$Q$-learning is an algorithm analogous to the TD(0) algorithm we&#39;ve described before. In TD(0), we have a table $V$ containing predictions for $V^\pi (s_t)$ for each state $s_t$, updating our predictions as follows: $$ V (s_t) \gets V (s_t) + \alpha \left( r_t + \gamma V (s_{t+1}) - V (s_t) \right) $$In $Q$-learning, we have a similar learning rule, except that our table now contains values for any state-action pair $(s_t, a_t)$. $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right) $$The advantage of $Q$-learning is that we have an estimate, for each action, of the expected discounted return we will get from taking that action. Recall the reward hypothesis: Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal. Consider being in some state $s_t$, and for each action $a_t$ we compute $Q(s_t, a_t)$. These values are our guesses for the discounted return we will get as a result of choosing each action. If we choose the action that maximizes $Q(s_t, a_t)$, then our agent is maximizing the discounted return. This defines a greedy policy $$ \pi(a_t \vert s_t) = \begin{cases} 1, &amp; a_t = \arg \max_{a_t} Q(s_t, a_t) \\ 0, &amp; \text{otherwise} \end{cases} $$Knowing this, we can estimate the $Q$ value for the next state-action pair $(s_{t+1}, a_{t+1})$ by assuming the agent chooses the action that maximizes that $Q$ value. This gives us the following update rule: $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + \gamma \max_{a_{t+1}} \left( Q(s_{t+1}, a_{t+1}) \right) - Q(s_t, a_t) \right) $$ Modified Bellman Equation and Terminal States&#182;Recall that some environments are episodic. This means that there must be some states $s_T$ where, regardless of what action is chosen, the next state is always $s_T$ and the reward is always $0$. The gym library defines an environment api with a step function that returns $s_{t+1}, r_t, d_t,$ info given an action $a_t$. Here, $d_t$ is a boolean flag denoting whether or not $s_{t+1}$ is terminal. By the definition of $Q(s_t, a_t)$, we can see that $r_t + \gamma Q(s_{t+1}, a_{t+1})$ is an unbiased estimate for $Q(s_t, a_t)$. This is what gives us the Bellman equation update rule. If we know that a state is terminal, then we have an even better estimate: if $s_t$ is terminal, then $G_t = r_t$, and our update rule is just $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t - Q(s_t, a_t) \right) $$If we interpret the truthiness of $d_t$ as an integer (i.e., $d_t = 1$ if True and $0$ if False), then we can define a modified update rule that takes terminal states into account: $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q(s_{t+1}, a_{t+1}) \right) - Q(s_t, a_t) \right) $$ Exploration-Exploitation&#182;Our agent can quickly become stuck in a suboptimal policy by always choosing the same action in the same state. Recall that we use the greedy policy to choose actions. If we choose a certain action in a certain state, it&#39;s because that action has the maximal $Q$ value associated with it. If we always follow the greedy policy, we will generally tend to continue to choose this action since we don&#39;t get the opportunity to explore (and thus update our predictions for) the other actions. This problem is known as the exploration-exploitation tradeoff: should we try to learn more about the $Q$ function by exploring (and thus finding a better policy) or should we exploit what we know about the environment to try to maximize the returns that we know about? $\epsilon$-greedy policies&#182;The simplest approach to solving the exploration-exploitation tradeoff is to randomly choose an action $a_t \sim \mathcal{A}$ rather than from the policy $a_t \sim \pi$. We choose a random action with a probability $\epsilon$. At the beginning of training, the agent should not follow its policy, since its estimates for $Q(s_t, a_t)$ are poor. Instead, the agent should focus on gathering as much data as possible to improve its guesses, and should do this by exploring. This means that initially, $\epsilon$ should be high (close to 1). Over time, as our predictions for $Q(s_t, a_t)$ improve, we can trust our prediction and thus our greedy policy more, and thus $\epsilon$ should be low (close to 0). We define $\epsilon_i$ to be the initial value for $\epsilon$ and $\epsilon_f$ to be the final value. We decay $\epsilon$ using either linear or exponential decay. Linear decay means we subtract a constant from $\epsilon$ at every time step, and exponential decay means we multiply $\epsilon$ by some decay factor less than 1. Here we use linear decay. In&nbsp;[2]: class Agent: def __init__(self, num_states, num_actions, epsilon_i=1.0, epsilon_f=0.0, n_epsilon=0.1, alpha=0.5, gamma = 0.95): &quot;&quot;&quot; num_states: the number of states in the discrete state space num_actions: the number of actions in the discrete action space epsilon_i: the initial value for epsilon epsilon_f: the final value for epsilon n_epsilon: a float between 0 and 1 determining at which point during training epsilon should have decayed from epsilon_i to epsilon_f alpha: the learning rate gamma: the decay rate &quot;&quot;&quot; self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.epsilon = epsilon_i self.n_epsilon = n_epsilon self.num_states = num_states self.num_actions = num_actions self.alpha = alpha self.gamma = gamma self.Q = np.zeros((num_states, num_actions)) def decay_epsilon(self, n): &quot;&quot;&quot; Decays the agent&#39;s exploration rate according to n, which is a float between 0 and 1 describing how far along training is, with 0 meaning &#39;just started&#39; and 1 meaning &#39;done&#39;. &quot;&quot;&quot; self.epsilon = max( self.epsilon_f, self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f)) def act(self, s_t): &quot;&quot;&quot; Epsilon-greedy policy. &quot;&quot;&quot; if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions) return np.argmax(self.Q[s_t]) def update(self, s_t, a_t, r_t, s_t_next, d_t): &quot;&quot;&quot; Uses the q-learning update rule to update the agent&#39;s predictions for Q(s_t, a_t). &quot;&quot;&quot; Q_next = np.max(self.Q[s_t_next]) self.Q[s_t, a_t] = self.Q[s_t, a_t] + \ self.alpha*(r_t + (1-d_t)*self.gamma*Q_next - \ self.Q[s_t, a_t]) In&nbsp;[3]: def plot(data, window=100): &quot;&quot;&quot; Given a pandas dataframe &#39;data&#39;, plots a rolling mean of the data using a window size of &#39;window&#39;. &quot;&quot;&quot; sns.lineplot( data=data.rolling(window=window).mean()[window-1::window] ) In&nbsp;[4]: def train(env_name, T=100000, alpha=0.8, gamma=0.95, epsilon_i = 1.0, epsilon_f = 0.0, n_epsilon = 0.1): env = gym.make(env_name) num_states = env.observation_space.n num_actions = env.action_space.n agent = Agent(num_states, num_actions, alpha=alpha, gamma=gamma, epsilon_i=epsilon_i, epsilon_f=epsilon_f, n_epsilon = n_epsilon) rewards = [] episode_rewards = 0 s_t = env.reset() for t in range(T): a_t = agent.act(s_t) s_t_next, r_t, d_t, info = env.step(a_t) agent.update(s_t, a_t, r_t, s_t_next, d_t) agent.decay_epsilon(t/T) s_t = s_t_next episode_rewards += r_t if d_t: rewards.append(episode_rewards) episode_rewards = 0 s_t = env.reset() plot(pd.DataFrame(rewards)) return agent In&nbsp;[7]: train(&quot;FrozenLake-v0&quot;, T=100000) Out[7]: &lt;__main__.Agent at 0x1a18692320&gt; In&nbsp;[&nbsp;]:" />
<link rel="canonical" href="http://localhost:4000/q-learning-numpy/" />
<meta property="og:url" content="http://localhost:4000/q-learning-numpy/" />
<meta property="og:site_name" content="alexandervandekleut.github.io" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-12T00:00:00-04:00" />
<script type="application/ld+json">
{"description":"In&nbsp;[1]: import numpy as np import gym import seaborn as sns import pandas as pd sns.set() Q-learning&#182; Value Function&#182;Recall the definition of the value function $V(s_t)$: $$ V(s_t) = \\mathbb{E} \\left[ G_t \\vert s_t \\right] $$In an MDP, our expected discounted return $G_t$ depends on the trajectories $\\tau$ that we generate. Whereas in a Markov process or Markov reward process this depends only on the underlying state dynamics, in an MDP, trajectories also depend on the actions that we choose. Since these actions are determined by our policy $\\pi$, we write the following: $$ V^\\pi (s_t) = \\mathbb{E}_\\pi \\left[ G_t \\vert s_t \\right] $$that is, given that we are currently in state $s_t$, what value should we expect our discounted return $G_t$ to be, given that we generate trajectories according to our policy $\\pi$? Action-Value function&#182;We can define an analogous definition for the action-value function, which extends the notion of the value function to account for choosing a specific action $a_t$ in a state $s_t$, rather than just choosing the one suggested by the policy: $$ Q^\\pi (s_t, a_t) = \\mathbb{E}_\\pi \\left[ G_t \\vert s_t, a_t \\right] $$that is, given that we are currently in state $s_t$, taking action $a_t$, what value should we expect our discounted return $G_t$ to be? We can see the relationship between $V^\\pi (s_t)$ and $Q^\\pi (s_t, a_t)$: $$ V^\\pi (s_t) = \\mathbb{E}_{a_t \\sim \\pi} \\left[ Q^\\pi (s_t, a_t) \\vert s_t \\right] $$where $a_t \\sim \\pi$ means &#39;over actions $a_t$ selected according to a probability distribution $\\pi$&#39;. We also have a Bellman equation for the state-value function: $$ Q^\\pi (s_t, a_t) = \\mathbb{E}_\\pi \\left[ r_t + \\gamma Q^\\pi (s_{t+1}, a_{t+1}) \\vert s_t, a_t \\right] $$ $Q$-learning&#182;$Q$-learning is an algorithm analogous to the TD(0) algorithm we&#39;ve described before. In TD(0), we have a table $V$ containing predictions for $V^\\pi (s_t)$ for each state $s_t$, updating our predictions as follows: $$ V (s_t) \\gets V (s_t) + \\alpha \\left( r_t + \\gamma V (s_{t+1}) - V (s_t) \\right) $$In $Q$-learning, we have a similar learning rule, except that our table now contains values for any state-action pair $(s_t, a_t)$. $$ Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha \\left( r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right) $$The advantage of $Q$-learning is that we have an estimate, for each action, of the expected discounted return we will get from taking that action. Recall the reward hypothesis: Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal. Consider being in some state $s_t$, and for each action $a_t$ we compute $Q(s_t, a_t)$. These values are our guesses for the discounted return we will get as a result of choosing each action. If we choose the action that maximizes $Q(s_t, a_t)$, then our agent is maximizing the discounted return. This defines a greedy policy $$ \\pi(a_t \\vert s_t) = \\begin{cases} 1, &amp; a_t = \\arg \\max_{a_t} Q(s_t, a_t) \\\\ 0, &amp; \\text{otherwise} \\end{cases} $$Knowing this, we can estimate the $Q$ value for the next state-action pair $(s_{t+1}, a_{t+1})$ by assuming the agent chooses the action that maximizes that $Q$ value. This gives us the following update rule: $$ Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha \\left( r_t + \\gamma \\max_{a_{t+1}} \\left( Q(s_{t+1}, a_{t+1}) \\right) - Q(s_t, a_t) \\right) $$ Modified Bellman Equation and Terminal States&#182;Recall that some environments are episodic. This means that there must be some states $s_T$ where, regardless of what action is chosen, the next state is always $s_T$ and the reward is always $0$. The gym library defines an environment api with a step function that returns $s_{t+1}, r_t, d_t,$ info given an action $a_t$. Here, $d_t$ is a boolean flag denoting whether or not $s_{t+1}$ is terminal. By the definition of $Q(s_t, a_t)$, we can see that $r_t + \\gamma Q(s_{t+1}, a_{t+1})$ is an unbiased estimate for $Q(s_t, a_t)$. This is what gives us the Bellman equation update rule. If we know that a state is terminal, then we have an even better estimate: if $s_t$ is terminal, then $G_t = r_t$, and our update rule is just $$ Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha \\left( r_t - Q(s_t, a_t) \\right) $$If we interpret the truthiness of $d_t$ as an integer (i.e., $d_t = 1$ if True and $0$ if False), then we can define a modified update rule that takes terminal states into account: $$ Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha \\left( r_t + (1-d_t)\\gamma \\max_{a_{t+1}} \\left( Q(s_{t+1}, a_{t+1}) \\right) - Q(s_t, a_t) \\right) $$ Exploration-Exploitation&#182;Our agent can quickly become stuck in a suboptimal policy by always choosing the same action in the same state. Recall that we use the greedy policy to choose actions. If we choose a certain action in a certain state, it&#39;s because that action has the maximal $Q$ value associated with it. If we always follow the greedy policy, we will generally tend to continue to choose this action since we don&#39;t get the opportunity to explore (and thus update our predictions for) the other actions. This problem is known as the exploration-exploitation tradeoff: should we try to learn more about the $Q$ function by exploring (and thus finding a better policy) or should we exploit what we know about the environment to try to maximize the returns that we know about? $\\epsilon$-greedy policies&#182;The simplest approach to solving the exploration-exploitation tradeoff is to randomly choose an action $a_t \\sim \\mathcal{A}$ rather than from the policy $a_t \\sim \\pi$. We choose a random action with a probability $\\epsilon$. At the beginning of training, the agent should not follow its policy, since its estimates for $Q(s_t, a_t)$ are poor. Instead, the agent should focus on gathering as much data as possible to improve its guesses, and should do this by exploring. This means that initially, $\\epsilon$ should be high (close to 1). Over time, as our predictions for $Q(s_t, a_t)$ improve, we can trust our prediction and thus our greedy policy more, and thus $\\epsilon$ should be low (close to 0). We define $\\epsilon_i$ to be the initial value for $\\epsilon$ and $\\epsilon_f$ to be the final value. We decay $\\epsilon$ using either linear or exponential decay. Linear decay means we subtract a constant from $\\epsilon$ at every time step, and exponential decay means we multiply $\\epsilon$ by some decay factor less than 1. Here we use linear decay. In&nbsp;[2]: class Agent: def __init__(self, num_states, num_actions, epsilon_i=1.0, epsilon_f=0.0, n_epsilon=0.1, alpha=0.5, gamma = 0.95): &quot;&quot;&quot; num_states: the number of states in the discrete state space num_actions: the number of actions in the discrete action space epsilon_i: the initial value for epsilon epsilon_f: the final value for epsilon n_epsilon: a float between 0 and 1 determining at which point during training epsilon should have decayed from epsilon_i to epsilon_f alpha: the learning rate gamma: the decay rate &quot;&quot;&quot; self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.epsilon = epsilon_i self.n_epsilon = n_epsilon self.num_states = num_states self.num_actions = num_actions self.alpha = alpha self.gamma = gamma self.Q = np.zeros((num_states, num_actions)) def decay_epsilon(self, n): &quot;&quot;&quot; Decays the agent&#39;s exploration rate according to n, which is a float between 0 and 1 describing how far along training is, with 0 meaning &#39;just started&#39; and 1 meaning &#39;done&#39;. &quot;&quot;&quot; self.epsilon = max( self.epsilon_f, self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f)) def act(self, s_t): &quot;&quot;&quot; Epsilon-greedy policy. &quot;&quot;&quot; if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions) return np.argmax(self.Q[s_t]) def update(self, s_t, a_t, r_t, s_t_next, d_t): &quot;&quot;&quot; Uses the q-learning update rule to update the agent&#39;s predictions for Q(s_t, a_t). &quot;&quot;&quot; Q_next = np.max(self.Q[s_t_next]) self.Q[s_t, a_t] = self.Q[s_t, a_t] + \\ self.alpha*(r_t + (1-d_t)*self.gamma*Q_next - \\ self.Q[s_t, a_t]) In&nbsp;[3]: def plot(data, window=100): &quot;&quot;&quot; Given a pandas dataframe &#39;data&#39;, plots a rolling mean of the data using a window size of &#39;window&#39;. &quot;&quot;&quot; sns.lineplot( data=data.rolling(window=window).mean()[window-1::window] ) In&nbsp;[4]: def train(env_name, T=100000, alpha=0.8, gamma=0.95, epsilon_i = 1.0, epsilon_f = 0.0, n_epsilon = 0.1): env = gym.make(env_name) num_states = env.observation_space.n num_actions = env.action_space.n agent = Agent(num_states, num_actions, alpha=alpha, gamma=gamma, epsilon_i=epsilon_i, epsilon_f=epsilon_f, n_epsilon = n_epsilon) rewards = [] episode_rewards = 0 s_t = env.reset() for t in range(T): a_t = agent.act(s_t) s_t_next, r_t, d_t, info = env.step(a_t) agent.update(s_t, a_t, r_t, s_t_next, d_t) agent.decay_epsilon(t/T) s_t = s_t_next episode_rewards += r_t if d_t: rewards.append(episode_rewards) episode_rewards = 0 s_t = env.reset() plot(pd.DataFrame(rewards)) return agent In&nbsp;[7]: train(&quot;FrozenLake-v0&quot;, T=100000) Out[7]: &lt;__main__.Agent at 0x1a18692320&gt; In&nbsp;[&nbsp;]:","@type":"BlogPosting","url":"http://localhost:4000/q-learning-numpy/","headline":"Q Learning Numpy","dateModified":"2019-05-12T00:00:00-04:00","datePublished":"2019-05-12T00:00:00-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/q-learning-numpy/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <h1 id="project_title"> TF 2.0 for Reinforcement Learning </h1>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        
        <h4>
          <a href="http://localhost:4000">Home</a>
        </h4>
        
        
        <p> Download the <a href="http://localhost:4000/assets/notebooks/q-learning-numpy.ipynb"> notebook </a> or follow along. </p>
        
        
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<h1 id="Q-learning">Q-learning<a class="anchor-link" href="#Q-learning">&#182;</a></h1><hr />
<h3 id="Value-Function">Value Function<a class="anchor-link" href="#Value-Function">&#182;</a></h3><p>Recall the definition of the <strong>value function</strong> $V(s_t)$:</p>
$$
V(s_t) = \mathbb{E} \left[ G_t \vert s_t \right]
$$<p>In an MDP, our expected discounted return $G_t$ depends on the trajectories $\tau$ that we generate. Whereas in a Markov process or Markov reward process this depends only on the underlying state dynamics, in an MDP, trajectories also depend on the actions that we choose. Since these actions are determined by our <strong>policy</strong> $\pi$, we write the following:</p>
$$
V^\pi (s_t) = \mathbb{E}_\pi \left[ G_t \vert s_t \right]
$$<p>that is, given that we are currently in state $s_t$, what value should we expect our discounted return $G_t$ to be, given that we generate trajectories according to our policy $\pi$?</p>
<hr />
<h3 id="Action-Value-function">Action-Value function<a class="anchor-link" href="#Action-Value-function">&#182;</a></h3><p>We can define an analogous definition for the <strong>action-value function</strong>, which extends the notion of the value function to account for choosing a <em>specific action</em> $a_t$ in a state $s_t$, rather than just choosing the one suggested by the policy:</p>
$$
Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ G_t \vert s_t, a_t \right]
$$<p>that is, given that we are currently in state $s_t$, taking action $a_t$, what value should we expect our discounted return $G_t$ to be? We can see the relationship between $V^\pi (s_t)$ and $Q^\pi (s_t, a_t)$:</p>
$$
V^\pi (s_t) = \mathbb{E}_{a_t \sim \pi} \left[ Q^\pi (s_t, a_t) \vert s_t \right]
$$<p>where $a_t \sim \pi$ means 'over actions $a_t$ selected according to a probability distribution $\pi$'.</p>
<p>We also have a Bellman equation for the state-value function:
$$
Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ r_t + \gamma Q^\pi (s_{t+1}, a_{t+1}) \vert s_t, a_t \right]
$$</p>
<hr />
<h3 id="$Q$-learning">$Q$-learning<a class="anchor-link" href="#$Q$-learning">&#182;</a></h3><p>$Q$-learning is an algorithm analogous to the TD(0) algorithm we've described before. In TD(0), we have a table $V$ containing predictions for $V^\pi (s_t)$ for each state $s_t$, updating our predictions as follows:</p>
$$
V (s_t) \gets V (s_t) + \alpha \left( r_t + \gamma V (s_{t+1}) - V (s_t) \right)
$$<p>In $Q$-learning, we have a similar learning rule, except that our table now contains values for any state-action pair $(s_t, a_t)$.</p>
$$
Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right)
$$<p>The advantage of $Q$-learning is that we have an estimate, for each action, of the expected discounted return we will get from taking that action. Recall the reward hypothesis:</p>
<blockquote><p>Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal.</p>
</blockquote>
<p>Consider being in some state $s_t$, and for each action $a_t$ we compute $Q(s_t, a_t)$. These values are our guesses for the discounted return we will get as a result of choosing each action. If we choose the action that maximizes $Q(s_t, a_t)$, then our agent is maximizing the discounted return. This defines a <strong>greedy policy</strong></p>
$$
\pi(a_t \vert s_t) = \begin{cases} 1, &amp; a_t = \arg \max_{a_t} Q(s_t, a_t) \\ 0, &amp; \text{otherwise} \end{cases}
$$<p>Knowing this, we can estimate the $Q$ value for the next state-action pair $(s_{t+1}, a_{t+1})$ by assuming the agent chooses the action that maximizes that $Q$ value. This gives us the following update rule:</p>
$$
Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + \gamma \max_{a_{t+1}} \left( Q(s_{t+1}, a_{t+1}) \right) - Q(s_t, a_t) \right)
$$<hr />
<h3 id="Modified-Bellman-Equation-and-Terminal-States">Modified Bellman Equation and Terminal States<a class="anchor-link" href="#Modified-Bellman-Equation-and-Terminal-States">&#182;</a></h3><p>Recall that some environments are <strong>episodic</strong>. This means that there must be some states $s_T$ where, regardless of what action is chosen, the next state is always $s_T$ and the reward is always $0$. The <code>gym</code> library defines an environment api with a <code>step</code> function that returns $s_{t+1}, r_t, d_t,$ info given an action $a_t$. Here, $d_t$ is a boolean flag denoting whether or not $s_{t+1}$ is terminal.</p>
<p>By the definition of $Q(s_t, a_t)$, we can see that $r_t + \gamma Q(s_{t+1}, a_{t+1})$ is an unbiased estimate for $Q(s_t, a_t)$. This is what gives us the Bellman equation update rule. If we know that a state is terminal, then we have an even better estimate: if $s_t$ is terminal, then $G_t = r_t$, and our update rule is just</p>
$$
Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t - Q(s_t, a_t) \right)
$$<p>If we interpret the truthiness of $d_t$ as an integer (i.e., $d_t = 1$ if <code>True</code> and $0$ if <code>False</code>), then we can define a modified update rule that takes terminal states into account:</p>
$$
Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q(s_{t+1}, a_{t+1}) \right) - Q(s_t, a_t) \right)
$$<hr />
<h3 id="Exploration-Exploitation">Exploration-Exploitation<a class="anchor-link" href="#Exploration-Exploitation">&#182;</a></h3><p>Our agent can quickly become stuck in a suboptimal policy by always choosing the same action in the same state. Recall that we use the greedy policy to choose actions. If we choose a certain action in a certain state, it's because that action has the maximal $Q$ value associated with it. If we always follow the greedy policy, we will generally tend to continue to choose this action since we don't get the opportunity to explore (and thus update our predictions for) the other actions. This problem is known as the <strong>exploration-exploitation tradeoff</strong>: should we try to learn more about the $Q$ function by exploring (and thus finding a better policy) or should we exploit what we know about the environment to try to maximize the returns that we know about?</p>
<h3 id="$\epsilon$-greedy-policies">$\epsilon$-greedy policies<a class="anchor-link" href="#$\epsilon$-greedy-policies">&#182;</a></h3><p>The simplest approach to solving the exploration-exploitation tradeoff is to randomly choose an action $a_t \sim \mathcal{A}$ rather than from the policy $a_t \sim \pi$. We choose a random action with a probability $\epsilon$. At the beginning of training, the agent should not follow its policy, since its estimates for $Q(s_t, a_t)$ are poor. Instead, the agent should focus on gathering as much data as possible to improve its guesses, and should do this by exploring. This means that initially, $\epsilon$ should be high (close to 1). Over time, as our predictions for $Q(s_t, a_t)$ improve, we can trust our prediction and thus our greedy policy more, and thus $\epsilon$ should be low (close to 0).</p>
<p>We define $\epsilon_i$ to be the initial value for $\epsilon$ and $\epsilon_f$ to be the final value. We decay $\epsilon$ using either linear or exponential decay. Linear decay means we subtract a constant from $\epsilon$ at every time step, and exponential decay means we multiply $\epsilon$ by some decay factor less than 1. Here we use linear decay.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> 
                 <span class="n">epsilon_i</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
                 <span class="n">epsilon_f</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> 
                 <span class="n">n_epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> 
                 <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">):</span>
        
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        num_states: the number of states in the discrete state space</span>
<span class="sd">        num_actions: the number of actions in the discrete action space</span>
<span class="sd">        epsilon_i: the initial value for epsilon</span>
<span class="sd">        epsilon_f: the final value for epsilon</span>
<span class="sd">        n_epsilon: a float between 0 and 1 determining at which point</span>
<span class="sd">        during training epsilon should have decayed from epsilon_i to</span>
<span class="sd">        epsilon_f</span>
<span class="sd">        alpha: the learning rate</span>
<span class="sd">        gamma: the decay rate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">=</span> <span class="n">epsilon_i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span> <span class="o">=</span> <span class="n">epsilon_f</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon_i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_epsilon</span> <span class="o">=</span> <span class="n">n_epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_states</span> <span class="o">=</span> <span class="n">num_states</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">num_actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">decay_epsilon</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Decays the agent&#39;s exploration rate according to n, which is a</span>
<span class="sd">        float between 0 and 1 describing how far along training is, </span>
<span class="sd">        with 0 meaning &#39;just started&#39; and 1 meaning &#39;done&#39;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span><span class="p">,</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">-</span> <span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">n_epsilon</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Epsilon-greedy policy.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">d_t</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Uses the q-learning update rule to update the agent&#39;s predictions</span>
<span class="sd">        for Q(s_t, a_t).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">Q_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t_next</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">]</span> <span class="o">+</span> \
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">r_t</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">d_t</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">Q_next</span> <span class="o">-</span> \
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given a pandas dataframe &#39;data&#39;, plots a rolling mean of the data</span>
<span class="sd">    using a window size of &#39;window&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">::</span><span class="n">window</span><span class="p">]</span>
    <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">env_name</span><span class="p">,</span>
         <span class="n">T</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> 
          <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> 
          <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> 
          <span class="n">epsilon_i</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> 
          <span class="n">epsilon_f</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> 
          <span class="n">n_epsilon</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">)</span>
    <span class="n">num_states</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">num_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> 
                  <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> 
                  <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> 
                  <span class="n">epsilon_i</span><span class="o">=</span><span class="n">epsilon_i</span><span class="p">,</span> 
                  <span class="n">epsilon_f</span><span class="o">=</span><span class="n">epsilon_f</span><span class="p">,</span> 
                  <span class="n">n_epsilon</span> <span class="o">=</span> <span class="n">n_epsilon</span><span class="p">)</span>

    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">episode_rewards</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">a_t</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s_t</span><span class="p">)</span>
        <span class="n">s_t_next</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">d_t</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a_t</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">d_t</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">decay_epsilon</span><span class="p">(</span><span class="n">t</span><span class="o">/</span><span class="n">T</span><span class="p">)</span>
        <span class="n">s_t</span> <span class="o">=</span> <span class="n">s_t_next</span>
        <span class="n">episode_rewards</span> <span class="o">+=</span> <span class="n">r_t</span>
        
        <span class="k">if</span> <span class="n">d_t</span><span class="p">:</span>
            <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">)</span>
            <span class="n">episode_rewards</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            
    <span class="n">plot</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">agent</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train</span><span class="p">(</span><span class="s2">&quot;FrozenLake-v0&quot;</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[7]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;__main__.Agent at 0x1a18692320&gt;</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>


        
        <h4>
          <a href="http://localhost:4000">Home</a>
        </h4>
        
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">alexandervandekleut.github.io maintained by <a href="https://github.com/alexandervandekleut">alexandervandekleut</a></p>
        
      </footer>
    </div>

    
  </body>
</html>
