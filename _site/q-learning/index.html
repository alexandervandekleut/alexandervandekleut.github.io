<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" type="text/css" media="screen" href="/assets/css/style.css?v=e88ea2ad56e95913e172df6f31c1ff82889ac891">
    <!-- Mathjax Support -->
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Q Learning | alexandervandekleut.github.io</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Q Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Q-learning&#182; Value Function&#182;Recall the definition of the value function $V(s_t)$: $$ V(s_t) = \mathbb{E} \left[ G_t \vert s_t \right] $$In an MDP, our expected discounted return $G_t$ depends on the trajectories $\tau$ that we generate. Whereas in a Markov process or Markov reward process this depends only on the underlying state dynamics, in an MDP, trajectories also depend on the actions that we choose. Since these actions are determined by our policy $\pi$, we write the following: $$ V^\pi (s_t) = \mathbb{E}_\pi \left[ G_t \vert s_t \right] $$that is, given that we are currently in state $s_t$, what value should we expect our discounted return $G_t$ to be, given that we generate trajectories according to our policy $\pi$? Action-Value function&#182;We can define an analogous definition for the action-value function, which extends the notion of the value function to account for choosing a specific action $a_t$ in a state $s_t$, rather than just choosing the one suggested by the policy: $$ Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ G_t \vert s_t, a_t \right] $$that is, given that we are currently in state $s_t$, taking action $a_t$, what value should we expect our discounted return $G_t$ to be? We can see the relationship between $V^\pi (s_t)$ and $Q^\pi (s_t, a_t)$: $$ V^\pi (s_t) = \mathbb{E}_{a_t \sim \pi} \left[ Q^\pi (s_t, a_t) \vert s_t \right] $$We also have a Bellman equation for the state-value function: $$ Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ r_t + \gamma Q^\pi (s_{t+1}, a_{t+1}) \vert s_t, a_t \right] $$ $Q$-learning&#182;$Q$-learning is an algorithm analogous to the TD(0) algorithm we&#39;ve described before. In TD(0), we have a table $V$ containing predictions for $V^\pi (s_t)$ for each state $s_t$, updating our predictions as follows: $$ V (s_t) \gets V (s_t) + \alpha \left( r_t + \gamma V (s_{t+1}) - V (s_t) \right) $$In $Q$-learning, we have a similar learning rule, except that our table now contains values for any state-action pair $(s_t, a_t)$. $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right) $$The advantage of $Q$-learning is that we have an estimate, for each action, of the expected discounted return we will get from taking that action. Recall the reward hypothesis: Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal. Consider being in some state $s_t$, and for each action $a_t$ we compute $Q(s_t, a_t)$. These values are our guesses for the discounted return we will get as a result of choosing each action. If we choose the action that maximizes $Q(s_t, a_t)$, then our agent is maximizing the discounted return. This defines a greedy policy $$ \pi(a_t \vert s_t) = \begin{cases} 1, &amp; a_t = \arg \max_{a_t} Q(s_t, a_t) \\ 0, &amp; \text{otherwise} \end{cases} $$ $\varepsilon$-greedy&#182;Our agent can quickly become stuck in a suboptimal policy by always choosing the same action in the same state. This prevents the agent from exploring, and consequently from improving its predictions about $Q$. This problem is known as the exploration-exploitation tradeoff: should we try to learn more about the $Q$ function by exploring (and thus finding a better policy) or should we exploit what we know about the environment to try to maximize the returns that we know about? We use a parameter $\varepsilon$ that determines with what probability we choose a random action versus choosing an action according to the greedy policy. We generate a random number $p \in \left[ 0, 1 \right]$. If $p &lt; \varepsilon$ then we choose a random action, and if $p \geq \varepsilon$ then we choose an action according to the greedy policy. Initially, our guesses about $Q(s_t, a_t)$ are poor because they are randomly intialized. Thus, we should not use them to guide our policy at first. Therefore, initially $\varepsilon$ should be high (closer to 1). Over time, the agent improves its guesses about $Q(s_t, a_t)$ and so it becomes more reliable in our greedy policy. A simple idea is to have some initial value $\varepsilon_i$ and $\varepsilon_f$ for the initial and final values of $\varepsilon$, and some number of timesteps $n_\varepsilon$ over which $\varepsilon_i$ decays to $\varepsilon_f$. In&nbsp;[1]: import numpy as np import gym In&nbsp;[2]: class Agent: def __init__(self, num_states, num_actions, epsilon_i=1.0, epsilon_f=0.0, n_epsilon=10000, alpha=0.1, gamma = 0.99): self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.epsilon = epsilon_i self.n_epsilon = n_epsilon self.num_states = num_states self.num_actions = num_actions self.alpha = alpha self.gamma = gamma self.Q = np.zeros((num_states, num_actions)) def decay_epsilon(self): self.epsilon = max( self.epsilon_f, self.epsilon - (self.epsilon_i - self.epsilon_f)/self.n_epsilon) def act(self, s_t): if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions) return np.argmax(self.Q[s_t]) def update(self, s_t, a_t, s_t_next, r_t): a_t_next = self.act(s_t_next) self.Q[s_t, a_t] = self.Q[s_t, a_t] + \ self.alpha*(r_t + self.gamma*self.Q[s_t_next, a_t_next] - \ self.Q[s_t, a_t]) A quick note: it is extremely important that you initialize your $Q$-table to zeros. Initially, I had it set to random values in the range $\left[ 0, 1 \right]$. However, this introduced a bias to the agent that prevented it from learning as these values were driving the agent to choose actions that prevented exploration. In&nbsp;[14]: env = gym.make(&quot;FrozenLake-v0&quot;) num_states = env.observation_space.n num_actions = env.action_space.n agent = Agent(num_states, num_actions, alpha=0.5, gamma=0.95, epsilon_i=1.0, epsilon_f=0.0) T = 100000 s_t = env.reset() rewards = [] episode_rewards = 0 for t in range(T): if t%1000 == 0: print(f&#39;{100*t/T}%&#39;, end=&#39;\r&#39;) a_t = agent.act(s_t) s_t_next, r_t, done, info = env.step(a_t) agent.update(s_t, a_t, s_t_next, r_t) agent.decay_epsilon() s_t = s_t_next episode_rewards += r_t if done: rewards.append(episode_rewards) episode_rewards = 0 s_t = env.reset() 99.9% We are going to plot these terminal rewards over the course of our training. We use pandas to store our data, and to perform a rolling mean calculation, and we use seaborn to plot the data. In reinforcement learning, rewards can be noisy (have high variance). To smooth out the plot of rewards over time, we calculate a rolling mean with a certain window size. In&nbsp;[15]: import pandas as pd import seaborn as sns sns.set() In&nbsp;[16]: window = 100 running_mean_rewards = pd.Series(rewards, name=&#39;q-learning&#39;).rolling(window=window).mean()[window-1:] # this slicing is because the first window-1 elements are NaN from the rolling operation. sns.lineplot(data=running_mean_rewards) Out[16]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x101f97e80&gt; It is always a good idea to ensure that your agent is performing better than random. We can essentially run the same loop, but choosing random actions each time. In&nbsp;[17]: T = 100000 s_t = env.reset() random_rewards = [] for t in range(T): a_t = env.action_space.sample() s_t, r_t, done, info = env.step(a_t) if done: random_rewards.append(r_t) s_t = env.reset() In&nbsp;[18]: random_rewards = np.asarray(random_rewards)[:len(rewards)] # make sure random_rewards is the same length as rewards. It will generally be much longer, since it will die more frequently causing more rewards to be stored given the same budget of timesteps. window = 100 running_mean_random_rewards = pd.Series(random_rewards, name=&#39;random&#39;).rolling(window=window).mean()[window-1:] In&nbsp;[19]: compare = pd.concat((running_mean_random_rewards, running_mean_rewards), axis=1) In&nbsp;[20]: sns.lineplot(data=compare) Out[20]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a108f9e80&gt; And there we have it! A fully-functional $Q$-learning agent that can perform strongly in FrozenLake-v0." />
<meta property="og:description" content="Q-learning&#182; Value Function&#182;Recall the definition of the value function $V(s_t)$: $$ V(s_t) = \mathbb{E} \left[ G_t \vert s_t \right] $$In an MDP, our expected discounted return $G_t$ depends on the trajectories $\tau$ that we generate. Whereas in a Markov process or Markov reward process this depends only on the underlying state dynamics, in an MDP, trajectories also depend on the actions that we choose. Since these actions are determined by our policy $\pi$, we write the following: $$ V^\pi (s_t) = \mathbb{E}_\pi \left[ G_t \vert s_t \right] $$that is, given that we are currently in state $s_t$, what value should we expect our discounted return $G_t$ to be, given that we generate trajectories according to our policy $\pi$? Action-Value function&#182;We can define an analogous definition for the action-value function, which extends the notion of the value function to account for choosing a specific action $a_t$ in a state $s_t$, rather than just choosing the one suggested by the policy: $$ Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ G_t \vert s_t, a_t \right] $$that is, given that we are currently in state $s_t$, taking action $a_t$, what value should we expect our discounted return $G_t$ to be? We can see the relationship between $V^\pi (s_t)$ and $Q^\pi (s_t, a_t)$: $$ V^\pi (s_t) = \mathbb{E}_{a_t \sim \pi} \left[ Q^\pi (s_t, a_t) \vert s_t \right] $$We also have a Bellman equation for the state-value function: $$ Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ r_t + \gamma Q^\pi (s_{t+1}, a_{t+1}) \vert s_t, a_t \right] $$ $Q$-learning&#182;$Q$-learning is an algorithm analogous to the TD(0) algorithm we&#39;ve described before. In TD(0), we have a table $V$ containing predictions for $V^\pi (s_t)$ for each state $s_t$, updating our predictions as follows: $$ V (s_t) \gets V (s_t) + \alpha \left( r_t + \gamma V (s_{t+1}) - V (s_t) \right) $$In $Q$-learning, we have a similar learning rule, except that our table now contains values for any state-action pair $(s_t, a_t)$. $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right) $$The advantage of $Q$-learning is that we have an estimate, for each action, of the expected discounted return we will get from taking that action. Recall the reward hypothesis: Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal. Consider being in some state $s_t$, and for each action $a_t$ we compute $Q(s_t, a_t)$. These values are our guesses for the discounted return we will get as a result of choosing each action. If we choose the action that maximizes $Q(s_t, a_t)$, then our agent is maximizing the discounted return. This defines a greedy policy $$ \pi(a_t \vert s_t) = \begin{cases} 1, &amp; a_t = \arg \max_{a_t} Q(s_t, a_t) \\ 0, &amp; \text{otherwise} \end{cases} $$ $\varepsilon$-greedy&#182;Our agent can quickly become stuck in a suboptimal policy by always choosing the same action in the same state. This prevents the agent from exploring, and consequently from improving its predictions about $Q$. This problem is known as the exploration-exploitation tradeoff: should we try to learn more about the $Q$ function by exploring (and thus finding a better policy) or should we exploit what we know about the environment to try to maximize the returns that we know about? We use a parameter $\varepsilon$ that determines with what probability we choose a random action versus choosing an action according to the greedy policy. We generate a random number $p \in \left[ 0, 1 \right]$. If $p &lt; \varepsilon$ then we choose a random action, and if $p \geq \varepsilon$ then we choose an action according to the greedy policy. Initially, our guesses about $Q(s_t, a_t)$ are poor because they are randomly intialized. Thus, we should not use them to guide our policy at first. Therefore, initially $\varepsilon$ should be high (closer to 1). Over time, the agent improves its guesses about $Q(s_t, a_t)$ and so it becomes more reliable in our greedy policy. A simple idea is to have some initial value $\varepsilon_i$ and $\varepsilon_f$ for the initial and final values of $\varepsilon$, and some number of timesteps $n_\varepsilon$ over which $\varepsilon_i$ decays to $\varepsilon_f$. In&nbsp;[1]: import numpy as np import gym In&nbsp;[2]: class Agent: def __init__(self, num_states, num_actions, epsilon_i=1.0, epsilon_f=0.0, n_epsilon=10000, alpha=0.1, gamma = 0.99): self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.epsilon = epsilon_i self.n_epsilon = n_epsilon self.num_states = num_states self.num_actions = num_actions self.alpha = alpha self.gamma = gamma self.Q = np.zeros((num_states, num_actions)) def decay_epsilon(self): self.epsilon = max( self.epsilon_f, self.epsilon - (self.epsilon_i - self.epsilon_f)/self.n_epsilon) def act(self, s_t): if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions) return np.argmax(self.Q[s_t]) def update(self, s_t, a_t, s_t_next, r_t): a_t_next = self.act(s_t_next) self.Q[s_t, a_t] = self.Q[s_t, a_t] + \ self.alpha*(r_t + self.gamma*self.Q[s_t_next, a_t_next] - \ self.Q[s_t, a_t]) A quick note: it is extremely important that you initialize your $Q$-table to zeros. Initially, I had it set to random values in the range $\left[ 0, 1 \right]$. However, this introduced a bias to the agent that prevented it from learning as these values were driving the agent to choose actions that prevented exploration. In&nbsp;[14]: env = gym.make(&quot;FrozenLake-v0&quot;) num_states = env.observation_space.n num_actions = env.action_space.n agent = Agent(num_states, num_actions, alpha=0.5, gamma=0.95, epsilon_i=1.0, epsilon_f=0.0) T = 100000 s_t = env.reset() rewards = [] episode_rewards = 0 for t in range(T): if t%1000 == 0: print(f&#39;{100*t/T}%&#39;, end=&#39;\r&#39;) a_t = agent.act(s_t) s_t_next, r_t, done, info = env.step(a_t) agent.update(s_t, a_t, s_t_next, r_t) agent.decay_epsilon() s_t = s_t_next episode_rewards += r_t if done: rewards.append(episode_rewards) episode_rewards = 0 s_t = env.reset() 99.9% We are going to plot these terminal rewards over the course of our training. We use pandas to store our data, and to perform a rolling mean calculation, and we use seaborn to plot the data. In reinforcement learning, rewards can be noisy (have high variance). To smooth out the plot of rewards over time, we calculate a rolling mean with a certain window size. In&nbsp;[15]: import pandas as pd import seaborn as sns sns.set() In&nbsp;[16]: window = 100 running_mean_rewards = pd.Series(rewards, name=&#39;q-learning&#39;).rolling(window=window).mean()[window-1:] # this slicing is because the first window-1 elements are NaN from the rolling operation. sns.lineplot(data=running_mean_rewards) Out[16]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x101f97e80&gt; It is always a good idea to ensure that your agent is performing better than random. We can essentially run the same loop, but choosing random actions each time. In&nbsp;[17]: T = 100000 s_t = env.reset() random_rewards = [] for t in range(T): a_t = env.action_space.sample() s_t, r_t, done, info = env.step(a_t) if done: random_rewards.append(r_t) s_t = env.reset() In&nbsp;[18]: random_rewards = np.asarray(random_rewards)[:len(rewards)] # make sure random_rewards is the same length as rewards. It will generally be much longer, since it will die more frequently causing more rewards to be stored given the same budget of timesteps. window = 100 running_mean_random_rewards = pd.Series(random_rewards, name=&#39;random&#39;).rolling(window=window).mean()[window-1:] In&nbsp;[19]: compare = pd.concat((running_mean_random_rewards, running_mean_rewards), axis=1) In&nbsp;[20]: sns.lineplot(data=compare) Out[20]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a108f9e80&gt; And there we have it! A fully-functional $Q$-learning agent that can perform strongly in FrozenLake-v0." />
<link rel="canonical" href="http://localhost:4000/q-learning/" />
<meta property="og:url" content="http://localhost:4000/q-learning/" />
<meta property="og:site_name" content="alexandervandekleut.github.io" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-12T00:00:00-04:00" />
<script type="application/ld+json">
{"description":"Q-learning&#182; Value Function&#182;Recall the definition of the value function $V(s_t)$: $$ V(s_t) = \\mathbb{E} \\left[ G_t \\vert s_t \\right] $$In an MDP, our expected discounted return $G_t$ depends on the trajectories $\\tau$ that we generate. Whereas in a Markov process or Markov reward process this depends only on the underlying state dynamics, in an MDP, trajectories also depend on the actions that we choose. Since these actions are determined by our policy $\\pi$, we write the following: $$ V^\\pi (s_t) = \\mathbb{E}_\\pi \\left[ G_t \\vert s_t \\right] $$that is, given that we are currently in state $s_t$, what value should we expect our discounted return $G_t$ to be, given that we generate trajectories according to our policy $\\pi$? Action-Value function&#182;We can define an analogous definition for the action-value function, which extends the notion of the value function to account for choosing a specific action $a_t$ in a state $s_t$, rather than just choosing the one suggested by the policy: $$ Q^\\pi (s_t, a_t) = \\mathbb{E}_\\pi \\left[ G_t \\vert s_t, a_t \\right] $$that is, given that we are currently in state $s_t$, taking action $a_t$, what value should we expect our discounted return $G_t$ to be? We can see the relationship between $V^\\pi (s_t)$ and $Q^\\pi (s_t, a_t)$: $$ V^\\pi (s_t) = \\mathbb{E}_{a_t \\sim \\pi} \\left[ Q^\\pi (s_t, a_t) \\vert s_t \\right] $$We also have a Bellman equation for the state-value function: $$ Q^\\pi (s_t, a_t) = \\mathbb{E}_\\pi \\left[ r_t + \\gamma Q^\\pi (s_{t+1}, a_{t+1}) \\vert s_t, a_t \\right] $$ $Q$-learning&#182;$Q$-learning is an algorithm analogous to the TD(0) algorithm we&#39;ve described before. In TD(0), we have a table $V$ containing predictions for $V^\\pi (s_t)$ for each state $s_t$, updating our predictions as follows: $$ V (s_t) \\gets V (s_t) + \\alpha \\left( r_t + \\gamma V (s_{t+1}) - V (s_t) \\right) $$In $Q$-learning, we have a similar learning rule, except that our table now contains values for any state-action pair $(s_t, a_t)$. $$ Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha \\left( r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right) $$The advantage of $Q$-learning is that we have an estimate, for each action, of the expected discounted return we will get from taking that action. Recall the reward hypothesis: Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal. Consider being in some state $s_t$, and for each action $a_t$ we compute $Q(s_t, a_t)$. These values are our guesses for the discounted return we will get as a result of choosing each action. If we choose the action that maximizes $Q(s_t, a_t)$, then our agent is maximizing the discounted return. This defines a greedy policy $$ \\pi(a_t \\vert s_t) = \\begin{cases} 1, &amp; a_t = \\arg \\max_{a_t} Q(s_t, a_t) \\\\ 0, &amp; \\text{otherwise} \\end{cases} $$ $\\varepsilon$-greedy&#182;Our agent can quickly become stuck in a suboptimal policy by always choosing the same action in the same state. This prevents the agent from exploring, and consequently from improving its predictions about $Q$. This problem is known as the exploration-exploitation tradeoff: should we try to learn more about the $Q$ function by exploring (and thus finding a better policy) or should we exploit what we know about the environment to try to maximize the returns that we know about? We use a parameter $\\varepsilon$ that determines with what probability we choose a random action versus choosing an action according to the greedy policy. We generate a random number $p \\in \\left[ 0, 1 \\right]$. If $p &lt; \\varepsilon$ then we choose a random action, and if $p \\geq \\varepsilon$ then we choose an action according to the greedy policy. Initially, our guesses about $Q(s_t, a_t)$ are poor because they are randomly intialized. Thus, we should not use them to guide our policy at first. Therefore, initially $\\varepsilon$ should be high (closer to 1). Over time, the agent improves its guesses about $Q(s_t, a_t)$ and so it becomes more reliable in our greedy policy. A simple idea is to have some initial value $\\varepsilon_i$ and $\\varepsilon_f$ for the initial and final values of $\\varepsilon$, and some number of timesteps $n_\\varepsilon$ over which $\\varepsilon_i$ decays to $\\varepsilon_f$. In&nbsp;[1]: import numpy as np import gym In&nbsp;[2]: class Agent: def __init__(self, num_states, num_actions, epsilon_i=1.0, epsilon_f=0.0, n_epsilon=10000, alpha=0.1, gamma = 0.99): self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.epsilon = epsilon_i self.n_epsilon = n_epsilon self.num_states = num_states self.num_actions = num_actions self.alpha = alpha self.gamma = gamma self.Q = np.zeros((num_states, num_actions)) def decay_epsilon(self): self.epsilon = max( self.epsilon_f, self.epsilon - (self.epsilon_i - self.epsilon_f)/self.n_epsilon) def act(self, s_t): if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions) return np.argmax(self.Q[s_t]) def update(self, s_t, a_t, s_t_next, r_t): a_t_next = self.act(s_t_next) self.Q[s_t, a_t] = self.Q[s_t, a_t] + \\ self.alpha*(r_t + self.gamma*self.Q[s_t_next, a_t_next] - \\ self.Q[s_t, a_t]) A quick note: it is extremely important that you initialize your $Q$-table to zeros. Initially, I had it set to random values in the range $\\left[ 0, 1 \\right]$. However, this introduced a bias to the agent that prevented it from learning as these values were driving the agent to choose actions that prevented exploration. In&nbsp;[14]: env = gym.make(&quot;FrozenLake-v0&quot;) num_states = env.observation_space.n num_actions = env.action_space.n agent = Agent(num_states, num_actions, alpha=0.5, gamma=0.95, epsilon_i=1.0, epsilon_f=0.0) T = 100000 s_t = env.reset() rewards = [] episode_rewards = 0 for t in range(T): if t%1000 == 0: print(f&#39;{100*t/T}%&#39;, end=&#39;\\r&#39;) a_t = agent.act(s_t) s_t_next, r_t, done, info = env.step(a_t) agent.update(s_t, a_t, s_t_next, r_t) agent.decay_epsilon() s_t = s_t_next episode_rewards += r_t if done: rewards.append(episode_rewards) episode_rewards = 0 s_t = env.reset() 99.9% We are going to plot these terminal rewards over the course of our training. We use pandas to store our data, and to perform a rolling mean calculation, and we use seaborn to plot the data. In reinforcement learning, rewards can be noisy (have high variance). To smooth out the plot of rewards over time, we calculate a rolling mean with a certain window size. In&nbsp;[15]: import pandas as pd import seaborn as sns sns.set() In&nbsp;[16]: window = 100 running_mean_rewards = pd.Series(rewards, name=&#39;q-learning&#39;).rolling(window=window).mean()[window-1:] # this slicing is because the first window-1 elements are NaN from the rolling operation. sns.lineplot(data=running_mean_rewards) Out[16]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x101f97e80&gt; It is always a good idea to ensure that your agent is performing better than random. We can essentially run the same loop, but choosing random actions each time. In&nbsp;[17]: T = 100000 s_t = env.reset() random_rewards = [] for t in range(T): a_t = env.action_space.sample() s_t, r_t, done, info = env.step(a_t) if done: random_rewards.append(r_t) s_t = env.reset() In&nbsp;[18]: random_rewards = np.asarray(random_rewards)[:len(rewards)] # make sure random_rewards is the same length as rewards. It will generally be much longer, since it will die more frequently causing more rewards to be stored given the same budget of timesteps. window = 100 running_mean_random_rewards = pd.Series(random_rewards, name=&#39;random&#39;).rolling(window=window).mean()[window-1:] In&nbsp;[19]: compare = pd.concat((running_mean_random_rewards, running_mean_rewards), axis=1) In&nbsp;[20]: sns.lineplot(data=compare) Out[20]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a108f9e80&gt; And there we have it! A fully-functional $Q$-learning agent that can perform strongly in FrozenLake-v0.","@type":"BlogPosting","url":"http://localhost:4000/q-learning/","headline":"Q Learning","dateModified":"2019-05-12T00:00:00-04:00","datePublished":"2019-05-12T00:00:00-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/q-learning/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <h1 id="project_title"> A Complete Guide to Reinforcement Learning with Tensorflow </h1>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        
        <h4>
          <a href="http://localhost:4000">Home</a>
        </h4>
        
        
        <p> Download the <a href="http://localhost:4000/assets/notebooks/q-learning.ipynb"> notebook </a> or follow along. </p>
        
        
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<h1 id="Q-learning">Q-learning<a class="anchor-link" href="#Q-learning">&#182;</a></h1><hr />
<h3 id="Value-Function">Value Function<a class="anchor-link" href="#Value-Function">&#182;</a></h3><p>Recall the definition of the <strong>value function</strong> $V(s_t)$:</p>
$$
V(s_t) = \mathbb{E} \left[ G_t \vert s_t \right]
$$<p>In an MDP, our expected discounted return $G_t$ depends on the trajectories $\tau$ that we generate. Whereas in a Markov process or Markov reward process this depends only on the underlying state dynamics, in an MDP, trajectories also depend on the actions that we choose. Since these actions are determined by our <strong>policy</strong> $\pi$, we write the following:</p>
$$
V^\pi (s_t) = \mathbb{E}_\pi \left[ G_t \vert s_t \right]
$$<p>that is, given that we are currently in state $s_t$, what value should we expect our discounted return $G_t$ to be, given that we generate trajectories according to our policy $\pi$?</p>
<hr />
<h3 id="Action-Value-function">Action-Value function<a class="anchor-link" href="#Action-Value-function">&#182;</a></h3><p>We can define an analogous definition for the <strong>action-value function</strong>, which extends the notion of the value function to account for choosing a <em>specific action</em> $a_t$ in a state $s_t$, rather than just choosing the one suggested by the policy:</p>
$$
Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ G_t \vert s_t, a_t \right]
$$<p>that is, given that we are currently in state $s_t$, taking action $a_t$, what value should we expect our discounted return $G_t$ to be? We can see the relationship between $V^\pi (s_t)$ and $Q^\pi (s_t, a_t)$:</p>
$$
V^\pi (s_t) = \mathbb{E}_{a_t \sim \pi} \left[ Q^\pi (s_t, a_t) \vert s_t \right]
$$<p>We also have a Bellman equation for the state-value function:
$$
Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ r_t + \gamma Q^\pi (s_{t+1}, a_{t+1}) \vert s_t, a_t \right]
$$</p>
<hr />
<h3 id="$Q$-learning">$Q$-learning<a class="anchor-link" href="#$Q$-learning">&#182;</a></h3><p>$Q$-learning is an algorithm analogous to the TD(0) algorithm we've described before. In TD(0), we have a table $V$ containing predictions for $V^\pi (s_t)$ for each state $s_t$, updating our predictions as follows:</p>
$$
V (s_t) \gets V (s_t) + \alpha \left( r_t + \gamma V (s_{t+1}) - V (s_t) \right)
$$<p>In $Q$-learning, we have a similar learning rule, except that our table now contains values for any state-action pair $(s_t, a_t)$.</p>
$$
Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right)
$$<p>The advantage of $Q$-learning is that we have an estimate, for each action, of the expected discounted return we will get from taking that action. Recall the reward hypothesis:</p>
<blockquote><p>Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal.</p>
</blockquote>
<p>Consider being in some state $s_t$, and for each action $a_t$ we compute $Q(s_t, a_t)$. These values are our guesses for the discounted return we will get as a result of choosing each action. If we choose the action that maximizes $Q(s_t, a_t)$, then our agent is maximizing the discounted return. This defines a <strong>greedy policy</strong></p>
$$
\pi(a_t \vert s_t) = \begin{cases} 1, &amp; a_t = \arg \max_{a_t} Q(s_t, a_t) \\ 0, &amp; \text{otherwise} \end{cases}
$$<hr />
<h5 id="$\varepsilon$-greedy">$\varepsilon$-greedy<a class="anchor-link" href="#$\varepsilon$-greedy">&#182;</a></h5><p>Our agent can quickly become stuck in a suboptimal policy by always choosing the same action in the same state. This prevents the agent from exploring, and consequently from improving its predictions about $Q$. This problem is known as the <strong>exploration-exploitation tradeoff</strong>: should we try to learn more about the $Q$ function by exploring (and thus finding a better policy) or should we exploit what we know about the environment to try to maximize the returns that we know about?</p>
<p>We use a parameter $\varepsilon$ that determines with what probability we choose a random action versus choosing an action according to the greedy policy. We generate a random number $p \in \left[ 0, 1 \right]$. If $p &lt; \varepsilon$ then we choose a random action, and if $p \geq \varepsilon$ then we choose an action according to the greedy policy.</p>
<p>Initially, our guesses about $Q(s_t, a_t)$ are poor because they are randomly intialized. Thus, we should not use them to guide our policy at first. Therefore, initially $\varepsilon$ should be high (closer to 1). Over time, the agent improves its guesses about $Q(s_t, a_t)$ and so it becomes more reliable in our greedy policy.</p>
<p>A simple idea is to have some initial value $\varepsilon_i$ and $\varepsilon_f$ for the initial and final values of $\varepsilon$, and some number of timesteps $n_\varepsilon$ over which $\varepsilon_i$ decays to $\varepsilon_f$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">gym</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> 
                 <span class="n">epsilon_i</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
                 <span class="n">epsilon_f</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> 
                 <span class="n">n_epsilon</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> 
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">=</span> <span class="n">epsilon_i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span> <span class="o">=</span> <span class="n">epsilon_f</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon_i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_epsilon</span> <span class="o">=</span> <span class="n">n_epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_states</span> <span class="o">=</span> <span class="n">num_states</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">num_actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">decay_epsilon</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span><span class="p">,</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">n_epsilon</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">r_t</span><span class="p">):</span>
        <span class="n">a_t_next</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s_t_next</span><span class="p">)</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">]</span> <span class="o">+</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">r_t</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t_next</span><span class="p">,</span> <span class="n">a_t_next</span><span class="p">]</span> <span class="o">-</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A quick note: <strong>it is extremely important that you initialize your $Q$-table to zeros</strong>. Initially, I had it set to random values in the range $\left[ 0, 1 \right]$. However, this introduced a bias to the agent that prevented it from learning as these values were driving the agent to choose actions that prevented exploration.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;FrozenLake-v0&quot;</span><span class="p">)</span>
<span class="n">num_states</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
<span class="n">num_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">epsilon_i</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon_f</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">episode_rewards</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">t</span><span class="o">%</span><span class="k">1000</span> == 0:
        <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;{100*t/T}%&#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\r</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">a_t</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s_t</span><span class="p">)</span>
    <span class="n">s_t_next</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a_t</span><span class="p">)</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">r_t</span><span class="p">)</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">decay_epsilon</span><span class="p">()</span>
    <span class="n">s_t</span> <span class="o">=</span> <span class="n">s_t_next</span>
    <span class="n">episode_rewards</span> <span class="o">+=</span> <span class="n">r_t</span>
    
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">)</span>
        <span class="n">episode_rewards</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>99.9%
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We are going to plot these terminal rewards over the course of our training. We use <code>pandas</code> to store our data, and to perform a rolling mean calculation, and we use <code>seaborn</code> to plot the data.</p>
<p>In reinforcement learning, rewards can be noisy (have high variance). To smooth out the plot of rewards over time, we calculate a <em>rolling mean</em> with a certain window size.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">window</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">running_mean_rewards</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;q-learning&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="c1"># this slicing is because the first window-1 elements are NaN from the rolling operation.</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">running_mean_rewards</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[16]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x101f97e80&gt;</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is <em>always</em> a good idea to ensure that your agent is performing better than random. We can essentially run the same loop, but choosing random actions each time.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">T</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="n">random_rewards</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="n">a_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">s_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a_t</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">random_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r_t</span><span class="p">)</span>
        <span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">random_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">random_rewards</span><span class="p">)[:</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)]</span> <span class="c1"># make sure random_rewards is the same length as rewards. It will generally be much longer, since it will die more frequently causing more rewards to be stored given the same budget of timesteps.</span>
<span class="n">window</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">running_mean_random_rewards</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">random_rewards</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">compare</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">running_mean_random_rewards</span><span class="p">,</span> <span class="n">running_mean_rewards</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">compare</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[20]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a108f9e80&gt;</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And there we have it! A fully-functional $Q$-learning agent that can perform strongly in <code>FrozenLake-v0</code>.</p>

</div>
</div>
</div>


        
        <h4>
          <a href="http://localhost:4000">Home</a>
        </h4>
        
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">alexandervandekleut.github.io maintained by <a href="https://github.com/alexandervandekleut">alexandervandekleut</a></p>
        
      </footer>
    </div>

    
  </body>
</html>
