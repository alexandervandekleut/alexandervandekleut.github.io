<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" type="text/css" media="screen" href="/assets/css/style.css?v=12678b22e5b360f66dc6229ba31f56b3542a7310">
    <!-- Mathjax Support -->
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>$Q$-learning | alexandervandekleut.github.io</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="$Q$-learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Q-learning&#182; Value Function&#182;Recall the definition of the value function $V(s_t)$: $$ V(s_t) = \mathbb{E} \left[ G_t \vert s_t \right] $$In an MDP, our expected discounted return $G_t$ depends on the trajectories $\tau$ that we generate. Whereas in a Markov process or Markov reward process this depends only on the underlying state dynamics, in an MDP, trajectories also depend on the actions that we choose. Since these actions are determined by our policy $\pi$, we write the following: $$ V^\pi (s_t) = \mathbb{E}_\pi \left[ G_t \vert s_t \right] $$that is, given that we are currently in state $s_t$, what value should we expect our discounted return $G_t$ to be, given that we generate trajectories according to our policy $\pi$? Action-Value function&#182;We can define an analogous definition for the action-value function, which extends the notion of the value function to account for choosing a specific action $a_t$ in a state $s_t$, rather than just choosing the one suggested by the policy: $$ Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ G_t \vert s_t, a_t \right] $$that is, given that we are currently in state $s_t$, taking action $a_t$, what value should we expect our discounted return $G_t$ to be? We can see the relationship between $V^\pi (s_t)$ and $Q^\pi (s_t, a_t)$: $$ V^\pi (s_t) = \mathbb{E}_{a_t \sim \pi} \left[ Q^\pi (s_t, a_t) \vert s_t \right] $$We also have a Bellman equation for the state-value function: $$ Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ r_t + \gamma Q^\pi (s_{t+1}, a_{t+1}) \vert s_t, a_t \right] $$ $Q$-learning&#182;$Q$-learning is an algorithm analogous to the TD(0) algorithm we&#39;ve described before. In TD(0), we have a table $V$ containing predictions for $V^\pi (s_t)$ for each state $s_t$, updating our predictions as follows: $$ V (s_t) \gets V (s_t) + \alpha \left( r_t + \gamma V (s_{t+1}) - V (s_t) \right) $$In $Q$-learning, we have a similar learning rule, except that our table now contains values for any state-action pair $(s_t, a_t)$. $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right) $$The advantage of $Q$-learning is that we have an estimate, for each action, of the expected discounted return we will get from taking that action. Recall the reward hypothesis: Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal. Consider being in some state $s_t$, and for each action $a_t$ we compute $Q(s_t, a_t)$. These values are our guesses for the discounted return we will get as a result of choosing each action. If we choose the action that maximizes $Q(s_t, a_t)$, then our agent is maximizing the discounted return. This defines a greedy policy $$ \pi(a_t \vert s_t) = \begin{cases} 1, &amp; a_t = \arg \max_{a_t} Q(s_t, a_t) \\ 0, &amp; \text{otherwise} \end{cases} $$ $\varepsilon$-greedy&#182;Our agent can quickly become stuck in a suboptimal policy by always choosing the same action in the same state. This prevents the agent from exploring, and consequently from improving its predictions about $Q$. This problem is known as the exploration-exploitation tradeoff: should we try to learn more about the $Q$ function by exploring (and thus finding a better policy) or should we exploit what we know about the environment to try to maximize the returns that we know about? We use a parameter $\varepsilon$ that determines with what probability we choose a random action versus choosing an action according to the greedy policy. We generate a random number $p \in \left[ 0, 1 \right]$. If $p &lt; \varepsilon$ then we choose a random action, and if $p \geq \varepsilon$ then we choose an action according to the greedy policy. Initially, our guesses about $Q(s_t, a_t)$ are poor because they are randomly intialized. Thus, we should not use them to guide our policy at first. Therefore, initially $\varepsilon$ should be high (closer to 1). Over time, the agent improves its guesses about $Q(s_t, a_t)$ and so it becomes more reliable in our greedy policy. A simple idea is to have some initial value $\varepsilon_i$ and $\varepsilon_f$ for the initial and final values of $\varepsilon$, and some number of timesteps $n_\varepsilon$ over which $\varepsilon_i$ decays to $\varepsilon_f$. In&nbsp;[1]: import numpy as np import gym In&nbsp;[2]: class Agent: def __init__(self, num_states, num_actions, epsilon_i=1.0, epsilon_f=0.0, n_epsilon=10000, alpha=0.1, gamma = 0.99): self.epsilon_i = epsilon_i # initial epsilon value self.epsilon_f = epsilon_f # final epsilon value self.epsilon = epsilon_i # current epsilon value self.n_epsilon = n_epsilon # number of timesteps over which to decay epsilon self.num_states = num_states self.num_actions = num_actions self.alpha = alpha # learning rate self.gamma = gamma # discount factor # self.Q = np.random.rand(num_states, num_actions) # Q-table self.Q = np.zeros((num_states, num_actions)) # Q-table. def decay_epsilon(self): self.epsilon = max(self.epsilon_f, self.epsilon - (self.epsilon_i - self.epsilon_f)/self.n_epsilon) # decrement epsilon by (epsilon_i - epsilon_f)/n_epsilon if we have further to go, if not just set to epsilon_f def act(self, s_t): if np.random.rand() &lt; self.epsilon: # epsilon-greedy return np.random.randint(self.num_actions) # random action return np.argmax(self.Q[s_t]) # greedy policy def update(self, s_t, a_t, s_t_next, r_t): a_t_next = self.act(s_t_next) self.Q[s_t, a_t] = self.Q[s_t, a_t] + self.alpha*(r_t + self.Q[s_t_next, a_t_next] - self.Q[s_t, a_t]) # update according to q-learning rule A quick note: it is extremely important that you initialize your $Q$-table to zeros. Initially, I had it set to random values in the range $\left[ 0, 1 \right]$. However, this introduced a bias to the agent that prevented it from learning as these values were driving the agent to choose actions that prevented exploration. In&nbsp;[3]: env = gym.make(&quot;FrozenLake-v0&quot;) In&nbsp;[4]: num_states = env.observation_space.n num_actions = env.action_space.n agent = Agent(num_states, num_actions) We made a default agent that starts with a random policy ($\varepsilon_i = 1$) and decays to a deterministic policy ($\varepsilon = 0$) over $n_\varepsilon = 10000$ timesteps. The agent uses a learning rate $\alpha = 0.1$ and discount factor $\gamma = 0.99$. Let&#39;s train the agent for $100000$ timesteps. According to the FrozenLake-v0 documentation, rewards are $0$ everywhere except at the goal where they are $1$. The episode can be terminal if we fall in a hole or if we reach the goal state. If we keep track of the reward we got at a terminal state, we can determine if we fell in a hole or got to the goal. In&nbsp;[5]: T = 100000 s_t = env.reset() rewards = [] for t in range(T): a_t = agent.act(s_t) s_t_next, r_t, done, info = env.step(a_t) agent.update(s_t, a_t, s_t_next, r_t) agent.decay_epsilon() s_t = s_t_next if done: rewards.append(r_t) s_t = env.reset() We are going to plot these terminal rewards over the course of our training. We use pandas to store our data, and to perform a rolling mean calculation, and we use seaborn to plot the data. In reinforcement learning, rewards can be noisy (have high variance). To smooth out the plot of rewards over time, we calculate a rolling mean with a certain window size. In&nbsp;[16]: import pandas as pd import seaborn as sns sns.set() In&nbsp;[17]: rewards = np.asarray(rewards) window = 100 running_mean_rewards = pd.Series(rewards, name=&#39;q-learning&#39;).rolling(window=window).mean()[window-1:] # this slicing is because the first window-1 elements are NaN from the rolling operation. sns.lineplot(data=running_mean_rewards) Out[17]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1bd0bc88&gt; It is always a good idea to ensure that your agent is performing better than random. We can essentially run the same loop, but choosing random actions each time. In&nbsp;[18]: T = 100000 s_t = env.reset() random_rewards = [] for t in range(T): a_t = env.action_space.sample() s_t, r_t, done, info = env.step(a_t) if done: random_rewards.append(r_t) s_t = env.reset() In&nbsp;[19]: random_rewards = np.asarray(random_rewards)[:len(rewards)] # make sure random_rewards is the same length as rewards. It will generally be much longer, since it will die more frequently causing more rewards to be stored given the same budget of timesteps. window = 100 running_mean_random_rewards = pd.Series(random_rewards, name=&#39;random&#39;).rolling(window=window).mean()[window-1:] In&nbsp;[20]: compare = pd.concat((running_mean_random_rewards, running_mean_rewards), axis=1) # generate one dataframe holding the terminal rewards from the q-learning agent and the random agent. In&nbsp;[21]: sns.lineplot(data=compare) Out[21]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1bd70080&gt; And there we have it! A fully-functional $Q$-learning agent that can perform strongly in FrozenLake-v0." />
<meta property="og:description" content="Q-learning&#182; Value Function&#182;Recall the definition of the value function $V(s_t)$: $$ V(s_t) = \mathbb{E} \left[ G_t \vert s_t \right] $$In an MDP, our expected discounted return $G_t$ depends on the trajectories $\tau$ that we generate. Whereas in a Markov process or Markov reward process this depends only on the underlying state dynamics, in an MDP, trajectories also depend on the actions that we choose. Since these actions are determined by our policy $\pi$, we write the following: $$ V^\pi (s_t) = \mathbb{E}_\pi \left[ G_t \vert s_t \right] $$that is, given that we are currently in state $s_t$, what value should we expect our discounted return $G_t$ to be, given that we generate trajectories according to our policy $\pi$? Action-Value function&#182;We can define an analogous definition for the action-value function, which extends the notion of the value function to account for choosing a specific action $a_t$ in a state $s_t$, rather than just choosing the one suggested by the policy: $$ Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ G_t \vert s_t, a_t \right] $$that is, given that we are currently in state $s_t$, taking action $a_t$, what value should we expect our discounted return $G_t$ to be? We can see the relationship between $V^\pi (s_t)$ and $Q^\pi (s_t, a_t)$: $$ V^\pi (s_t) = \mathbb{E}_{a_t \sim \pi} \left[ Q^\pi (s_t, a_t) \vert s_t \right] $$We also have a Bellman equation for the state-value function: $$ Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ r_t + \gamma Q^\pi (s_{t+1}, a_{t+1}) \vert s_t, a_t \right] $$ $Q$-learning&#182;$Q$-learning is an algorithm analogous to the TD(0) algorithm we&#39;ve described before. In TD(0), we have a table $V$ containing predictions for $V^\pi (s_t)$ for each state $s_t$, updating our predictions as follows: $$ V (s_t) \gets V (s_t) + \alpha \left( r_t + \gamma V (s_{t+1}) - V (s_t) \right) $$In $Q$-learning, we have a similar learning rule, except that our table now contains values for any state-action pair $(s_t, a_t)$. $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right) $$The advantage of $Q$-learning is that we have an estimate, for each action, of the expected discounted return we will get from taking that action. Recall the reward hypothesis: Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal. Consider being in some state $s_t$, and for each action $a_t$ we compute $Q(s_t, a_t)$. These values are our guesses for the discounted return we will get as a result of choosing each action. If we choose the action that maximizes $Q(s_t, a_t)$, then our agent is maximizing the discounted return. This defines a greedy policy $$ \pi(a_t \vert s_t) = \begin{cases} 1, &amp; a_t = \arg \max_{a_t} Q(s_t, a_t) \\ 0, &amp; \text{otherwise} \end{cases} $$ $\varepsilon$-greedy&#182;Our agent can quickly become stuck in a suboptimal policy by always choosing the same action in the same state. This prevents the agent from exploring, and consequently from improving its predictions about $Q$. This problem is known as the exploration-exploitation tradeoff: should we try to learn more about the $Q$ function by exploring (and thus finding a better policy) or should we exploit what we know about the environment to try to maximize the returns that we know about? We use a parameter $\varepsilon$ that determines with what probability we choose a random action versus choosing an action according to the greedy policy. We generate a random number $p \in \left[ 0, 1 \right]$. If $p &lt; \varepsilon$ then we choose a random action, and if $p \geq \varepsilon$ then we choose an action according to the greedy policy. Initially, our guesses about $Q(s_t, a_t)$ are poor because they are randomly intialized. Thus, we should not use them to guide our policy at first. Therefore, initially $\varepsilon$ should be high (closer to 1). Over time, the agent improves its guesses about $Q(s_t, a_t)$ and so it becomes more reliable in our greedy policy. A simple idea is to have some initial value $\varepsilon_i$ and $\varepsilon_f$ for the initial and final values of $\varepsilon$, and some number of timesteps $n_\varepsilon$ over which $\varepsilon_i$ decays to $\varepsilon_f$. In&nbsp;[1]: import numpy as np import gym In&nbsp;[2]: class Agent: def __init__(self, num_states, num_actions, epsilon_i=1.0, epsilon_f=0.0, n_epsilon=10000, alpha=0.1, gamma = 0.99): self.epsilon_i = epsilon_i # initial epsilon value self.epsilon_f = epsilon_f # final epsilon value self.epsilon = epsilon_i # current epsilon value self.n_epsilon = n_epsilon # number of timesteps over which to decay epsilon self.num_states = num_states self.num_actions = num_actions self.alpha = alpha # learning rate self.gamma = gamma # discount factor # self.Q = np.random.rand(num_states, num_actions) # Q-table self.Q = np.zeros((num_states, num_actions)) # Q-table. def decay_epsilon(self): self.epsilon = max(self.epsilon_f, self.epsilon - (self.epsilon_i - self.epsilon_f)/self.n_epsilon) # decrement epsilon by (epsilon_i - epsilon_f)/n_epsilon if we have further to go, if not just set to epsilon_f def act(self, s_t): if np.random.rand() &lt; self.epsilon: # epsilon-greedy return np.random.randint(self.num_actions) # random action return np.argmax(self.Q[s_t]) # greedy policy def update(self, s_t, a_t, s_t_next, r_t): a_t_next = self.act(s_t_next) self.Q[s_t, a_t] = self.Q[s_t, a_t] + self.alpha*(r_t + self.Q[s_t_next, a_t_next] - self.Q[s_t, a_t]) # update according to q-learning rule A quick note: it is extremely important that you initialize your $Q$-table to zeros. Initially, I had it set to random values in the range $\left[ 0, 1 \right]$. However, this introduced a bias to the agent that prevented it from learning as these values were driving the agent to choose actions that prevented exploration. In&nbsp;[3]: env = gym.make(&quot;FrozenLake-v0&quot;) In&nbsp;[4]: num_states = env.observation_space.n num_actions = env.action_space.n agent = Agent(num_states, num_actions) We made a default agent that starts with a random policy ($\varepsilon_i = 1$) and decays to a deterministic policy ($\varepsilon = 0$) over $n_\varepsilon = 10000$ timesteps. The agent uses a learning rate $\alpha = 0.1$ and discount factor $\gamma = 0.99$. Let&#39;s train the agent for $100000$ timesteps. According to the FrozenLake-v0 documentation, rewards are $0$ everywhere except at the goal where they are $1$. The episode can be terminal if we fall in a hole or if we reach the goal state. If we keep track of the reward we got at a terminal state, we can determine if we fell in a hole or got to the goal. In&nbsp;[5]: T = 100000 s_t = env.reset() rewards = [] for t in range(T): a_t = agent.act(s_t) s_t_next, r_t, done, info = env.step(a_t) agent.update(s_t, a_t, s_t_next, r_t) agent.decay_epsilon() s_t = s_t_next if done: rewards.append(r_t) s_t = env.reset() We are going to plot these terminal rewards over the course of our training. We use pandas to store our data, and to perform a rolling mean calculation, and we use seaborn to plot the data. In reinforcement learning, rewards can be noisy (have high variance). To smooth out the plot of rewards over time, we calculate a rolling mean with a certain window size. In&nbsp;[16]: import pandas as pd import seaborn as sns sns.set() In&nbsp;[17]: rewards = np.asarray(rewards) window = 100 running_mean_rewards = pd.Series(rewards, name=&#39;q-learning&#39;).rolling(window=window).mean()[window-1:] # this slicing is because the first window-1 elements are NaN from the rolling operation. sns.lineplot(data=running_mean_rewards) Out[17]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1bd0bc88&gt; It is always a good idea to ensure that your agent is performing better than random. We can essentially run the same loop, but choosing random actions each time. In&nbsp;[18]: T = 100000 s_t = env.reset() random_rewards = [] for t in range(T): a_t = env.action_space.sample() s_t, r_t, done, info = env.step(a_t) if done: random_rewards.append(r_t) s_t = env.reset() In&nbsp;[19]: random_rewards = np.asarray(random_rewards)[:len(rewards)] # make sure random_rewards is the same length as rewards. It will generally be much longer, since it will die more frequently causing more rewards to be stored given the same budget of timesteps. window = 100 running_mean_random_rewards = pd.Series(random_rewards, name=&#39;random&#39;).rolling(window=window).mean()[window-1:] In&nbsp;[20]: compare = pd.concat((running_mean_random_rewards, running_mean_rewards), axis=1) # generate one dataframe holding the terminal rewards from the q-learning agent and the random agent. In&nbsp;[21]: sns.lineplot(data=compare) Out[21]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1bd70080&gt; And there we have it! A fully-functional $Q$-learning agent that can perform strongly in FrozenLake-v0." />
<link rel="canonical" href="http://localhost:4000/q-learning/" />
<meta property="og:url" content="http://localhost:4000/q-learning/" />
<meta property="og:site_name" content="alexandervandekleut.github.io" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-12T00:00:00-04:00" />
<script type="application/ld+json">
{"description":"Q-learning&#182; Value Function&#182;Recall the definition of the value function $V(s_t)$: $$ V(s_t) = \\mathbb{E} \\left[ G_t \\vert s_t \\right] $$In an MDP, our expected discounted return $G_t$ depends on the trajectories $\\tau$ that we generate. Whereas in a Markov process or Markov reward process this depends only on the underlying state dynamics, in an MDP, trajectories also depend on the actions that we choose. Since these actions are determined by our policy $\\pi$, we write the following: $$ V^\\pi (s_t) = \\mathbb{E}_\\pi \\left[ G_t \\vert s_t \\right] $$that is, given that we are currently in state $s_t$, what value should we expect our discounted return $G_t$ to be, given that we generate trajectories according to our policy $\\pi$? Action-Value function&#182;We can define an analogous definition for the action-value function, which extends the notion of the value function to account for choosing a specific action $a_t$ in a state $s_t$, rather than just choosing the one suggested by the policy: $$ Q^\\pi (s_t, a_t) = \\mathbb{E}_\\pi \\left[ G_t \\vert s_t, a_t \\right] $$that is, given that we are currently in state $s_t$, taking action $a_t$, what value should we expect our discounted return $G_t$ to be? We can see the relationship between $V^\\pi (s_t)$ and $Q^\\pi (s_t, a_t)$: $$ V^\\pi (s_t) = \\mathbb{E}_{a_t \\sim \\pi} \\left[ Q^\\pi (s_t, a_t) \\vert s_t \\right] $$We also have a Bellman equation for the state-value function: $$ Q^\\pi (s_t, a_t) = \\mathbb{E}_\\pi \\left[ r_t + \\gamma Q^\\pi (s_{t+1}, a_{t+1}) \\vert s_t, a_t \\right] $$ $Q$-learning&#182;$Q$-learning is an algorithm analogous to the TD(0) algorithm we&#39;ve described before. In TD(0), we have a table $V$ containing predictions for $V^\\pi (s_t)$ for each state $s_t$, updating our predictions as follows: $$ V (s_t) \\gets V (s_t) + \\alpha \\left( r_t + \\gamma V (s_{t+1}) - V (s_t) \\right) $$In $Q$-learning, we have a similar learning rule, except that our table now contains values for any state-action pair $(s_t, a_t)$. $$ Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha \\left( r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right) $$The advantage of $Q$-learning is that we have an estimate, for each action, of the expected discounted return we will get from taking that action. Recall the reward hypothesis: Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal. Consider being in some state $s_t$, and for each action $a_t$ we compute $Q(s_t, a_t)$. These values are our guesses for the discounted return we will get as a result of choosing each action. If we choose the action that maximizes $Q(s_t, a_t)$, then our agent is maximizing the discounted return. This defines a greedy policy $$ \\pi(a_t \\vert s_t) = \\begin{cases} 1, &amp; a_t = \\arg \\max_{a_t} Q(s_t, a_t) \\\\ 0, &amp; \\text{otherwise} \\end{cases} $$ $\\varepsilon$-greedy&#182;Our agent can quickly become stuck in a suboptimal policy by always choosing the same action in the same state. This prevents the agent from exploring, and consequently from improving its predictions about $Q$. This problem is known as the exploration-exploitation tradeoff: should we try to learn more about the $Q$ function by exploring (and thus finding a better policy) or should we exploit what we know about the environment to try to maximize the returns that we know about? We use a parameter $\\varepsilon$ that determines with what probability we choose a random action versus choosing an action according to the greedy policy. We generate a random number $p \\in \\left[ 0, 1 \\right]$. If $p &lt; \\varepsilon$ then we choose a random action, and if $p \\geq \\varepsilon$ then we choose an action according to the greedy policy. Initially, our guesses about $Q(s_t, a_t)$ are poor because they are randomly intialized. Thus, we should not use them to guide our policy at first. Therefore, initially $\\varepsilon$ should be high (closer to 1). Over time, the agent improves its guesses about $Q(s_t, a_t)$ and so it becomes more reliable in our greedy policy. A simple idea is to have some initial value $\\varepsilon_i$ and $\\varepsilon_f$ for the initial and final values of $\\varepsilon$, and some number of timesteps $n_\\varepsilon$ over which $\\varepsilon_i$ decays to $\\varepsilon_f$. In&nbsp;[1]: import numpy as np import gym In&nbsp;[2]: class Agent: def __init__(self, num_states, num_actions, epsilon_i=1.0, epsilon_f=0.0, n_epsilon=10000, alpha=0.1, gamma = 0.99): self.epsilon_i = epsilon_i # initial epsilon value self.epsilon_f = epsilon_f # final epsilon value self.epsilon = epsilon_i # current epsilon value self.n_epsilon = n_epsilon # number of timesteps over which to decay epsilon self.num_states = num_states self.num_actions = num_actions self.alpha = alpha # learning rate self.gamma = gamma # discount factor # self.Q = np.random.rand(num_states, num_actions) # Q-table self.Q = np.zeros((num_states, num_actions)) # Q-table. def decay_epsilon(self): self.epsilon = max(self.epsilon_f, self.epsilon - (self.epsilon_i - self.epsilon_f)/self.n_epsilon) # decrement epsilon by (epsilon_i - epsilon_f)/n_epsilon if we have further to go, if not just set to epsilon_f def act(self, s_t): if np.random.rand() &lt; self.epsilon: # epsilon-greedy return np.random.randint(self.num_actions) # random action return np.argmax(self.Q[s_t]) # greedy policy def update(self, s_t, a_t, s_t_next, r_t): a_t_next = self.act(s_t_next) self.Q[s_t, a_t] = self.Q[s_t, a_t] + self.alpha*(r_t + self.Q[s_t_next, a_t_next] - self.Q[s_t, a_t]) # update according to q-learning rule A quick note: it is extremely important that you initialize your $Q$-table to zeros. Initially, I had it set to random values in the range $\\left[ 0, 1 \\right]$. However, this introduced a bias to the agent that prevented it from learning as these values were driving the agent to choose actions that prevented exploration. In&nbsp;[3]: env = gym.make(&quot;FrozenLake-v0&quot;) In&nbsp;[4]: num_states = env.observation_space.n num_actions = env.action_space.n agent = Agent(num_states, num_actions) We made a default agent that starts with a random policy ($\\varepsilon_i = 1$) and decays to a deterministic policy ($\\varepsilon = 0$) over $n_\\varepsilon = 10000$ timesteps. The agent uses a learning rate $\\alpha = 0.1$ and discount factor $\\gamma = 0.99$. Let&#39;s train the agent for $100000$ timesteps. According to the FrozenLake-v0 documentation, rewards are $0$ everywhere except at the goal where they are $1$. The episode can be terminal if we fall in a hole or if we reach the goal state. If we keep track of the reward we got at a terminal state, we can determine if we fell in a hole or got to the goal. In&nbsp;[5]: T = 100000 s_t = env.reset() rewards = [] for t in range(T): a_t = agent.act(s_t) s_t_next, r_t, done, info = env.step(a_t) agent.update(s_t, a_t, s_t_next, r_t) agent.decay_epsilon() s_t = s_t_next if done: rewards.append(r_t) s_t = env.reset() We are going to plot these terminal rewards over the course of our training. We use pandas to store our data, and to perform a rolling mean calculation, and we use seaborn to plot the data. In reinforcement learning, rewards can be noisy (have high variance). To smooth out the plot of rewards over time, we calculate a rolling mean with a certain window size. In&nbsp;[16]: import pandas as pd import seaborn as sns sns.set() In&nbsp;[17]: rewards = np.asarray(rewards) window = 100 running_mean_rewards = pd.Series(rewards, name=&#39;q-learning&#39;).rolling(window=window).mean()[window-1:] # this slicing is because the first window-1 elements are NaN from the rolling operation. sns.lineplot(data=running_mean_rewards) Out[17]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1bd0bc88&gt; It is always a good idea to ensure that your agent is performing better than random. We can essentially run the same loop, but choosing random actions each time. In&nbsp;[18]: T = 100000 s_t = env.reset() random_rewards = [] for t in range(T): a_t = env.action_space.sample() s_t, r_t, done, info = env.step(a_t) if done: random_rewards.append(r_t) s_t = env.reset() In&nbsp;[19]: random_rewards = np.asarray(random_rewards)[:len(rewards)] # make sure random_rewards is the same length as rewards. It will generally be much longer, since it will die more frequently causing more rewards to be stored given the same budget of timesteps. window = 100 running_mean_random_rewards = pd.Series(random_rewards, name=&#39;random&#39;).rolling(window=window).mean()[window-1:] In&nbsp;[20]: compare = pd.concat((running_mean_random_rewards, running_mean_rewards), axis=1) # generate one dataframe holding the terminal rewards from the q-learning agent and the random agent. In&nbsp;[21]: sns.lineplot(data=compare) Out[21]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1bd70080&gt; And there we have it! A fully-functional $Q$-learning agent that can perform strongly in FrozenLake-v0.","@type":"BlogPosting","url":"http://localhost:4000/q-learning/","headline":"$Q$-learning","dateModified":"2019-05-12T00:00:00-04:00","datePublished":"2019-05-12T00:00:00-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/q-learning/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <h1 id="project_title"> A Complete Guide to Reinforcement Learning with Tensorflow </h1>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        
        <h4>
          <a href="http://localhost:4000">Home</a>
        </h4>
        
        
        
        <p> Download the <a href="http://localhost:4000/assets/notebooks/q-learning.ipynb"> notebook </a> or follow along. </p>
        
        
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<h1 id="Q-learning">Q-learning<a class="anchor-link" href="#Q-learning">&#182;</a></h1><hr />
<h3 id="Value-Function">Value Function<a class="anchor-link" href="#Value-Function">&#182;</a></h3><p>Recall the definition of the <strong>value function</strong> $V(s_t)$:</p>
$$
V(s_t) = \mathbb{E} \left[ G_t \vert s_t \right]
$$<p>In an MDP, our expected discounted return $G_t$ depends on the trajectories $\tau$ that we generate. Whereas in a Markov process or Markov reward process this depends only on the underlying state dynamics, in an MDP, trajectories also depend on the actions that we choose. Since these actions are determined by our <strong>policy</strong> $\pi$, we write the following:</p>
$$
V^\pi (s_t) = \mathbb{E}_\pi \left[ G_t \vert s_t \right]
$$<p>that is, given that we are currently in state $s_t$, what value should we expect our discounted return $G_t$ to be, given that we generate trajectories according to our policy $\pi$?</p>
<hr />
<h3 id="Action-Value-function">Action-Value function<a class="anchor-link" href="#Action-Value-function">&#182;</a></h3><p>We can define an analogous definition for the <strong>action-value function</strong>, which extends the notion of the value function to account for choosing a <em>specific action</em> $a_t$ in a state $s_t$, rather than just choosing the one suggested by the policy:</p>
$$
Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ G_t \vert s_t, a_t \right]
$$<p>that is, given that we are currently in state $s_t$, taking action $a_t$, what value should we expect our discounted return $G_t$ to be? We can see the relationship between $V^\pi (s_t)$ and $Q^\pi (s_t, a_t)$:</p>
$$
V^\pi (s_t) = \mathbb{E}_{a_t \sim \pi} \left[ Q^\pi (s_t, a_t) \vert s_t \right]
$$<p>We also have a Bellman equation for the state-value function:
$$
Q^\pi (s_t, a_t) = \mathbb{E}_\pi \left[ r_t + \gamma Q^\pi (s_{t+1}, a_{t+1}) \vert s_t, a_t \right]
$$</p>
<hr />
<h3 id="$Q$-learning">$Q$-learning<a class="anchor-link" href="#$Q$-learning">&#182;</a></h3><p>$Q$-learning is an algorithm analogous to the TD(0) algorithm we've described before. In TD(0), we have a table $V$ containing predictions for $V^\pi (s_t)$ for each state $s_t$, updating our predictions as follows:</p>
$$
V (s_t) \gets V (s_t) + \alpha \left( r_t + \gamma V (s_{t+1}) - V (s_t) \right)
$$<p>In $Q$-learning, we have a similar learning rule, except that our table now contains values for any state-action pair $(s_t, a_t)$.</p>
$$
Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right)
$$<p>The advantage of $Q$-learning is that we have an estimate, for each action, of the expected discounted return we will get from taking that action. Recall the reward hypothesis:</p>
<blockquote><p>Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal.</p>
</blockquote>
<p>Consider being in some state $s_t$, and for each action $a_t$ we compute $Q(s_t, a_t)$. These values are our guesses for the discounted return we will get as a result of choosing each action. If we choose the action that maximizes $Q(s_t, a_t)$, then our agent is maximizing the discounted return. This defines a <strong>greedy policy</strong></p>
$$
\pi(a_t \vert s_t) = \begin{cases} 1, &amp; a_t = \arg \max_{a_t} Q(s_t, a_t) \\ 0, &amp; \text{otherwise} \end{cases}
$$<hr />
<h5 id="$\varepsilon$-greedy">$\varepsilon$-greedy<a class="anchor-link" href="#$\varepsilon$-greedy">&#182;</a></h5><p>Our agent can quickly become stuck in a suboptimal policy by always choosing the same action in the same state. This prevents the agent from exploring, and consequently from improving its predictions about $Q$. This problem is known as the <strong>exploration-exploitation tradeoff</strong>: should we try to learn more about the $Q$ function by exploring (and thus finding a better policy) or should we exploit what we know about the environment to try to maximize the returns that we know about?</p>
<p>We use a parameter $\varepsilon$ that determines with what probability we choose a random action versus choosing an action according to the greedy policy. We generate a random number $p \in \left[ 0, 1 \right]$. If $p &lt; \varepsilon$ then we choose a random action, and if $p \geq \varepsilon$ then we choose an action according to the greedy policy.</p>
<p>Initially, our guesses about $Q(s_t, a_t)$ are poor because they are randomly intialized. Thus, we should not use them to guide our policy at first. Therefore, initially $\varepsilon$ should be high (closer to 1). Over time, the agent improves its guesses about $Q(s_t, a_t)$ and so it becomes more reliable in our greedy policy.</p>
<p>A simple idea is to have some initial value $\varepsilon_i$ and $\varepsilon_f$ for the initial and final values of $\varepsilon$, and some number of timesteps $n_\varepsilon$ over which $\varepsilon_i$ decays to $\varepsilon_f$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">gym</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> <span class="n">epsilon_i</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon_f</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">n_epsilon</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">=</span> <span class="n">epsilon_i</span> <span class="c1"># initial epsilon value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span> <span class="o">=</span> <span class="n">epsilon_f</span> <span class="c1"># final epsilon value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon_i</span> <span class="c1"># current epsilon value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_epsilon</span> <span class="o">=</span> <span class="n">n_epsilon</span> <span class="c1"># number of timesteps over which to decay epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_states</span> <span class="o">=</span> <span class="n">num_states</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">num_actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="c1"># learning rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span> <span class="c1"># discount factor</span>
<span class="c1">#         self.Q = np.random.rand(num_states, num_actions) # Q-table</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">))</span> <span class="c1"># Q-table.</span>

    <span class="k">def</span> <span class="nf">decay_epsilon</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">n_epsilon</span><span class="p">)</span> <span class="c1"># decrement epsilon by (epsilon_i - epsilon_f)/n_epsilon if we have further to go, if not just set to epsilon_f</span>
    
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span> <span class="c1"># epsilon-greedy</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">)</span> <span class="c1"># random action</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">])</span> <span class="c1"># greedy policy</span>
    
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">r_t</span><span class="p">):</span>
        <span class="n">a_t_next</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s_t_next</span><span class="p">)</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">r_t</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t_next</span><span class="p">,</span> <span class="n">a_t_next</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">])</span> <span class="c1"># update according to q-learning rule</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A quick note: <strong>it is extremely important that you initialize your $Q$-table to zeros</strong>. Initially, I had it set to random values in the range $\left[ 0, 1 \right]$. However, this introduced a bias to the agent that prevented it from learning as these values were driving the agent to choose actions that prevented exploration.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;FrozenLake-v0&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_states</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
<span class="n">num_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We made a default agent that starts with a random policy ($\varepsilon_i = 1$) and decays to a deterministic policy ($\varepsilon = 0$) over $n_\varepsilon = 10000$ timesteps. The agent uses a learning rate $\alpha = 0.1$ and discount factor $\gamma = 0.99$. Let's train the agent for $100000$ timesteps. According to the <a href="https://gym.openai.com/envs/FrozenLake-v0/"><code>FrozenLake-v0</code> documentation</a>, rewards are $0$ everywhere except at the goal where they are $1$. The episode can be terminal if we fall in a hole or if we reach the goal state. If we keep track of the reward we got at a terminal state, we can determine if we fell in a hole or got to the goal.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">T</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="n">a_t</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s_t</span><span class="p">)</span>
    <span class="n">s_t_next</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a_t</span><span class="p">)</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">r_t</span><span class="p">)</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">decay_epsilon</span><span class="p">()</span>
    <span class="n">s_t</span> <span class="o">=</span> <span class="n">s_t_next</span>
    
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r_t</span><span class="p">)</span>
        <span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We are going to plot these terminal rewards over the course of our training. We use <code>pandas</code> to store our data, and to perform a rolling mean calculation, and we use <code>seaborn</code> to plot the data.</p>
<p>In reinforcement learning, rewards can be noisy (have high variance). To smooth out the plot of rewards over time, we calculate a <em>rolling mean</em> with a certain window size.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
<span class="n">window</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">running_mean_rewards</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;q-learning&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="c1"># this slicing is because the first window-1 elements are NaN from the rolling operation.</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">running_mean_rewards</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[17]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1bd0bc88&gt;</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is <em>always</em> a good idea to ensure that your agent is performing better than random. We can essentially run the same loop, but choosing random actions each time.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">T</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="n">random_rewards</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="n">a_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">s_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a_t</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">random_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r_t</span><span class="p">)</span>
        <span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">random_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">random_rewards</span><span class="p">)[:</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)]</span> <span class="c1"># make sure random_rewards is the same length as rewards. It will generally be much longer, since it will die more frequently causing more rewards to be stored given the same budget of timesteps.</span>
<span class="n">window</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">running_mean_random_rewards</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">random_rewards</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">compare</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">running_mean_random_rewards</span><span class="p">,</span> <span class="n">running_mean_rewards</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># generate one dataframe holding the terminal rewards from the q-learning agent and the random agent.</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">compare</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[21]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1bd70080&gt;</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And there we have it! A fully-functional $Q$-learning agent that can perform strongly in <code>FrozenLake-v0</code>.</p>

</div>
</div>
</div>


      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">alexandervandekleut.github.io maintained by <a href="https://github.com/alexandervandekleut">alexandervandekleut</a></p>
        
      </footer>
    </div>

    
  </body>
</html>
