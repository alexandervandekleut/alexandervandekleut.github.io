<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" type="text/css" media="screen" href="/assets/css/style.css?v=c63ddc962432b9bbec2b450e5d88d8041dc5bff1">
    <!-- Mathjax Support -->
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Q Learning Tensorflow | alexandervandekleut.github.io</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Q Learning Tensorflow" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In&nbsp;[1]: import numpy as np import gym import seaborn as sns import pandas as pd import tensorflow as tf sns.set() $Q$-learning as a Regression Problem&#182;When we first learned about $Q$-learning, we used the Bellman equation to learn the $Q$ function: $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q(s_{t+1}, a_{t+1}) \right) - Q(s_t, a_t) \right) $$ Compare this to gradient descent for a regression problem: $$ \theta \gets \theta - \alpha 2 \left( \hat{y} - y \right) \nabla_\theta \hat{y} $$ These methods are essentially analogous: we update parameters about our function in a manner proportional to the difference between our prediction and the &#39;true&#39; value. The difference for tabular $Q$-learning is that we essentially have a different parameter for each state-action pair. If we think about our loss function, then, we have $$ L(\theta) = \frac{1}{2} \left( r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q(s_{t+1}, a_{t+1}) \right) - Q(s_t, a_t) \right)^2 $$ which we can minimize using gradient descent. FrozenLake with Tensorflow&#182;Before diving deep into using techniques like deep neural networks, I want to show you how we might do $Q$-learning in tensorflow using the same FrozenLake-v0 environment from earlier. In&nbsp;[2]: class Agent: def __init__(self, num_states, num_actions, epsilon_i=1.0, epsilon_f=0.0, n_epsilon=0.1, alpha=0.5, gamma = 0.95, hidden_layers = [] ): self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.epsilon = epsilon_i self.n_epsilon = n_epsilon self.num_states = num_states self.num_actions = num_actions self.alpha = alpha self.gamma = gamma self.Q = tf.Variable(tf.zeros((num_states, num_actions)), name=&quot;Q&quot;) self.optimizer = tf.keras.optimizers.SGD(alpha) def decay_epsilon(self, n): self.epsilon = max( self.epsilon_f, self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f)) def act(self, s_t): if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions) return np.argmax(self.Q[s_t]) def update(self, s_t, a_t, r_t, s_t_next, d_t): Q_next = tf.stop_gradient(np.max(self.Q[s_t_next])) with tf.GradientTape() as tape: loss = 0.5*tf.reduce_mean(r_t + (1-d_t)*self.gamma*Q_next - self.Q[s_t, a_t])**2 grads = tape.gradient(loss, [self.Q]) self.optimizer.apply_gradients(zip(grads, [self.Q])) In&nbsp;[3]: def plot(data, window=100): sns.lineplot( data=data.rolling(window=window).mean()[window-1::window] ) In&nbsp;[4]: def train(env_name, T=100000, alpha=0.8, gamma=0.95, epsilon_i = 1.0, epsilon_f = 0.0, n_epsilon = 0.1): env = gym.make(env_name) num_states = env.observation_space.n num_actions = env.action_space.n agent = Agent(num_states, num_actions, alpha=alpha, gamma=gamma, epsilon_i=epsilon_i, epsilon_f=epsilon_f, n_epsilon = n_epsilon) rewards = [] episode_rewards = 0 s_t = env.reset() for t in range(T): a_t = agent.act(s_t) s_t_next, r_t, d_t, info = env.step(a_t) agent.update(s_t, a_t, r_t, s_t_next, d_t) agent.decay_epsilon(t/T) s_t = s_t_next episode_rewards += r_t if d_t: rewards.append(episode_rewards) episode_rewards = 0 s_t = env.reset() plot(pd.DataFrame(rewards)) return agent In&nbsp;[5]: train(&quot;FrozenLake-v0&quot;, T=100000) WARNING: Logging before flag parsing goes to stderr. W0612 19:29:56.133569 4606301632 deprecation.py:323] From /anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Out[5]: &lt;__main__.Agent at 0x1a28c9a748&gt;" />
<meta property="og:description" content="In&nbsp;[1]: import numpy as np import gym import seaborn as sns import pandas as pd import tensorflow as tf sns.set() $Q$-learning as a Regression Problem&#182;When we first learned about $Q$-learning, we used the Bellman equation to learn the $Q$ function: $$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q(s_{t+1}, a_{t+1}) \right) - Q(s_t, a_t) \right) $$ Compare this to gradient descent for a regression problem: $$ \theta \gets \theta - \alpha 2 \left( \hat{y} - y \right) \nabla_\theta \hat{y} $$ These methods are essentially analogous: we update parameters about our function in a manner proportional to the difference between our prediction and the &#39;true&#39; value. The difference for tabular $Q$-learning is that we essentially have a different parameter for each state-action pair. If we think about our loss function, then, we have $$ L(\theta) = \frac{1}{2} \left( r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q(s_{t+1}, a_{t+1}) \right) - Q(s_t, a_t) \right)^2 $$ which we can minimize using gradient descent. FrozenLake with Tensorflow&#182;Before diving deep into using techniques like deep neural networks, I want to show you how we might do $Q$-learning in tensorflow using the same FrozenLake-v0 environment from earlier. In&nbsp;[2]: class Agent: def __init__(self, num_states, num_actions, epsilon_i=1.0, epsilon_f=0.0, n_epsilon=0.1, alpha=0.5, gamma = 0.95, hidden_layers = [] ): self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.epsilon = epsilon_i self.n_epsilon = n_epsilon self.num_states = num_states self.num_actions = num_actions self.alpha = alpha self.gamma = gamma self.Q = tf.Variable(tf.zeros((num_states, num_actions)), name=&quot;Q&quot;) self.optimizer = tf.keras.optimizers.SGD(alpha) def decay_epsilon(self, n): self.epsilon = max( self.epsilon_f, self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f)) def act(self, s_t): if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions) return np.argmax(self.Q[s_t]) def update(self, s_t, a_t, r_t, s_t_next, d_t): Q_next = tf.stop_gradient(np.max(self.Q[s_t_next])) with tf.GradientTape() as tape: loss = 0.5*tf.reduce_mean(r_t + (1-d_t)*self.gamma*Q_next - self.Q[s_t, a_t])**2 grads = tape.gradient(loss, [self.Q]) self.optimizer.apply_gradients(zip(grads, [self.Q])) In&nbsp;[3]: def plot(data, window=100): sns.lineplot( data=data.rolling(window=window).mean()[window-1::window] ) In&nbsp;[4]: def train(env_name, T=100000, alpha=0.8, gamma=0.95, epsilon_i = 1.0, epsilon_f = 0.0, n_epsilon = 0.1): env = gym.make(env_name) num_states = env.observation_space.n num_actions = env.action_space.n agent = Agent(num_states, num_actions, alpha=alpha, gamma=gamma, epsilon_i=epsilon_i, epsilon_f=epsilon_f, n_epsilon = n_epsilon) rewards = [] episode_rewards = 0 s_t = env.reset() for t in range(T): a_t = agent.act(s_t) s_t_next, r_t, d_t, info = env.step(a_t) agent.update(s_t, a_t, r_t, s_t_next, d_t) agent.decay_epsilon(t/T) s_t = s_t_next episode_rewards += r_t if d_t: rewards.append(episode_rewards) episode_rewards = 0 s_t = env.reset() plot(pd.DataFrame(rewards)) return agent In&nbsp;[5]: train(&quot;FrozenLake-v0&quot;, T=100000) WARNING: Logging before flag parsing goes to stderr. W0612 19:29:56.133569 4606301632 deprecation.py:323] From /anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Out[5]: &lt;__main__.Agent at 0x1a28c9a748&gt;" />
<link rel="canonical" href="http://localhost:4000/q-learning-tensorflow/" />
<meta property="og:url" content="http://localhost:4000/q-learning-tensorflow/" />
<meta property="og:site_name" content="alexandervandekleut.github.io" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-12T00:00:00-04:00" />
<script type="application/ld+json">
{"description":"In&nbsp;[1]: import numpy as np import gym import seaborn as sns import pandas as pd import tensorflow as tf sns.set() $Q$-learning as a Regression Problem&#182;When we first learned about $Q$-learning, we used the Bellman equation to learn the $Q$ function: $$ Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha \\left( r_t + (1-d_t)\\gamma \\max_{a_{t+1}} \\left( Q(s_{t+1}, a_{t+1}) \\right) - Q(s_t, a_t) \\right) $$ Compare this to gradient descent for a regression problem: $$ \\theta \\gets \\theta - \\alpha 2 \\left( \\hat{y} - y \\right) \\nabla_\\theta \\hat{y} $$ These methods are essentially analogous: we update parameters about our function in a manner proportional to the difference between our prediction and the &#39;true&#39; value. The difference for tabular $Q$-learning is that we essentially have a different parameter for each state-action pair. If we think about our loss function, then, we have $$ L(\\theta) = \\frac{1}{2} \\left( r_t + (1-d_t)\\gamma \\max_{a_{t+1}} \\left( Q(s_{t+1}, a_{t+1}) \\right) - Q(s_t, a_t) \\right)^2 $$ which we can minimize using gradient descent. FrozenLake with Tensorflow&#182;Before diving deep into using techniques like deep neural networks, I want to show you how we might do $Q$-learning in tensorflow using the same FrozenLake-v0 environment from earlier. In&nbsp;[2]: class Agent: def __init__(self, num_states, num_actions, epsilon_i=1.0, epsilon_f=0.0, n_epsilon=0.1, alpha=0.5, gamma = 0.95, hidden_layers = [] ): self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.epsilon = epsilon_i self.n_epsilon = n_epsilon self.num_states = num_states self.num_actions = num_actions self.alpha = alpha self.gamma = gamma self.Q = tf.Variable(tf.zeros((num_states, num_actions)), name=&quot;Q&quot;) self.optimizer = tf.keras.optimizers.SGD(alpha) def decay_epsilon(self, n): self.epsilon = max( self.epsilon_f, self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f)) def act(self, s_t): if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions) return np.argmax(self.Q[s_t]) def update(self, s_t, a_t, r_t, s_t_next, d_t): Q_next = tf.stop_gradient(np.max(self.Q[s_t_next])) with tf.GradientTape() as tape: loss = 0.5*tf.reduce_mean(r_t + (1-d_t)*self.gamma*Q_next - self.Q[s_t, a_t])**2 grads = tape.gradient(loss, [self.Q]) self.optimizer.apply_gradients(zip(grads, [self.Q])) In&nbsp;[3]: def plot(data, window=100): sns.lineplot( data=data.rolling(window=window).mean()[window-1::window] ) In&nbsp;[4]: def train(env_name, T=100000, alpha=0.8, gamma=0.95, epsilon_i = 1.0, epsilon_f = 0.0, n_epsilon = 0.1): env = gym.make(env_name) num_states = env.observation_space.n num_actions = env.action_space.n agent = Agent(num_states, num_actions, alpha=alpha, gamma=gamma, epsilon_i=epsilon_i, epsilon_f=epsilon_f, n_epsilon = n_epsilon) rewards = [] episode_rewards = 0 s_t = env.reset() for t in range(T): a_t = agent.act(s_t) s_t_next, r_t, d_t, info = env.step(a_t) agent.update(s_t, a_t, r_t, s_t_next, d_t) agent.decay_epsilon(t/T) s_t = s_t_next episode_rewards += r_t if d_t: rewards.append(episode_rewards) episode_rewards = 0 s_t = env.reset() plot(pd.DataFrame(rewards)) return agent In&nbsp;[5]: train(&quot;FrozenLake-v0&quot;, T=100000) WARNING: Logging before flag parsing goes to stderr. W0612 19:29:56.133569 4606301632 deprecation.py:323] From /anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Out[5]: &lt;__main__.Agent at 0x1a28c9a748&gt;","@type":"BlogPosting","url":"http://localhost:4000/q-learning-tensorflow/","headline":"Q Learning Tensorflow","dateModified":"2019-05-12T00:00:00-04:00","datePublished":"2019-05-12T00:00:00-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/q-learning-tensorflow/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <h1 id="project_title"> TF 2.0 for Reinforcement Learning </h1>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        
        <h4>
          <a href="http://localhost:4000">Home</a>
        </h4>
        
        
        <p> Download the <a href="http://localhost:4000/assets/notebooks/q-learning-tensorflow.ipynb"> notebook </a> or follow along. </p>
        
        
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<h3 id="$Q$-learning-as-a-Regression-Problem">$Q$-learning as a Regression Problem<a class="anchor-link" href="#$Q$-learning-as-a-Regression-Problem">&#182;</a></h3><p>When we first learned about $Q$-learning, we used the Bellman equation to learn the $Q$ function:
$$
Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha \left( r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q(s_{t+1}, a_{t+1}) \right) - Q(s_t, a_t) \right)
$$</p>
<p>Compare this to gradient descent for a regression problem:
$$
\theta \gets \theta - \alpha 2 \left( \hat{y} - y \right) \nabla_\theta \hat{y}
$$</p>
<p>These methods are essentially analogous: we update parameters about our function in a manner proportional to the difference between our prediction and the 'true' value. The difference for tabular $Q$-learning is that we essentially have a different parameter for each state-action pair. If we think about our loss function, then, we have
$$
L(\theta) = \frac{1}{2} \left( r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q(s_{t+1}, a_{t+1}) \right) - Q(s_t, a_t) \right)^2
$$</p>
<p>which we can minimize using gradient descent.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="FrozenLake-with-Tensorflow">FrozenLake with Tensorflow<a class="anchor-link" href="#FrozenLake-with-Tensorflow">&#182;</a></h3><p>Before diving deep into using techniques like deep neural networks, I want to show you how we might do $Q$-learning in tensorflow using the same <code>FrozenLake-v0</code> environment from earlier.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> 
                 <span class="n">epsilon_i</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
                 <span class="n">epsilon_f</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> 
                 <span class="n">n_epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> 
                 <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">,</span>
                 <span class="n">hidden_layers</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">=</span> <span class="n">epsilon_i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span> <span class="o">=</span> <span class="n">epsilon_f</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon_i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_epsilon</span> <span class="o">=</span> <span class="n">n_epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_states</span> <span class="o">=</span> <span class="n">num_states</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">num_actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Q&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">decay_epsilon</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span><span class="p">,</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">-</span> <span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">n_epsilon</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">d_t</span><span class="p">):</span>
        <span class="n">Q_next</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t_next</span><span class="p">]))</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">r_t</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">d_t</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">Q_next</span> <span class="o">-</span>  <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">]))</span>
        
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">::</span><span class="n">window</span><span class="p">]</span>
    <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">env_name</span><span class="p">,</span>
         <span class="n">T</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">epsilon_i</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon_f</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">n_epsilon</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">)</span>
    <span class="n">num_states</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">num_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">epsilon_i</span><span class="o">=</span><span class="n">epsilon_i</span><span class="p">,</span> <span class="n">epsilon_f</span><span class="o">=</span><span class="n">epsilon_f</span><span class="p">,</span> <span class="n">n_epsilon</span> <span class="o">=</span> <span class="n">n_epsilon</span><span class="p">)</span>

    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">episode_rewards</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">a_t</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s_t</span><span class="p">)</span>
        <span class="n">s_t_next</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">d_t</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a_t</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">d_t</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">decay_epsilon</span><span class="p">(</span><span class="n">t</span><span class="o">/</span><span class="n">T</span><span class="p">)</span>
        <span class="n">s_t</span> <span class="o">=</span> <span class="n">s_t_next</span>
        <span class="n">episode_rewards</span> <span class="o">+=</span> <span class="n">r_t</span>
        
        <span class="k">if</span> <span class="n">d_t</span><span class="p">:</span>
            <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">)</span>
            <span class="n">episode_rewards</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            
    <span class="n">plot</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">agent</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train</span><span class="p">(</span><span class="s2">&quot;FrozenLake-v0&quot;</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>WARNING: Logging before flag parsing goes to stderr.
W0612 19:29:56.133569 4606301632 deprecation.py:323] From /anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[5]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;__main__.Agent at 0x1a28c9a748&gt;</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>


        
        <h4>
          <a href="http://localhost:4000">Home</a>
        </h4>
        
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">alexandervandekleut.github.io maintained by <a href="https://github.com/alexandervandekleut">alexandervandekleut</a></p>
        
      </footer>
    </div>

    
  </body>
</html>
