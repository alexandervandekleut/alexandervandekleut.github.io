<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" type="text/css" media="screen" href="/assets/css/style.css?v=c63ddc962432b9bbec2b450e5d88d8041dc5bff1">
    <!-- Mathjax Support -->
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Deep Q Learning | alexandervandekleut.github.io</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Deep Q Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In&nbsp;[1]: import random import gym import numpy as np from collections import deque from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import Adam import tensorflow as tf import pandas as pd import seaborn as sns sns.set() Deep $Q$ networks (DQN)&#182;In previous notebooks, we have seen how we can use tensorflow and autodifferentiation to do tabular $Q$-learning in the context of a regression problem. While this technique is powerful for environments with small (finite) observation spaces $\mathcal{S}$ and action spaces $\mathcal{O}$, we run into problems when our observation space is continuous (or even just large!). Tabular $Q$-learning is only guaranteed to converge if all state-action pairs are visited infinitely many times. In practice, this generally just means a very large number to get a reasonable approximation of the $Q$-function. However, when the observation space becomes large (such as using image inputs), it is likely that we only encounter each state-action pair at most once. Thus, $Q$-learning is not guaranteed to converge. Instead, we want a technique that can estimate $Q$-values such that similar states produce similar outputs. This would allow us to learn from some state-action pairs, and then generalize to other unseen state-action pairs. By using a differentiable function approximator, we get this kind of behaviour. Recall that $Q$-learning is a regression problem, meaning any kind of regression model could work - even a linear regression. However, the most popular model used by deep reinforcement learning researchers is the deep neural network. If you are familiar with supervised learning in deep learning, you may be familiar with techniques like dropout, batch normalization, and activity regularization. So far, these kinds of techniques do not prove extremely useful in the context of reinforcement learning. Instead, fully-connected neural networks with a small number of hidden units consisting of rectified linear units tend to perform best. (Pieter Abbeel notes that on simple problems, linear feedback control can perform well even in complex environments. Fully-connected neural networks that use ReLU activations function as multi-step piecewise linear feedback controllers, hence their success). When using neural networks, rather than passing many state-action pairs to the network and predicting a scalar $Q(s_t, a_t)$, we pass only the state $s_t$ and produce a vectorized output $\vec{Q}(s_t)$ where each entry in the vector corresponds to the predicted $Q$-value for each action available to the agent. Note that this necessitates that $\mathcal{A}$ is finite (and generally small), a limitation of $DQN$ that we will overcome later in the section on policy gradients. In this notebook, we make use of tensorflow&#39;s keras API to build neural networks. We also take advantage of batched environments to accelerate data collection. The keras api build neural networks that process inputs in batches. This means that if our observation space for a single environment has a shape $84 \times 84 \times 3$, then the network expects inputs of shape $B \times 84 \times 84 \times 3$ where $B$ is the number of inputs in the batch. When running the tabular $Q$-learning agent in tensorflow in the previous notebook, runtime was considerably slower than the simply numpy-based agent. The computation time spent evaluating and updating the policy dominated the time to perform a single step/update of the agent, compared to the time spent simulating a step in the environment. Ideally, the time spent should be 50% policy evaluation and 50% environment stepping. By using batched environments, we can even this out. Furthermore, this means that we get more data per wall-clock-time, which will accelerate learning. This will be different from most tutorials which use keras, where a single environment is used, and inputs are manipulated to trick keras into treating them like a batch. Increasing the number of environments can stabilize training by diversifying the collected data over time. In&nbsp;[2]: class ReplayBuffer: def __init__(self, size=1000000): self.memory = deque(maxlen=size) def remember(self, s_t, a_t, r_t, s_t_next, d_t): self.memory.append((s_t, a_t, r_t, s_t_next, d_t)) def sample(self, num=32): num = min(num, len(self.memory)) return random.sample(self.memory, num) In&nbsp;[3]: class Agent: def __init__(self, state_shape, num_actions, num_envs, alpha=0.001, gamma=0.95, epsilon_i=1.0, epsilon_f=0.01, n_epsilon=0.1, hidden_sizes = []): self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.n_epsilon = n_epsilon self.epsilon = epsilon_i self.gamma = gamma self.num_actions = num_actions self.num_envs = num_envs self.Q = Sequential() for size in hidden_sizes: self.Q.add(Dense(size, activation=&#39;relu&#39;, use_bias=&#39;false&#39;, kernel_initializer=&#39;he_uniform&#39;, dtype=&#39;float64&#39;)) self.Q.add(Dense(self.num_actions, activation=&quot;linear&quot;, use_bias=&#39;false&#39;, kernel_initializer=&#39;zeros&#39;, dtype=&#39;float64&#39;)) # self.optimizer = tf.keras.optimizers.SGD(alpha) self.optimizer = tf.keras.optimizers.Adam(alpha) def act(self, s_t): if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions, size=self.num_envs) return np.argmax(self.Q(s_t), axis=1) def decay_epsilon(self, n): self.epsilon = max( self.epsilon_f, self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f)) def update(self, s_t, a_t, r_t, s_t_next, d_t): with tf.GradientTape() as tape: Q_next = tf.stop_gradient(tf.reduce_max(self.Q(s_t_next), axis=1)) Q_pred = tf.reduce_sum(self.Q(s_t)*tf.one_hot(a_t, self.num_actions, dtype=tf.float64), axis=1) loss = tf.reduce_mean(0.5*(r_t + (1-d_t)*self.gamma*Q_next - Q_pred)**2) grads = tape.gradient(loss, self.Q.trainable_variables) self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables)) In&nbsp;[4]: class DiscreteToBoxWrapper(gym.ObservationWrapper): def __init__(self, env): super().__init__(env) assert isinstance(env.observation_space, gym.spaces.Discrete), \ &quot;Should only be used to wrap Discrete envs.&quot; self.n = self.observation_space.n self.observation_space = gym.spaces.Box(0, 1, (self.n,)) def observation(self, obs): new_obs = np.zeros(self.n) new_obs[obs] = 1 return new_obs In&nbsp;[5]: class VectorizedEnvWrapper(gym.Wrapper): def __init__(self, make_env, num_envs=1): super().__init__(make_env()) self.num_envs = num_envs self.envs = [make_env() for env_index in range(num_envs)] def reset(self): return np.asarray([env.reset() for env in self.envs]) def reset_at(self, env_index): return self.envs[env_index].reset() def step(self, actions): next_states, rewards, dones, infos = [], [], [], [] for env, action in zip(self.envs, actions): next_state, reward, done, info = env.step(action) next_states.append(next_state) rewards.append(reward) dones.append(done) infos.append(info) return np.asarray(next_states), np.asarray(rewards), \ np.asarray(dones), np.asarray(infos) In&nbsp;[6]: def plot(data, window=100): sns.lineplot( data=data.rolling(window=window).mean()[window-1::window] ) In&nbsp;[7]: def train(env_name, T=20000, num_envs=32, batch_size=32, hidden_sizes=[24, 24], alpha=0.001, gamma=0.95): env = VectorizedEnvWrapper(lambda: gym.make(env_name), num_envs) state_shape = env.observation_space.shape num_actions = env.action_space.n agent = Agent(state_shape, num_actions, num_envs, alpha=alpha, hidden_sizes=hidden_sizes, gamma=gamma) rewards = [] buffer = ReplayBuffer() episode_rewards = 0 s_t = env.reset() for t in range(T): a_t = agent.act(s_t) s_t_next, r_t, d_t, info = env.step(a_t) buffer.remember(s_t, a_t, r_t, s_t_next, d_t) s_t = s_t_next for batch in buffer.sample(batch_size): agent.update(*batch) agent.decay_epsilon(t/T) episode_rewards += r_t for i in range(env.num_envs): if d_t[i]: rewards.append(episode_rewards[i]) episode_rewards[i] = 0 s_t[i] = env.reset_at(i) plot(pd.DataFrame(rewards), window=10) return agent In&nbsp;[8]: train(&quot;CartPole-v0&quot;, T=20000, num_envs=32, batch_size=1) WARNING: Logging before flag parsing goes to stderr. W0612 19:29:48.104817 4657370560 deprecation.py:323] From /anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Out[8]: &lt;__main__.Agent at 0x1a2e574cf8&gt;" />
<meta property="og:description" content="In&nbsp;[1]: import random import gym import numpy as np from collections import deque from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import Adam import tensorflow as tf import pandas as pd import seaborn as sns sns.set() Deep $Q$ networks (DQN)&#182;In previous notebooks, we have seen how we can use tensorflow and autodifferentiation to do tabular $Q$-learning in the context of a regression problem. While this technique is powerful for environments with small (finite) observation spaces $\mathcal{S}$ and action spaces $\mathcal{O}$, we run into problems when our observation space is continuous (or even just large!). Tabular $Q$-learning is only guaranteed to converge if all state-action pairs are visited infinitely many times. In practice, this generally just means a very large number to get a reasonable approximation of the $Q$-function. However, when the observation space becomes large (such as using image inputs), it is likely that we only encounter each state-action pair at most once. Thus, $Q$-learning is not guaranteed to converge. Instead, we want a technique that can estimate $Q$-values such that similar states produce similar outputs. This would allow us to learn from some state-action pairs, and then generalize to other unseen state-action pairs. By using a differentiable function approximator, we get this kind of behaviour. Recall that $Q$-learning is a regression problem, meaning any kind of regression model could work - even a linear regression. However, the most popular model used by deep reinforcement learning researchers is the deep neural network. If you are familiar with supervised learning in deep learning, you may be familiar with techniques like dropout, batch normalization, and activity regularization. So far, these kinds of techniques do not prove extremely useful in the context of reinforcement learning. Instead, fully-connected neural networks with a small number of hidden units consisting of rectified linear units tend to perform best. (Pieter Abbeel notes that on simple problems, linear feedback control can perform well even in complex environments. Fully-connected neural networks that use ReLU activations function as multi-step piecewise linear feedback controllers, hence their success). When using neural networks, rather than passing many state-action pairs to the network and predicting a scalar $Q(s_t, a_t)$, we pass only the state $s_t$ and produce a vectorized output $\vec{Q}(s_t)$ where each entry in the vector corresponds to the predicted $Q$-value for each action available to the agent. Note that this necessitates that $\mathcal{A}$ is finite (and generally small), a limitation of $DQN$ that we will overcome later in the section on policy gradients. In this notebook, we make use of tensorflow&#39;s keras API to build neural networks. We also take advantage of batched environments to accelerate data collection. The keras api build neural networks that process inputs in batches. This means that if our observation space for a single environment has a shape $84 \times 84 \times 3$, then the network expects inputs of shape $B \times 84 \times 84 \times 3$ where $B$ is the number of inputs in the batch. When running the tabular $Q$-learning agent in tensorflow in the previous notebook, runtime was considerably slower than the simply numpy-based agent. The computation time spent evaluating and updating the policy dominated the time to perform a single step/update of the agent, compared to the time spent simulating a step in the environment. Ideally, the time spent should be 50% policy evaluation and 50% environment stepping. By using batched environments, we can even this out. Furthermore, this means that we get more data per wall-clock-time, which will accelerate learning. This will be different from most tutorials which use keras, where a single environment is used, and inputs are manipulated to trick keras into treating them like a batch. Increasing the number of environments can stabilize training by diversifying the collected data over time. In&nbsp;[2]: class ReplayBuffer: def __init__(self, size=1000000): self.memory = deque(maxlen=size) def remember(self, s_t, a_t, r_t, s_t_next, d_t): self.memory.append((s_t, a_t, r_t, s_t_next, d_t)) def sample(self, num=32): num = min(num, len(self.memory)) return random.sample(self.memory, num) In&nbsp;[3]: class Agent: def __init__(self, state_shape, num_actions, num_envs, alpha=0.001, gamma=0.95, epsilon_i=1.0, epsilon_f=0.01, n_epsilon=0.1, hidden_sizes = []): self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.n_epsilon = n_epsilon self.epsilon = epsilon_i self.gamma = gamma self.num_actions = num_actions self.num_envs = num_envs self.Q = Sequential() for size in hidden_sizes: self.Q.add(Dense(size, activation=&#39;relu&#39;, use_bias=&#39;false&#39;, kernel_initializer=&#39;he_uniform&#39;, dtype=&#39;float64&#39;)) self.Q.add(Dense(self.num_actions, activation=&quot;linear&quot;, use_bias=&#39;false&#39;, kernel_initializer=&#39;zeros&#39;, dtype=&#39;float64&#39;)) # self.optimizer = tf.keras.optimizers.SGD(alpha) self.optimizer = tf.keras.optimizers.Adam(alpha) def act(self, s_t): if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions, size=self.num_envs) return np.argmax(self.Q(s_t), axis=1) def decay_epsilon(self, n): self.epsilon = max( self.epsilon_f, self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f)) def update(self, s_t, a_t, r_t, s_t_next, d_t): with tf.GradientTape() as tape: Q_next = tf.stop_gradient(tf.reduce_max(self.Q(s_t_next), axis=1)) Q_pred = tf.reduce_sum(self.Q(s_t)*tf.one_hot(a_t, self.num_actions, dtype=tf.float64), axis=1) loss = tf.reduce_mean(0.5*(r_t + (1-d_t)*self.gamma*Q_next - Q_pred)**2) grads = tape.gradient(loss, self.Q.trainable_variables) self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables)) In&nbsp;[4]: class DiscreteToBoxWrapper(gym.ObservationWrapper): def __init__(self, env): super().__init__(env) assert isinstance(env.observation_space, gym.spaces.Discrete), \ &quot;Should only be used to wrap Discrete envs.&quot; self.n = self.observation_space.n self.observation_space = gym.spaces.Box(0, 1, (self.n,)) def observation(self, obs): new_obs = np.zeros(self.n) new_obs[obs] = 1 return new_obs In&nbsp;[5]: class VectorizedEnvWrapper(gym.Wrapper): def __init__(self, make_env, num_envs=1): super().__init__(make_env()) self.num_envs = num_envs self.envs = [make_env() for env_index in range(num_envs)] def reset(self): return np.asarray([env.reset() for env in self.envs]) def reset_at(self, env_index): return self.envs[env_index].reset() def step(self, actions): next_states, rewards, dones, infos = [], [], [], [] for env, action in zip(self.envs, actions): next_state, reward, done, info = env.step(action) next_states.append(next_state) rewards.append(reward) dones.append(done) infos.append(info) return np.asarray(next_states), np.asarray(rewards), \ np.asarray(dones), np.asarray(infos) In&nbsp;[6]: def plot(data, window=100): sns.lineplot( data=data.rolling(window=window).mean()[window-1::window] ) In&nbsp;[7]: def train(env_name, T=20000, num_envs=32, batch_size=32, hidden_sizes=[24, 24], alpha=0.001, gamma=0.95): env = VectorizedEnvWrapper(lambda: gym.make(env_name), num_envs) state_shape = env.observation_space.shape num_actions = env.action_space.n agent = Agent(state_shape, num_actions, num_envs, alpha=alpha, hidden_sizes=hidden_sizes, gamma=gamma) rewards = [] buffer = ReplayBuffer() episode_rewards = 0 s_t = env.reset() for t in range(T): a_t = agent.act(s_t) s_t_next, r_t, d_t, info = env.step(a_t) buffer.remember(s_t, a_t, r_t, s_t_next, d_t) s_t = s_t_next for batch in buffer.sample(batch_size): agent.update(*batch) agent.decay_epsilon(t/T) episode_rewards += r_t for i in range(env.num_envs): if d_t[i]: rewards.append(episode_rewards[i]) episode_rewards[i] = 0 s_t[i] = env.reset_at(i) plot(pd.DataFrame(rewards), window=10) return agent In&nbsp;[8]: train(&quot;CartPole-v0&quot;, T=20000, num_envs=32, batch_size=1) WARNING: Logging before flag parsing goes to stderr. W0612 19:29:48.104817 4657370560 deprecation.py:323] From /anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Out[8]: &lt;__main__.Agent at 0x1a2e574cf8&gt;" />
<link rel="canonical" href="http://localhost:4000/deep-q-learning/" />
<meta property="og:url" content="http://localhost:4000/deep-q-learning/" />
<meta property="og:site_name" content="alexandervandekleut.github.io" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-12T00:00:00-04:00" />
<script type="application/ld+json">
{"description":"In&nbsp;[1]: import random import gym import numpy as np from collections import deque from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import Adam import tensorflow as tf import pandas as pd import seaborn as sns sns.set() Deep $Q$ networks (DQN)&#182;In previous notebooks, we have seen how we can use tensorflow and autodifferentiation to do tabular $Q$-learning in the context of a regression problem. While this technique is powerful for environments with small (finite) observation spaces $\\mathcal{S}$ and action spaces $\\mathcal{O}$, we run into problems when our observation space is continuous (or even just large!). Tabular $Q$-learning is only guaranteed to converge if all state-action pairs are visited infinitely many times. In practice, this generally just means a very large number to get a reasonable approximation of the $Q$-function. However, when the observation space becomes large (such as using image inputs), it is likely that we only encounter each state-action pair at most once. Thus, $Q$-learning is not guaranteed to converge. Instead, we want a technique that can estimate $Q$-values such that similar states produce similar outputs. This would allow us to learn from some state-action pairs, and then generalize to other unseen state-action pairs. By using a differentiable function approximator, we get this kind of behaviour. Recall that $Q$-learning is a regression problem, meaning any kind of regression model could work - even a linear regression. However, the most popular model used by deep reinforcement learning researchers is the deep neural network. If you are familiar with supervised learning in deep learning, you may be familiar with techniques like dropout, batch normalization, and activity regularization. So far, these kinds of techniques do not prove extremely useful in the context of reinforcement learning. Instead, fully-connected neural networks with a small number of hidden units consisting of rectified linear units tend to perform best. (Pieter Abbeel notes that on simple problems, linear feedback control can perform well even in complex environments. Fully-connected neural networks that use ReLU activations function as multi-step piecewise linear feedback controllers, hence their success). When using neural networks, rather than passing many state-action pairs to the network and predicting a scalar $Q(s_t, a_t)$, we pass only the state $s_t$ and produce a vectorized output $\\vec{Q}(s_t)$ where each entry in the vector corresponds to the predicted $Q$-value for each action available to the agent. Note that this necessitates that $\\mathcal{A}$ is finite (and generally small), a limitation of $DQN$ that we will overcome later in the section on policy gradients. In this notebook, we make use of tensorflow&#39;s keras API to build neural networks. We also take advantage of batched environments to accelerate data collection. The keras api build neural networks that process inputs in batches. This means that if our observation space for a single environment has a shape $84 \\times 84 \\times 3$, then the network expects inputs of shape $B \\times 84 \\times 84 \\times 3$ where $B$ is the number of inputs in the batch. When running the tabular $Q$-learning agent in tensorflow in the previous notebook, runtime was considerably slower than the simply numpy-based agent. The computation time spent evaluating and updating the policy dominated the time to perform a single step/update of the agent, compared to the time spent simulating a step in the environment. Ideally, the time spent should be 50% policy evaluation and 50% environment stepping. By using batched environments, we can even this out. Furthermore, this means that we get more data per wall-clock-time, which will accelerate learning. This will be different from most tutorials which use keras, where a single environment is used, and inputs are manipulated to trick keras into treating them like a batch. Increasing the number of environments can stabilize training by diversifying the collected data over time. In&nbsp;[2]: class ReplayBuffer: def __init__(self, size=1000000): self.memory = deque(maxlen=size) def remember(self, s_t, a_t, r_t, s_t_next, d_t): self.memory.append((s_t, a_t, r_t, s_t_next, d_t)) def sample(self, num=32): num = min(num, len(self.memory)) return random.sample(self.memory, num) In&nbsp;[3]: class Agent: def __init__(self, state_shape, num_actions, num_envs, alpha=0.001, gamma=0.95, epsilon_i=1.0, epsilon_f=0.01, n_epsilon=0.1, hidden_sizes = []): self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.n_epsilon = n_epsilon self.epsilon = epsilon_i self.gamma = gamma self.num_actions = num_actions self.num_envs = num_envs self.Q = Sequential() for size in hidden_sizes: self.Q.add(Dense(size, activation=&#39;relu&#39;, use_bias=&#39;false&#39;, kernel_initializer=&#39;he_uniform&#39;, dtype=&#39;float64&#39;)) self.Q.add(Dense(self.num_actions, activation=&quot;linear&quot;, use_bias=&#39;false&#39;, kernel_initializer=&#39;zeros&#39;, dtype=&#39;float64&#39;)) # self.optimizer = tf.keras.optimizers.SGD(alpha) self.optimizer = tf.keras.optimizers.Adam(alpha) def act(self, s_t): if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions, size=self.num_envs) return np.argmax(self.Q(s_t), axis=1) def decay_epsilon(self, n): self.epsilon = max( self.epsilon_f, self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f)) def update(self, s_t, a_t, r_t, s_t_next, d_t): with tf.GradientTape() as tape: Q_next = tf.stop_gradient(tf.reduce_max(self.Q(s_t_next), axis=1)) Q_pred = tf.reduce_sum(self.Q(s_t)*tf.one_hot(a_t, self.num_actions, dtype=tf.float64), axis=1) loss = tf.reduce_mean(0.5*(r_t + (1-d_t)*self.gamma*Q_next - Q_pred)**2) grads = tape.gradient(loss, self.Q.trainable_variables) self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables)) In&nbsp;[4]: class DiscreteToBoxWrapper(gym.ObservationWrapper): def __init__(self, env): super().__init__(env) assert isinstance(env.observation_space, gym.spaces.Discrete), \\ &quot;Should only be used to wrap Discrete envs.&quot; self.n = self.observation_space.n self.observation_space = gym.spaces.Box(0, 1, (self.n,)) def observation(self, obs): new_obs = np.zeros(self.n) new_obs[obs] = 1 return new_obs In&nbsp;[5]: class VectorizedEnvWrapper(gym.Wrapper): def __init__(self, make_env, num_envs=1): super().__init__(make_env()) self.num_envs = num_envs self.envs = [make_env() for env_index in range(num_envs)] def reset(self): return np.asarray([env.reset() for env in self.envs]) def reset_at(self, env_index): return self.envs[env_index].reset() def step(self, actions): next_states, rewards, dones, infos = [], [], [], [] for env, action in zip(self.envs, actions): next_state, reward, done, info = env.step(action) next_states.append(next_state) rewards.append(reward) dones.append(done) infos.append(info) return np.asarray(next_states), np.asarray(rewards), \\ np.asarray(dones), np.asarray(infos) In&nbsp;[6]: def plot(data, window=100): sns.lineplot( data=data.rolling(window=window).mean()[window-1::window] ) In&nbsp;[7]: def train(env_name, T=20000, num_envs=32, batch_size=32, hidden_sizes=[24, 24], alpha=0.001, gamma=0.95): env = VectorizedEnvWrapper(lambda: gym.make(env_name), num_envs) state_shape = env.observation_space.shape num_actions = env.action_space.n agent = Agent(state_shape, num_actions, num_envs, alpha=alpha, hidden_sizes=hidden_sizes, gamma=gamma) rewards = [] buffer = ReplayBuffer() episode_rewards = 0 s_t = env.reset() for t in range(T): a_t = agent.act(s_t) s_t_next, r_t, d_t, info = env.step(a_t) buffer.remember(s_t, a_t, r_t, s_t_next, d_t) s_t = s_t_next for batch in buffer.sample(batch_size): agent.update(*batch) agent.decay_epsilon(t/T) episode_rewards += r_t for i in range(env.num_envs): if d_t[i]: rewards.append(episode_rewards[i]) episode_rewards[i] = 0 s_t[i] = env.reset_at(i) plot(pd.DataFrame(rewards), window=10) return agent In&nbsp;[8]: train(&quot;CartPole-v0&quot;, T=20000, num_envs=32, batch_size=1) WARNING: Logging before flag parsing goes to stderr. W0612 19:29:48.104817 4657370560 deprecation.py:323] From /anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Out[8]: &lt;__main__.Agent at 0x1a2e574cf8&gt;","@type":"BlogPosting","url":"http://localhost:4000/deep-q-learning/","headline":"Deep Q Learning","dateModified":"2019-05-12T00:00:00-04:00","datePublished":"2019-05-12T00:00:00-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/deep-q-learning/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <h1 id="project_title"> TF 2.0 for Reinforcement Learning </h1>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        
        <h4>
          <a href="http://localhost:4000">Home</a>
        </h4>
        
        
        <p> Download the <a href="http://localhost:4000/assets/notebooks/deep-q-learning.ipynb"> notebook </a> or follow along. </p>
        
        
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="k">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="k">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="k">import</span> <span class="n">Adam</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Deep-$Q$-networks-(DQN)">Deep $Q$ networks (DQN)<a class="anchor-link" href="#Deep-$Q$-networks-(DQN)">&#182;</a></h1><p>In previous notebooks, we have seen how we can use <code>tensorflow</code> and autodifferentiation to do tabular $Q$-learning in the context of a regression problem. While this technique is powerful for environments with small (finite) observation spaces $\mathcal{S}$ and action spaces $\mathcal{O}$, we run into problems when our observation space is continuous (or even just large!).</p>
<p>Tabular $Q$-learning is only guaranteed to converge if all state-action pairs are visited infinitely many times. In practice, this generally just means a very large number to get a reasonable approximation of the $Q$-function. However, when the observation space becomes large (such as using image inputs), it is likely that we only encounter each state-action pair at most once. Thus, $Q$-learning is not guaranteed to converge.</p>
<p>Instead, we want a technique that can estimate $Q$-values such that similar states produce similar outputs. This would allow us to learn from some state-action pairs, and then generalize to other unseen state-action pairs. By using a <strong>differentiable function approximator</strong>, we get this kind of behaviour. Recall that $Q$-learning is a <em>regression</em> problem, meaning any kind of regression model could work - even a linear regression. However, the most popular model used by <em>deep</em> reinforcement learning researchers is the <em>deep</em> neural network.</p>
<p>If you are familiar with supervised learning in deep learning, you may be familiar with techniques like dropout, batch normalization, and activity regularization. So far, these kinds of techniques do not prove extremely useful in the context of reinforcement learning. Instead, fully-connected neural networks with a small number of hidden units consisting of rectified linear units tend to perform best. (<a href="https://www.youtube.com/watch?v=l-mYLq6eZPY">Pieter Abbeel</a> notes that on simple problems, linear feedback control can perform well even in complex environments. Fully-connected neural networks that use ReLU activations function as multi-step piecewise linear feedback controllers, hence their success).</p>
<p>When using neural networks, rather than passing many state-action pairs to the network and predicting a scalar $Q(s_t, a_t)$, we pass only the state $s_t$ and produce a vectorized output $\vec{Q}(s_t)$ where each entry in the vector corresponds to the predicted $Q$-value for each action available to the agent. Note that this necessitates that $\mathcal{A}$ is finite (and generally small), a limitation of $DQN$ that we will overcome later in the section on policy gradients.</p>
<p><img src="../images/q-network.png" alt="image of deep-q-network mapping single state to multiple outputs" /></p>
<p>In this notebook, we make use of <code>tensorflow</code>'s <code>keras</code> API to build neural networks. We also take advantage of <strong>batched environments</strong> to accelerate data collection. The <code>keras</code> api build neural networks that process inputs in <strong>batches</strong>. This means that if our observation space for a single environment has a shape $84 \times 84 \times 3$, then the network expects inputs of shape $B \times 84 \times 84 \times 3$ where $B$ is the number of inputs in the batch.</p>
<p>When running the tabular $Q$-learning agent in <code>tensorflow</code> in the previous notebook, runtime was considerably slower than the simply numpy-based agent. The computation time spent evaluating and updating the policy dominated the time to perform a single step/update of the agent, compared to the time spent simulating a step in the environment. Ideally, the time spent should be 50% policy evaluation and 50% environment stepping. By using batched environments, we can even this out. Furthermore, this means that we get more data per wall-clock-time, which will accelerate learning. This will be different from most tutorials which use <code>keras</code>, where a single environment is used, and inputs are manipulated to trick keras into treating them like a batch.</p>
<p>Increasing the number of environments can stabilize training by diversifying the collected data over time.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">remember</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">d_t</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">d_t</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="n">num</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> <span class="n">num_envs</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">epsilon_i</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon_f</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">hidden_sizes</span> <span class="o">=</span> <span class="p">[]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">=</span> <span class="n">epsilon_i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span> <span class="o">=</span> <span class="n">epsilon_f</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_epsilon</span> <span class="o">=</span> <span class="n">n_epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon_i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">num_actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_envs</span> <span class="o">=</span> <span class="n">num_envs</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">hidden_sizes</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="s1">&#39;false&#39;</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;he_uniform&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float64&#39;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="s1">&#39;false&#39;</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float64&#39;</span><span class="p">))</span>
<span class="c1">#         self.optimizer = tf.keras.optimizers.SGD(alpha)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>        

    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_envs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">s_t</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">decay_epsilon</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span><span class="p">,</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">-</span> <span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">n_epsilon</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">d_t</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">Q_next</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">s_t_next</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">Q_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">s_t</span><span class="p">)</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">a_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">r_t</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">d_t</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">Q_next</span> <span class="o">-</span> <span class="n">Q_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DiscreteToBoxWrapper</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">ObservationWrapper</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">),</span> \
            <span class="s2">&quot;Should only be used to wrap Discrete envs.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,))</span>
    
    <span class="k">def</span> <span class="nf">observation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">):</span>
        <span class="n">new_obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>
        <span class="n">new_obs</span><span class="p">[</span><span class="n">obs</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">new_obs</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">VectorizedEnvWrapper</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">Wrapper</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">make_env</span><span class="p">,</span> <span class="n">num_envs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">make_env</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_envs</span> <span class="o">=</span> <span class="n">num_envs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">envs</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_env</span><span class="p">()</span> <span class="k">for</span> <span class="n">env_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_envs</span><span class="p">)]</span>
    
    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span> <span class="k">for</span> <span class="n">env</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">envs</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">reset_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env_index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">envs</span><span class="p">[</span><span class="n">env_index</span><span class="p">]</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
        <span class="n">next_states</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">env</span><span class="p">,</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">envs</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">next_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
            <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
            <span class="n">dones</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">done</span><span class="p">)</span>
            <span class="n">infos</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">info</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">next_states</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">rewards</span><span class="p">),</span> \
            <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">dones</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">infos</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">::</span><span class="n">window</span><span class="p">]</span>
    <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">env_name</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">num_envs</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">):</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">VectorizedEnvWrapper</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">),</span> <span class="n">num_envs</span><span class="p">)</span>
    <span class="n">state_shape</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">num_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">state_shape</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> <span class="n">num_envs</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="o">=</span><span class="n">hidden_sizes</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">)</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">()</span>
    <span class="n">episode_rewards</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">a_t</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s_t</span><span class="p">)</span>
        <span class="n">s_t_next</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">d_t</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a_t</span><span class="p">)</span>
        <span class="n">buffer</span><span class="o">.</span><span class="n">remember</span><span class="p">(</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">d_t</span><span class="p">)</span>
        <span class="n">s_t</span> <span class="o">=</span> <span class="n">s_t_next</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">decay_epsilon</span><span class="p">(</span><span class="n">t</span><span class="o">/</span><span class="n">T</span><span class="p">)</span>
        <span class="n">episode_rewards</span> <span class="o">+=</span> <span class="n">r_t</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">num_envs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">d_t</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">episode_rewards</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">s_t</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset_at</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            
    <span class="n">plot</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rewards</span><span class="p">),</span> <span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">agent</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train</span><span class="p">(</span><span class="s2">&quot;CartPole-v0&quot;</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">num_envs</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>WARNING: Logging before flag parsing goes to stderr.
W0612 19:29:48.104817 4657370560 deprecation.py:323] From /anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[8]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;__main__.Agent at 0x1a2e574cf8&gt;</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>


        
        <h4>
          <a href="http://localhost:4000">Home</a>
        </h4>
        
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">alexandervandekleut.github.io maintained by <a href="https://github.com/alexandervandekleut">alexandervandekleut</a></p>
        
      </footer>
    </div>

    
  </body>
</html>
