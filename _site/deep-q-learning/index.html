<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" type="text/css" media="screen" href="/assets/css/style.css?v=4347d9b846f2cebdc47e657f7c3c9e451d3ad96a">
    <!-- Mathjax Support -->
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Deep Q Learning | alexandervandekleut.github.io</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Deep Q Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In&nbsp;[1]: import random import gym import numpy as np from collections import deque import tensorflow as tf import pandas as pd import seaborn as sns sns.set() Deep $Q$ networks (DQN)&#182;In previous notebooks, we have seen how we can use tensorflow and autodifferentiation to do tabular $Q$-learning in the context of a regression problem. While this technique is powerful for environments with small (finite) observation spaces $\mathcal{S}$ and action spaces $\mathcal{A}$, we run into problems when our observation space is continuous (or even just large!). Tabular $Q$-learning is only guaranteed to converge if all state-action pairs are visited infinitely many times. In practice, this generally just means a very large number to get a reasonable approximation of the $Q$-function. However, when the observation space becomes large (such as using image inputs), it is likely that we only encounter each state-action pair at most once. Thus, $Q$-learning is not guaranteed to converge. Instead, we want a technique that can estimate $Q$-values such that similar states produce similar outputs. This would allow us to learn from some state-action pairs, and then generalize to other unseen state-action pairs. By using a differentiable function approximator, we get this kind of behaviour. Recall that $Q$-learning is a regression problem, meaning any kind of regression model could work - even a linear regression. However, the most popular model used by deep reinforcement learning researchers is the deep neural network. If you are familiar with supervised learning in deep learning, you may be familiar with techniques like dropout, batch normalization, and activity regularization. So far, these kinds of techniques do not prove extremely useful in the context of reinforcement learning. Instead, fully-connected neural networks with a small number of hidden units consisting of rectified linear units tend to perform best. (Pieter Abbeel notes that on simple problems, linear feedback control can perform well even in complex environments. Fully-connected neural networks that use ReLU activations function as multi-step piecewise linear feedback controllers, hence their success). When using neural networks, rather than passing many state-action pairs to the network and predicting a scalar $Q(s_t, a_t)$, we pass only the state $s_t$ and produce a vectorized output $\vec{Q}(s_t)$ where each entry in the vector corresponds to the predicted $Q$-value for each action available to the agent. Note that this necessitates that $\mathcal{A}$ is finite (and generally small), a limitation of $DQN$ that we will overcome later in the section on policy gradients. In this notebook, we make use of tensorflow&#39;s keras API to build neural networks. We also take advantage of batched environments to accelerate data collection. The keras api build neural networks that process inputs in batches. This means that if our observation space for a single environment has a shape $84 \times 84 \times 3$, then the network expects inputs of shape $B \times 84 \times 84 \times 3$ where $B$ is the number of inputs in the batch. When running the tabular $Q$-learning agent in tensorflow in the previous notebook, runtime was considerably slower than the simply numpy-based agent. The computation time spent evaluating and updating the policy dominated the time to perform a single step/update of the agent, compared to the time spent simulating a step in the environment. Ideally, the time spent should be 50% policy evaluation and 50% environment stepping. By using batched environments, we can even this out. Furthermore, this means that we get more data per wall-clock-time, which will accelerate learning. This will be different from most tutorials which use keras, where a single environment is used, and inputs are manipulated to trick keras into treating them like a batch. Increasing the number of environments can also stabilize training by diversifying the collected data over time. Target Networks and Experience Replay&#182;In this notebook, we are going to implement two techniques required for stabilizing training: target networks and experience replay. Target Networks&#182;Neural networks are differentiable, meaning that similar inputs produce similar outputs. In most environments, consecutive states are often similar to each other (with small changes occurring as a result of actions chosen). In deep $Q$-learning, we are doing a regression problem with a moving target - we are trying to predict our own output. When we do this, we perform a maximization step over our output: $$ L(\theta) = \frac{1}{2} \left( r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q_\theta(s_{t+1}, a_{t+1}) \right) - Q_\theta(s_t, a_t) \right)^2 $$Minimizing our prediction error, in general, will tend to increase the value of our prediction for $Q(s_t)$ because of this maximization step. This poses a problem. Consider the following sequence of events: The agent is in a state $s_t$ The agent chooses an action that maximizes $Q(s_t)$ The state transitions from $s_t$ to $s_{t+1}$, giving a reward of $r_t$ and a terminal flag $d_t$. Using the transition $s_t, a_t, r_t, s_{t+1}, d_t$, the agent minimizes $L(\theta)$ Since $s_{t+1}$ and $s_t$ are temporally close, they tend to be similar in general. During step 4, we updated our prediction for $Q(s_t)$ according to a maximization over all next possible values of $Q(s_{t+1})$. This tends to increase the value for $Q(s_t)$. When the agent goes to repeat this cycle the next time, its predictions for $Q(s_{t+1})$ will already be higher, because the values for $Q(s_t)$ are higher and $s_t \approx s_{t+1}$. To handle this, we introduce a target network with parameters $\theta^-$ which is used when computing the TD-target $$ r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q_{\theta^-}(s_{t+1}, a_{t+1}) \right) $$This way, the act of updating $\theta$ to minimize $L(\theta)$ has no effect on our regression targets. To keep the predictions made by the target networks $Q_{\theta^-}$ somewhat in line with the actual $Q$-network, we synchronize parameters every fixed number of timesteps. Experience Replay&#182;The vanilla $Q$-learning algorithm (and DQN, as thus far described) uses only the most recent transition to train on. While this makes the agent good at predicting recent $Q$-values, it can cause it to perform worse on older or uncommon transitions. To stabilize training, we instead store transitions in memory and sample them randomly from batches to ensure the agent gets a good mix of experiences at each training step. In&nbsp;[2]: class ReplayBuffer: def __init__(self, size=1000000): self.memory = deque(maxlen=size) def remember(self, s_t, a_t, r_t, s_t_next, d_t): self.memory.append((s_t, a_t, r_t, s_t_next, d_t)) def sample(self, num=32): num = min(num, len(self.memory)) return random.sample(self.memory, num) In&nbsp;[3]: class Agent: def __init__(self, state_shape, num_actions, num_envs, alpha=0.001, gamma=0.95, epsilon_i=1.0, epsilon_f=0.01, n_epsilon=0.1, hidden_sizes = []): self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.n_epsilon = n_epsilon self.epsilon = epsilon_i self.gamma = gamma self.num_actions = num_actions self.num_envs = num_envs self.Q = tf.keras.models.Sequential() self.Q.add(tf.keras.layers.Input(shape=state_shape)) for size in hidden_sizes: self.Q.add(tf.keras.layers.Dense(size, activation=&#39;relu&#39;, use_bias=&#39;false&#39;, kernel_initializer=&#39;he_uniform&#39;, dtype=&#39;float64&#39;)) self.Q.add(tf.keras.layers.Dense(self.num_actions, activation=&quot;linear&quot;, use_bias=&#39;false&#39;, kernel_initializer=&#39;zeros&#39;, dtype=&#39;float64&#39;)) # target network self.Q_ = tf.keras.models.Sequential() self.Q_.add(tf.keras.layers.Input(shape=state_shape)) for size in hidden_sizes: self.Q_.add(tf.keras.layers.Dense(size, activation=&#39;relu&#39;, use_bias=&#39;false&#39;, kernel_initializer=&#39;he_uniform&#39;, dtype=&#39;float64&#39;)) self.Q_.add(tf.keras.layers.Dense(self.num_actions, activation=&quot;linear&quot;, use_bias=&#39;false&#39;, kernel_initializer=&#39;zeros&#39;, dtype=&#39;float64&#39;)) self.optimizer = tf.keras.optimizers.Adam(alpha) def synchronize(self): self.Q_.set_weights(self.Q.get_weights()) def act(self, s_t): if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions, size=self.num_envs) return np.argmax(self.Q(s_t), axis=1) def decay_epsilon(self, n): self.epsilon = max( self.epsilon_f, self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f)) def update(self, s_t, a_t, r_t, s_t_next, d_t): with tf.GradientTape() as tape: Q_next = tf.stop_gradient(tf.reduce_max(self.Q_(s_t_next), axis=1)) # note we use Q_ Q_pred = tf.reduce_sum(self.Q(s_t)*tf.one_hot(a_t, self.num_actions, dtype=tf.float64), axis=1) loss = tf.reduce_mean(0.5*(r_t + (1-d_t)*self.gamma*Q_next - Q_pred)**2) grads = tape.gradient(loss, self.Q.trainable_variables) self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables)) In&nbsp;[4]: class DiscreteToBoxWrapper(gym.ObservationWrapper): def __init__(self, env): super().__init__(env) assert isinstance(env.observation_space, gym.spaces.Discrete), \ &quot;Should only be used to wrap Discrete envs.&quot; self.n = self.observation_space.n self.observation_space = gym.spaces.Box(0, 1, (self.n,)) def observation(self, obs): new_obs = np.zeros(self.n) new_obs[obs] = 1 return new_obs In&nbsp;[5]: class VectorizedEnvWrapper(gym.Wrapper): def __init__(self, make_env, num_envs=1): super().__init__(make_env()) self.num_envs = num_envs self.envs = [make_env() for env_index in range(num_envs)] def reset(self): return np.asarray([env.reset() for env in self.envs]) def reset_at(self, env_index): return self.envs[env_index].reset() def step(self, actions): next_states, rewards, dones, infos = [], [], [], [] for env, action in zip(self.envs, actions): next_state, reward, done, info = env.step(action) next_states.append(next_state) rewards.append(reward) dones.append(done) infos.append(info) return np.asarray(next_states), np.asarray(rewards), \ np.asarray(dones), np.asarray(infos) In&nbsp;[6]: def plot(data, window=100): sns.lineplot( data=data.rolling(window=window).mean()[window-1::window] ) In&nbsp;[7]: def train(env_name, T=20000, num_envs=32, batch_size=32, sync_every=100, hidden_sizes=[24, 24], alpha=0.001, gamma=0.95): env = VectorizedEnvWrapper(lambda: gym.make(env_name), num_envs) state_shape = env.observation_space.shape num_actions = env.action_space.n agent = Agent(state_shape, num_actions, num_envs, alpha=alpha, hidden_sizes=hidden_sizes, gamma=gamma) rewards = [] buffer = ReplayBuffer() episode_rewards = 0 s_t = env.reset() for t in range(T): if t%sync_every == 0: agent.synchronize() a_t = agent.act(s_t) s_t_next, r_t, d_t, info = env.step(a_t) buffer.remember(s_t, a_t, r_t, s_t_next, d_t) s_t = s_t_next for batch in buffer.sample(batch_size): agent.update(*batch) agent.decay_epsilon(t/T) episode_rewards += r_t for i in range(env.num_envs): if d_t[i]: rewards.append(episode_rewards[i]) episode_rewards[i] = 0 s_t[i] = env.reset_at(i) plot(pd.DataFrame(rewards), window=10) return agent In&nbsp;[8]: train(&quot;CartPole-v0&quot;, T=20000, num_envs=32, batch_size=1) WARNING: Logging before flag parsing goes to stderr. W0615 21:46:40.499374 4411987392 deprecation.py:323] From /anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Out[8]: &lt;__main__.Agent at 0x1a2449d668&gt;" />
<meta property="og:description" content="In&nbsp;[1]: import random import gym import numpy as np from collections import deque import tensorflow as tf import pandas as pd import seaborn as sns sns.set() Deep $Q$ networks (DQN)&#182;In previous notebooks, we have seen how we can use tensorflow and autodifferentiation to do tabular $Q$-learning in the context of a regression problem. While this technique is powerful for environments with small (finite) observation spaces $\mathcal{S}$ and action spaces $\mathcal{A}$, we run into problems when our observation space is continuous (or even just large!). Tabular $Q$-learning is only guaranteed to converge if all state-action pairs are visited infinitely many times. In practice, this generally just means a very large number to get a reasonable approximation of the $Q$-function. However, when the observation space becomes large (such as using image inputs), it is likely that we only encounter each state-action pair at most once. Thus, $Q$-learning is not guaranteed to converge. Instead, we want a technique that can estimate $Q$-values such that similar states produce similar outputs. This would allow us to learn from some state-action pairs, and then generalize to other unseen state-action pairs. By using a differentiable function approximator, we get this kind of behaviour. Recall that $Q$-learning is a regression problem, meaning any kind of regression model could work - even a linear regression. However, the most popular model used by deep reinforcement learning researchers is the deep neural network. If you are familiar with supervised learning in deep learning, you may be familiar with techniques like dropout, batch normalization, and activity regularization. So far, these kinds of techniques do not prove extremely useful in the context of reinforcement learning. Instead, fully-connected neural networks with a small number of hidden units consisting of rectified linear units tend to perform best. (Pieter Abbeel notes that on simple problems, linear feedback control can perform well even in complex environments. Fully-connected neural networks that use ReLU activations function as multi-step piecewise linear feedback controllers, hence their success). When using neural networks, rather than passing many state-action pairs to the network and predicting a scalar $Q(s_t, a_t)$, we pass only the state $s_t$ and produce a vectorized output $\vec{Q}(s_t)$ where each entry in the vector corresponds to the predicted $Q$-value for each action available to the agent. Note that this necessitates that $\mathcal{A}$ is finite (and generally small), a limitation of $DQN$ that we will overcome later in the section on policy gradients. In this notebook, we make use of tensorflow&#39;s keras API to build neural networks. We also take advantage of batched environments to accelerate data collection. The keras api build neural networks that process inputs in batches. This means that if our observation space for a single environment has a shape $84 \times 84 \times 3$, then the network expects inputs of shape $B \times 84 \times 84 \times 3$ where $B$ is the number of inputs in the batch. When running the tabular $Q$-learning agent in tensorflow in the previous notebook, runtime was considerably slower than the simply numpy-based agent. The computation time spent evaluating and updating the policy dominated the time to perform a single step/update of the agent, compared to the time spent simulating a step in the environment. Ideally, the time spent should be 50% policy evaluation and 50% environment stepping. By using batched environments, we can even this out. Furthermore, this means that we get more data per wall-clock-time, which will accelerate learning. This will be different from most tutorials which use keras, where a single environment is used, and inputs are manipulated to trick keras into treating them like a batch. Increasing the number of environments can also stabilize training by diversifying the collected data over time. Target Networks and Experience Replay&#182;In this notebook, we are going to implement two techniques required for stabilizing training: target networks and experience replay. Target Networks&#182;Neural networks are differentiable, meaning that similar inputs produce similar outputs. In most environments, consecutive states are often similar to each other (with small changes occurring as a result of actions chosen). In deep $Q$-learning, we are doing a regression problem with a moving target - we are trying to predict our own output. When we do this, we perform a maximization step over our output: $$ L(\theta) = \frac{1}{2} \left( r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q_\theta(s_{t+1}, a_{t+1}) \right) - Q_\theta(s_t, a_t) \right)^2 $$Minimizing our prediction error, in general, will tend to increase the value of our prediction for $Q(s_t)$ because of this maximization step. This poses a problem. Consider the following sequence of events: The agent is in a state $s_t$ The agent chooses an action that maximizes $Q(s_t)$ The state transitions from $s_t$ to $s_{t+1}$, giving a reward of $r_t$ and a terminal flag $d_t$. Using the transition $s_t, a_t, r_t, s_{t+1}, d_t$, the agent minimizes $L(\theta)$ Since $s_{t+1}$ and $s_t$ are temporally close, they tend to be similar in general. During step 4, we updated our prediction for $Q(s_t)$ according to a maximization over all next possible values of $Q(s_{t+1})$. This tends to increase the value for $Q(s_t)$. When the agent goes to repeat this cycle the next time, its predictions for $Q(s_{t+1})$ will already be higher, because the values for $Q(s_t)$ are higher and $s_t \approx s_{t+1}$. To handle this, we introduce a target network with parameters $\theta^-$ which is used when computing the TD-target $$ r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q_{\theta^-}(s_{t+1}, a_{t+1}) \right) $$This way, the act of updating $\theta$ to minimize $L(\theta)$ has no effect on our regression targets. To keep the predictions made by the target networks $Q_{\theta^-}$ somewhat in line with the actual $Q$-network, we synchronize parameters every fixed number of timesteps. Experience Replay&#182;The vanilla $Q$-learning algorithm (and DQN, as thus far described) uses only the most recent transition to train on. While this makes the agent good at predicting recent $Q$-values, it can cause it to perform worse on older or uncommon transitions. To stabilize training, we instead store transitions in memory and sample them randomly from batches to ensure the agent gets a good mix of experiences at each training step. In&nbsp;[2]: class ReplayBuffer: def __init__(self, size=1000000): self.memory = deque(maxlen=size) def remember(self, s_t, a_t, r_t, s_t_next, d_t): self.memory.append((s_t, a_t, r_t, s_t_next, d_t)) def sample(self, num=32): num = min(num, len(self.memory)) return random.sample(self.memory, num) In&nbsp;[3]: class Agent: def __init__(self, state_shape, num_actions, num_envs, alpha=0.001, gamma=0.95, epsilon_i=1.0, epsilon_f=0.01, n_epsilon=0.1, hidden_sizes = []): self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.n_epsilon = n_epsilon self.epsilon = epsilon_i self.gamma = gamma self.num_actions = num_actions self.num_envs = num_envs self.Q = tf.keras.models.Sequential() self.Q.add(tf.keras.layers.Input(shape=state_shape)) for size in hidden_sizes: self.Q.add(tf.keras.layers.Dense(size, activation=&#39;relu&#39;, use_bias=&#39;false&#39;, kernel_initializer=&#39;he_uniform&#39;, dtype=&#39;float64&#39;)) self.Q.add(tf.keras.layers.Dense(self.num_actions, activation=&quot;linear&quot;, use_bias=&#39;false&#39;, kernel_initializer=&#39;zeros&#39;, dtype=&#39;float64&#39;)) # target network self.Q_ = tf.keras.models.Sequential() self.Q_.add(tf.keras.layers.Input(shape=state_shape)) for size in hidden_sizes: self.Q_.add(tf.keras.layers.Dense(size, activation=&#39;relu&#39;, use_bias=&#39;false&#39;, kernel_initializer=&#39;he_uniform&#39;, dtype=&#39;float64&#39;)) self.Q_.add(tf.keras.layers.Dense(self.num_actions, activation=&quot;linear&quot;, use_bias=&#39;false&#39;, kernel_initializer=&#39;zeros&#39;, dtype=&#39;float64&#39;)) self.optimizer = tf.keras.optimizers.Adam(alpha) def synchronize(self): self.Q_.set_weights(self.Q.get_weights()) def act(self, s_t): if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions, size=self.num_envs) return np.argmax(self.Q(s_t), axis=1) def decay_epsilon(self, n): self.epsilon = max( self.epsilon_f, self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f)) def update(self, s_t, a_t, r_t, s_t_next, d_t): with tf.GradientTape() as tape: Q_next = tf.stop_gradient(tf.reduce_max(self.Q_(s_t_next), axis=1)) # note we use Q_ Q_pred = tf.reduce_sum(self.Q(s_t)*tf.one_hot(a_t, self.num_actions, dtype=tf.float64), axis=1) loss = tf.reduce_mean(0.5*(r_t + (1-d_t)*self.gamma*Q_next - Q_pred)**2) grads = tape.gradient(loss, self.Q.trainable_variables) self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables)) In&nbsp;[4]: class DiscreteToBoxWrapper(gym.ObservationWrapper): def __init__(self, env): super().__init__(env) assert isinstance(env.observation_space, gym.spaces.Discrete), \ &quot;Should only be used to wrap Discrete envs.&quot; self.n = self.observation_space.n self.observation_space = gym.spaces.Box(0, 1, (self.n,)) def observation(self, obs): new_obs = np.zeros(self.n) new_obs[obs] = 1 return new_obs In&nbsp;[5]: class VectorizedEnvWrapper(gym.Wrapper): def __init__(self, make_env, num_envs=1): super().__init__(make_env()) self.num_envs = num_envs self.envs = [make_env() for env_index in range(num_envs)] def reset(self): return np.asarray([env.reset() for env in self.envs]) def reset_at(self, env_index): return self.envs[env_index].reset() def step(self, actions): next_states, rewards, dones, infos = [], [], [], [] for env, action in zip(self.envs, actions): next_state, reward, done, info = env.step(action) next_states.append(next_state) rewards.append(reward) dones.append(done) infos.append(info) return np.asarray(next_states), np.asarray(rewards), \ np.asarray(dones), np.asarray(infos) In&nbsp;[6]: def plot(data, window=100): sns.lineplot( data=data.rolling(window=window).mean()[window-1::window] ) In&nbsp;[7]: def train(env_name, T=20000, num_envs=32, batch_size=32, sync_every=100, hidden_sizes=[24, 24], alpha=0.001, gamma=0.95): env = VectorizedEnvWrapper(lambda: gym.make(env_name), num_envs) state_shape = env.observation_space.shape num_actions = env.action_space.n agent = Agent(state_shape, num_actions, num_envs, alpha=alpha, hidden_sizes=hidden_sizes, gamma=gamma) rewards = [] buffer = ReplayBuffer() episode_rewards = 0 s_t = env.reset() for t in range(T): if t%sync_every == 0: agent.synchronize() a_t = agent.act(s_t) s_t_next, r_t, d_t, info = env.step(a_t) buffer.remember(s_t, a_t, r_t, s_t_next, d_t) s_t = s_t_next for batch in buffer.sample(batch_size): agent.update(*batch) agent.decay_epsilon(t/T) episode_rewards += r_t for i in range(env.num_envs): if d_t[i]: rewards.append(episode_rewards[i]) episode_rewards[i] = 0 s_t[i] = env.reset_at(i) plot(pd.DataFrame(rewards), window=10) return agent In&nbsp;[8]: train(&quot;CartPole-v0&quot;, T=20000, num_envs=32, batch_size=1) WARNING: Logging before flag parsing goes to stderr. W0615 21:46:40.499374 4411987392 deprecation.py:323] From /anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Out[8]: &lt;__main__.Agent at 0x1a2449d668&gt;" />
<link rel="canonical" href="http://localhost:4000/deep-q-learning/" />
<meta property="og:url" content="http://localhost:4000/deep-q-learning/" />
<meta property="og:site_name" content="alexandervandekleut.github.io" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-12T00:00:00-04:00" />
<script type="application/ld+json">
{"description":"In&nbsp;[1]: import random import gym import numpy as np from collections import deque import tensorflow as tf import pandas as pd import seaborn as sns sns.set() Deep $Q$ networks (DQN)&#182;In previous notebooks, we have seen how we can use tensorflow and autodifferentiation to do tabular $Q$-learning in the context of a regression problem. While this technique is powerful for environments with small (finite) observation spaces $\\mathcal{S}$ and action spaces $\\mathcal{A}$, we run into problems when our observation space is continuous (or even just large!). Tabular $Q$-learning is only guaranteed to converge if all state-action pairs are visited infinitely many times. In practice, this generally just means a very large number to get a reasonable approximation of the $Q$-function. However, when the observation space becomes large (such as using image inputs), it is likely that we only encounter each state-action pair at most once. Thus, $Q$-learning is not guaranteed to converge. Instead, we want a technique that can estimate $Q$-values such that similar states produce similar outputs. This would allow us to learn from some state-action pairs, and then generalize to other unseen state-action pairs. By using a differentiable function approximator, we get this kind of behaviour. Recall that $Q$-learning is a regression problem, meaning any kind of regression model could work - even a linear regression. However, the most popular model used by deep reinforcement learning researchers is the deep neural network. If you are familiar with supervised learning in deep learning, you may be familiar with techniques like dropout, batch normalization, and activity regularization. So far, these kinds of techniques do not prove extremely useful in the context of reinforcement learning. Instead, fully-connected neural networks with a small number of hidden units consisting of rectified linear units tend to perform best. (Pieter Abbeel notes that on simple problems, linear feedback control can perform well even in complex environments. Fully-connected neural networks that use ReLU activations function as multi-step piecewise linear feedback controllers, hence their success). When using neural networks, rather than passing many state-action pairs to the network and predicting a scalar $Q(s_t, a_t)$, we pass only the state $s_t$ and produce a vectorized output $\\vec{Q}(s_t)$ where each entry in the vector corresponds to the predicted $Q$-value for each action available to the agent. Note that this necessitates that $\\mathcal{A}$ is finite (and generally small), a limitation of $DQN$ that we will overcome later in the section on policy gradients. In this notebook, we make use of tensorflow&#39;s keras API to build neural networks. We also take advantage of batched environments to accelerate data collection. The keras api build neural networks that process inputs in batches. This means that if our observation space for a single environment has a shape $84 \\times 84 \\times 3$, then the network expects inputs of shape $B \\times 84 \\times 84 \\times 3$ where $B$ is the number of inputs in the batch. When running the tabular $Q$-learning agent in tensorflow in the previous notebook, runtime was considerably slower than the simply numpy-based agent. The computation time spent evaluating and updating the policy dominated the time to perform a single step/update of the agent, compared to the time spent simulating a step in the environment. Ideally, the time spent should be 50% policy evaluation and 50% environment stepping. By using batched environments, we can even this out. Furthermore, this means that we get more data per wall-clock-time, which will accelerate learning. This will be different from most tutorials which use keras, where a single environment is used, and inputs are manipulated to trick keras into treating them like a batch. Increasing the number of environments can also stabilize training by diversifying the collected data over time. Target Networks and Experience Replay&#182;In this notebook, we are going to implement two techniques required for stabilizing training: target networks and experience replay. Target Networks&#182;Neural networks are differentiable, meaning that similar inputs produce similar outputs. In most environments, consecutive states are often similar to each other (with small changes occurring as a result of actions chosen). In deep $Q$-learning, we are doing a regression problem with a moving target - we are trying to predict our own output. When we do this, we perform a maximization step over our output: $$ L(\\theta) = \\frac{1}{2} \\left( r_t + (1-d_t)\\gamma \\max_{a_{t+1}} \\left( Q_\\theta(s_{t+1}, a_{t+1}) \\right) - Q_\\theta(s_t, a_t) \\right)^2 $$Minimizing our prediction error, in general, will tend to increase the value of our prediction for $Q(s_t)$ because of this maximization step. This poses a problem. Consider the following sequence of events: The agent is in a state $s_t$ The agent chooses an action that maximizes $Q(s_t)$ The state transitions from $s_t$ to $s_{t+1}$, giving a reward of $r_t$ and a terminal flag $d_t$. Using the transition $s_t, a_t, r_t, s_{t+1}, d_t$, the agent minimizes $L(\\theta)$ Since $s_{t+1}$ and $s_t$ are temporally close, they tend to be similar in general. During step 4, we updated our prediction for $Q(s_t)$ according to a maximization over all next possible values of $Q(s_{t+1})$. This tends to increase the value for $Q(s_t)$. When the agent goes to repeat this cycle the next time, its predictions for $Q(s_{t+1})$ will already be higher, because the values for $Q(s_t)$ are higher and $s_t \\approx s_{t+1}$. To handle this, we introduce a target network with parameters $\\theta^-$ which is used when computing the TD-target $$ r_t + (1-d_t)\\gamma \\max_{a_{t+1}} \\left( Q_{\\theta^-}(s_{t+1}, a_{t+1}) \\right) $$This way, the act of updating $\\theta$ to minimize $L(\\theta)$ has no effect on our regression targets. To keep the predictions made by the target networks $Q_{\\theta^-}$ somewhat in line with the actual $Q$-network, we synchronize parameters every fixed number of timesteps. Experience Replay&#182;The vanilla $Q$-learning algorithm (and DQN, as thus far described) uses only the most recent transition to train on. While this makes the agent good at predicting recent $Q$-values, it can cause it to perform worse on older or uncommon transitions. To stabilize training, we instead store transitions in memory and sample them randomly from batches to ensure the agent gets a good mix of experiences at each training step. In&nbsp;[2]: class ReplayBuffer: def __init__(self, size=1000000): self.memory = deque(maxlen=size) def remember(self, s_t, a_t, r_t, s_t_next, d_t): self.memory.append((s_t, a_t, r_t, s_t_next, d_t)) def sample(self, num=32): num = min(num, len(self.memory)) return random.sample(self.memory, num) In&nbsp;[3]: class Agent: def __init__(self, state_shape, num_actions, num_envs, alpha=0.001, gamma=0.95, epsilon_i=1.0, epsilon_f=0.01, n_epsilon=0.1, hidden_sizes = []): self.epsilon_i = epsilon_i self.epsilon_f = epsilon_f self.n_epsilon = n_epsilon self.epsilon = epsilon_i self.gamma = gamma self.num_actions = num_actions self.num_envs = num_envs self.Q = tf.keras.models.Sequential() self.Q.add(tf.keras.layers.Input(shape=state_shape)) for size in hidden_sizes: self.Q.add(tf.keras.layers.Dense(size, activation=&#39;relu&#39;, use_bias=&#39;false&#39;, kernel_initializer=&#39;he_uniform&#39;, dtype=&#39;float64&#39;)) self.Q.add(tf.keras.layers.Dense(self.num_actions, activation=&quot;linear&quot;, use_bias=&#39;false&#39;, kernel_initializer=&#39;zeros&#39;, dtype=&#39;float64&#39;)) # target network self.Q_ = tf.keras.models.Sequential() self.Q_.add(tf.keras.layers.Input(shape=state_shape)) for size in hidden_sizes: self.Q_.add(tf.keras.layers.Dense(size, activation=&#39;relu&#39;, use_bias=&#39;false&#39;, kernel_initializer=&#39;he_uniform&#39;, dtype=&#39;float64&#39;)) self.Q_.add(tf.keras.layers.Dense(self.num_actions, activation=&quot;linear&quot;, use_bias=&#39;false&#39;, kernel_initializer=&#39;zeros&#39;, dtype=&#39;float64&#39;)) self.optimizer = tf.keras.optimizers.Adam(alpha) def synchronize(self): self.Q_.set_weights(self.Q.get_weights()) def act(self, s_t): if np.random.rand() &lt; self.epsilon: return np.random.randint(self.num_actions, size=self.num_envs) return np.argmax(self.Q(s_t), axis=1) def decay_epsilon(self, n): self.epsilon = max( self.epsilon_f, self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f)) def update(self, s_t, a_t, r_t, s_t_next, d_t): with tf.GradientTape() as tape: Q_next = tf.stop_gradient(tf.reduce_max(self.Q_(s_t_next), axis=1)) # note we use Q_ Q_pred = tf.reduce_sum(self.Q(s_t)*tf.one_hot(a_t, self.num_actions, dtype=tf.float64), axis=1) loss = tf.reduce_mean(0.5*(r_t + (1-d_t)*self.gamma*Q_next - Q_pred)**2) grads = tape.gradient(loss, self.Q.trainable_variables) self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables)) In&nbsp;[4]: class DiscreteToBoxWrapper(gym.ObservationWrapper): def __init__(self, env): super().__init__(env) assert isinstance(env.observation_space, gym.spaces.Discrete), \\ &quot;Should only be used to wrap Discrete envs.&quot; self.n = self.observation_space.n self.observation_space = gym.spaces.Box(0, 1, (self.n,)) def observation(self, obs): new_obs = np.zeros(self.n) new_obs[obs] = 1 return new_obs In&nbsp;[5]: class VectorizedEnvWrapper(gym.Wrapper): def __init__(self, make_env, num_envs=1): super().__init__(make_env()) self.num_envs = num_envs self.envs = [make_env() for env_index in range(num_envs)] def reset(self): return np.asarray([env.reset() for env in self.envs]) def reset_at(self, env_index): return self.envs[env_index].reset() def step(self, actions): next_states, rewards, dones, infos = [], [], [], [] for env, action in zip(self.envs, actions): next_state, reward, done, info = env.step(action) next_states.append(next_state) rewards.append(reward) dones.append(done) infos.append(info) return np.asarray(next_states), np.asarray(rewards), \\ np.asarray(dones), np.asarray(infos) In&nbsp;[6]: def plot(data, window=100): sns.lineplot( data=data.rolling(window=window).mean()[window-1::window] ) In&nbsp;[7]: def train(env_name, T=20000, num_envs=32, batch_size=32, sync_every=100, hidden_sizes=[24, 24], alpha=0.001, gamma=0.95): env = VectorizedEnvWrapper(lambda: gym.make(env_name), num_envs) state_shape = env.observation_space.shape num_actions = env.action_space.n agent = Agent(state_shape, num_actions, num_envs, alpha=alpha, hidden_sizes=hidden_sizes, gamma=gamma) rewards = [] buffer = ReplayBuffer() episode_rewards = 0 s_t = env.reset() for t in range(T): if t%sync_every == 0: agent.synchronize() a_t = agent.act(s_t) s_t_next, r_t, d_t, info = env.step(a_t) buffer.remember(s_t, a_t, r_t, s_t_next, d_t) s_t = s_t_next for batch in buffer.sample(batch_size): agent.update(*batch) agent.decay_epsilon(t/T) episode_rewards += r_t for i in range(env.num_envs): if d_t[i]: rewards.append(episode_rewards[i]) episode_rewards[i] = 0 s_t[i] = env.reset_at(i) plot(pd.DataFrame(rewards), window=10) return agent In&nbsp;[8]: train(&quot;CartPole-v0&quot;, T=20000, num_envs=32, batch_size=1) WARNING: Logging before flag parsing goes to stderr. W0615 21:46:40.499374 4411987392 deprecation.py:323] From /anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Out[8]: &lt;__main__.Agent at 0x1a2449d668&gt;","@type":"BlogPosting","url":"http://localhost:4000/deep-q-learning/","headline":"Deep Q Learning","dateModified":"2019-05-12T00:00:00-04:00","datePublished":"2019-05-12T00:00:00-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/deep-q-learning/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <h1 id="project_title"> TF 2.0 for Reinforcement Learning </h1>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        
        <h4>
          <a href="http://localhost:4000">Home</a>
        </h4>
        
        
        <p> Download the <a href="http://localhost:4000/assets/notebooks/deep-q-learning.ipynb"> notebook </a> or follow along. </p>
        
        
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">deque</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Deep-$Q$-networks-(DQN)">Deep $Q$ networks (DQN)<a class="anchor-link" href="#Deep-$Q$-networks-(DQN)">&#182;</a></h1><p>In previous notebooks, we have seen how we can use <code>tensorflow</code> and autodifferentiation to do tabular $Q$-learning in the context of a regression problem. While this technique is powerful for environments with small (finite) observation spaces $\mathcal{S}$ and action spaces $\mathcal{A}$, we run into problems when our observation space is continuous (or even just large!).</p>
<p>Tabular $Q$-learning is only guaranteed to converge if all state-action pairs are visited infinitely many times. In practice, this generally just means a very large number to get a reasonable approximation of the $Q$-function. However, when the observation space becomes large (such as using image inputs), it is likely that we only encounter each state-action pair at most once. Thus, $Q$-learning is not guaranteed to converge.</p>
<p>Instead, we want a technique that can estimate $Q$-values such that similar states produce similar outputs. This would allow us to learn from some state-action pairs, and then generalize to other unseen state-action pairs. By using a <strong>differentiable function approximator</strong>, we get this kind of behaviour. Recall that $Q$-learning is a <em>regression</em> problem, meaning any kind of regression model could work - even a linear regression. However, the most popular model used by <em>deep</em> reinforcement learning researchers is the <em>deep</em> neural network.</p>
<p>If you are familiar with supervised learning in deep learning, you may be familiar with techniques like dropout, batch normalization, and activity regularization. So far, these kinds of techniques do not prove extremely useful in the context of reinforcement learning. Instead, fully-connected neural networks with a small number of hidden units consisting of rectified linear units tend to perform best. (<a href="https://www.youtube.com/watch?v=l-mYLq6eZPY">Pieter Abbeel</a> notes that on simple problems, linear feedback control can perform well even in complex environments. Fully-connected neural networks that use ReLU activations function as multi-step piecewise linear feedback controllers, hence their success).</p>
<p>When using neural networks, rather than passing many state-action pairs to the network and predicting a scalar $Q(s_t, a_t)$, we pass only the state $s_t$ and produce a vectorized output $\vec{Q}(s_t)$ where each entry in the vector corresponds to the predicted $Q$-value for each action available to the agent. Note that this necessitates that $\mathcal{A}$ is finite (and generally small), a limitation of $DQN$ that we will overcome later in the section on policy gradients.</p>
<p><img src="images/q-network.png" alt="**missing image (q-network)**" /></p>
<p>In this notebook, we make use of <code>tensorflow</code>'s <code>keras</code> API to build neural networks. We also take advantage of <strong>batched environments</strong> to accelerate data collection. The <code>keras</code> api build neural networks that process inputs in <strong>batches</strong>. This means that if our observation space for a single environment has a shape $84 \times 84 \times 3$, then the network expects inputs of shape $B \times 84 \times 84 \times 3$ where $B$ is the number of inputs in the batch.</p>
<p>When running the tabular $Q$-learning agent in <code>tensorflow</code> in the previous notebook, runtime was considerably slower than the simply numpy-based agent. The computation time spent evaluating and updating the policy dominated the time to perform a single step/update of the agent, compared to the time spent simulating a step in the environment. Ideally, the time spent should be 50% policy evaluation and 50% environment stepping. By using batched environments, we can even this out. Furthermore, this means that we get more data per wall-clock-time, which will accelerate learning. This will be different from most tutorials which use <code>keras</code>, where a single environment is used, and inputs are manipulated to trick keras into treating them like a batch. Increasing the number of environments can also stabilize training by diversifying the collected data over time.</p>
<h3 id="Target-Networks-and-Experience-Replay">Target Networks and Experience Replay<a class="anchor-link" href="#Target-Networks-and-Experience-Replay">&#182;</a></h3><p>In this notebook, we are going to implement two techniques required for stabilizing training: <strong>target networks</strong> and <strong>experience replay</strong>.</p>
<h3 id="Target-Networks">Target Networks<a class="anchor-link" href="#Target-Networks">&#182;</a></h3><p>Neural networks are <em>differentiable</em>, meaning that similar inputs produce similar outputs. In most environments, consecutive states are often similar to each other (with small changes occurring as a result of actions chosen). In deep $Q$-learning, we are doing a regression problem with a moving target - we are trying to predict our own output. When we do this, we perform a maximization step over our output:</p>
$$
L(\theta) = \frac{1}{2} \left( r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q_\theta(s_{t+1}, a_{t+1}) \right) - Q_\theta(s_t, a_t) \right)^2
$$<p>Minimizing our prediction error, in general, will tend to increase the value of our prediction for $Q(s_t)$ because of this maximization step. This poses a problem. Consider the following sequence of events:</p>
<ol>
<li>The agent is in a state $s_t$</li>
<li>The agent chooses an action that maximizes $Q(s_t)$</li>
<li>The state transitions from $s_t$ to $s_{t+1}$, giving a reward of $r_t$ and a terminal flag $d_t$.</li>
<li>Using the transition $s_t, a_t, r_t, s_{t+1}, d_t$, the agent minimizes $L(\theta)$</li>
</ol>
<p>Since $s_{t+1}$ and $s_t$ are temporally close, they tend to be similar in general. During step 4, we updated our prediction for $Q(s_t)$ according to a maximization over all next possible values of $Q(s_{t+1})$. This tends to increase the value for $Q(s_t)$. When the agent goes to repeat this cycle the next time, its predictions for $Q(s_{t+1})$ will already be higher, because the values for $Q(s_t)$ are higher and $s_t \approx s_{t+1}$.</p>
<p>To handle this, we introduce a <strong>target network</strong> with parameters $\theta^-$ which is used when computing the TD-target</p>
$$
r_t + (1-d_t)\gamma \max_{a_{t+1}} \left( Q_{\theta^-}(s_{t+1}, a_{t+1}) \right)
$$<p>This way, the act of updating $\theta$ to minimize $L(\theta)$ has no effect on our regression targets. To keep the predictions made by the target networks $Q_{\theta^-}$ somewhat in line with the actual $Q$-network, we synchronize parameters every fixed number of timesteps.</p>
<h3 id="Experience-Replay">Experience Replay<a class="anchor-link" href="#Experience-Replay">&#182;</a></h3><p>The vanilla $Q$-learning algorithm (and DQN, as thus far described) uses only the most recent transition to train on. While this makes the agent good at predicting recent $Q$-values, it can cause it to perform worse on older or uncommon transitions. To stabilize training, we instead store transitions in memory and sample them randomly from batches to ensure the agent gets a good mix of experiences at each training step.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">remember</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">d_t</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">d_t</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="n">num</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> <span class="n">num_envs</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">epsilon_i</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon_f</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">hidden_sizes</span> <span class="o">=</span> <span class="p">[]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">=</span> <span class="n">epsilon_i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span> <span class="o">=</span> <span class="n">epsilon_f</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_epsilon</span> <span class="o">=</span> <span class="n">n_epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon_i</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">num_actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_envs</span> <span class="o">=</span> <span class="n">num_envs</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">state_shape</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">hidden_sizes</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="s1">&#39;false&#39;</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;he_uniform&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float64&#39;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="s1">&#39;false&#39;</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float64&#39;</span><span class="p">))</span>
        
        <span class="c1"># target network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q_</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">state_shape</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">hidden_sizes</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Q_</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="s1">&#39;false&#39;</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;he_uniform&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float64&#39;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q_</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="s1">&#39;false&#39;</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float64&#39;</span><span class="p">))</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>  
    
    <span class="k">def</span> <span class="nf">synchronize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q_</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_envs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">s_t</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">decay_epsilon</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span><span class="p">,</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">-</span> <span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">n_epsilon</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_i</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_f</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">d_t</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">Q_next</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q_</span><span class="p">(</span><span class="n">s_t_next</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># note we use Q_ </span>
            <span class="n">Q_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">s_t</span><span class="p">)</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">a_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">r_t</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">d_t</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">Q_next</span> <span class="o">-</span> <span class="n">Q_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DiscreteToBoxWrapper</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">ObservationWrapper</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">),</span> \
            <span class="s2">&quot;Should only be used to wrap Discrete envs.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,))</span>
    
    <span class="k">def</span> <span class="nf">observation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">):</span>
        <span class="n">new_obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>
        <span class="n">new_obs</span><span class="p">[</span><span class="n">obs</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">new_obs</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">VectorizedEnvWrapper</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">Wrapper</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">make_env</span><span class="p">,</span> <span class="n">num_envs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">make_env</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_envs</span> <span class="o">=</span> <span class="n">num_envs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">envs</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_env</span><span class="p">()</span> <span class="k">for</span> <span class="n">env_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_envs</span><span class="p">)]</span>
    
    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span> <span class="k">for</span> <span class="n">env</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">envs</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">reset_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env_index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">envs</span><span class="p">[</span><span class="n">env_index</span><span class="p">]</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
        <span class="n">next_states</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">env</span><span class="p">,</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">envs</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">next_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
            <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
            <span class="n">dones</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">done</span><span class="p">)</span>
            <span class="n">infos</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">info</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">next_states</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">rewards</span><span class="p">),</span> \
            <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">dones</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">infos</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">::</span><span class="n">window</span><span class="p">]</span>
    <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">env_name</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">num_envs</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">sync_every</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">):</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">VectorizedEnvWrapper</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">),</span> <span class="n">num_envs</span><span class="p">)</span>
    <span class="n">state_shape</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">num_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">state_shape</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> <span class="n">num_envs</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="o">=</span><span class="n">hidden_sizes</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">)</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">()</span>
    <span class="n">episode_rewards</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">s_t</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">t</span><span class="o">%</span><span class="k">sync_every</span> == 0:
            <span class="n">agent</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        
        <span class="n">a_t</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s_t</span><span class="p">)</span>
        <span class="n">s_t_next</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">d_t</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a_t</span><span class="p">)</span>
        <span class="n">buffer</span><span class="o">.</span><span class="n">remember</span><span class="p">(</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t_next</span><span class="p">,</span> <span class="n">d_t</span><span class="p">)</span>
        <span class="n">s_t</span> <span class="o">=</span> <span class="n">s_t_next</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">decay_epsilon</span><span class="p">(</span><span class="n">t</span><span class="o">/</span><span class="n">T</span><span class="p">)</span>
        <span class="n">episode_rewards</span> <span class="o">+=</span> <span class="n">r_t</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">num_envs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">d_t</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">episode_rewards</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">s_t</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset_at</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            
    <span class="n">plot</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rewards</span><span class="p">),</span> <span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">agent</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train</span><span class="p">(</span><span class="s2">&quot;CartPole-v0&quot;</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">num_envs</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>WARNING: Logging before flag parsing goes to stderr.
W0615 21:46:40.499374 4411987392 deprecation.py:323] From /anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[8]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;__main__.Agent at 0x1a2449d668&gt;</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>


        
        <h4>
          <a href="http://localhost:4000">Home</a>
        </h4>
        
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">alexandervandekleut.github.io maintained by <a href="https://github.com/alexandervandekleut">alexandervandekleut</a></p>
        
      </footer>
    </div>

    
  </body>
</html>
