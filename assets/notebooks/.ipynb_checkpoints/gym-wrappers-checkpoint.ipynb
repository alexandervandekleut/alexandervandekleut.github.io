{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Gym Wrappers\n",
    "\n",
    "In this lesson, we will be learning about the extremely powerful feature of **wrappers** made available to us courtesy of OpenAI's `gym`. Wrappers will allow us to add functionality to environments, such as modifying observations and rewards to be fed to our agent. It is common in reinforcement learning to preprocess observations in order to make them more easy to learn from. A common example is when using image-based inputs, to ensure that all values are between $0$ and $1$ rather than between $0$ and $255$, as is more common with RGB images.\n",
    "\n",
    "The `gym.Wrapper` class inherits from the `gym.Env` class, which defines environments according to the OpenAI API for reinforcement learning. Implementing the `gym.Wrapper` class requires defining an `__init__` method that accepts the environment to be extended as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        # modify ...\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicWrapper(gym.make(\"CartPole-v0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify specific aspects of the environment by using subclasses of `gym.Wrapper` that override how the environment processes observations, rewards, and action.\n",
    "\n",
    "The following three classes provide this functionality:\n",
    "1. `gym.ObservationWrapper`: Used to modify the observations returned by the environment. To do this, override the `observation` method of the environment. This method accepts a single parameter (the observation to be modified) and returns the modified observation.\n",
    "2. `gym.RewardWrapper`: Used to modify the rewards returned by the environment. To do this, override the `reward` method of the environment. This method accepts a single parameter (the reward to be modified) and returns the modified reward.\n",
    "2. `gym.ActionWrapper`: Used to modify the actions passed to the environment. To do this, override the `action` method of the environment. This method accepts a single parameter (the action to be modified) and returns the modified action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        # modify obs\n",
    "        return obs\n",
    "    \n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        # modify rew\n",
    "        return rew\n",
    "    \n",
    "class ActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, act):\n",
    "        # modify act\n",
    "        return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrappers can be used to modify how an environment works to meet the preprocessing criteria of published papers. The [OpenAI Baselines implementations](https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py) include wrappers that reproduce preprocessing used in the original DQN paper and susbequent Deepmind publications.\n",
    "\n",
    "Here we define a wrapper that takes an environment with a `gym.Discrete` observation space and generates a new environment with a one-hot encoding of the discrete states, for use in, for example, neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteToBoxWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete), \\\n",
    "            \"Should only be used to wrap Discrete envs.\"\n",
    "        self.n = self.observation_space.n\n",
    "        self.observation_space = gym.spaces.Box(0, 1, (self.n,))\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        new_obs = np.zeros(self.n)\n",
    "        new_obs[obs] = 1\n",
    "        return new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "env = DiscreteToBoxWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "T = 10\n",
    "s_t = env.reset()\n",
    "for t in range(T):\n",
    "    a_t = env.action_space.sample()\n",
    "    s_t, r_t, done, info = env.step(a_t)\n",
    "    print(s_t)\n",
    "    if done:\n",
    "        s_t = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Beyond the Wrapper Class\n",
    "\n",
    "It is possible to apply the concept of wrappers beyond what is defined here to add functionality to the environment, such as providing auxillary `observation` functions that allow for multiple preprocessing streams to occur.\n",
    "\n",
    "In more complex applications of deep reinforcement learning, evaluating the policy can take significantly longer than stepping the environment. This means that the majority of computational time is spent choosing actions, which makes data collection slow. Since deep reinforcement learning is extremely data intensive (often requiring millions of timesteps of experience to achieve good performance), we should prioritize rapidly acquiring data.\n",
    "\n",
    "The following class accepts a function that returns an environment, and returns a **vectorized** version of the environment. It essentially generates $n$ copies of the environment. Its `step` function expects a vector of $n$ actions, and returns vectors of $n$ next states, $n$ rewards, $n$ done flags, and $n$ infos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, make_env, num_envs=1):\n",
    "        super().__init__(make_env())\n",
    "        self.num_envs = num_envs\n",
    "        self.envs = [make_env() for env_index in range(num_envs)]\n",
    "    \n",
    "    def reset(self):\n",
    "        return np.asarray([env.reset() for env in self.envs])\n",
    "    \n",
    "    def reset_at(self, env_index):\n",
    "        return self.envs[env_index].reset()\n",
    "    \n",
    "    def step(self, actions):\n",
    "        next_states, rewards, dones, infos = [], [], [], []\n",
    "        for env, action in zip(self.envs, actions):\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            infos.append(info)\n",
    "        return np.asarray(next_states), np.asarray(rewards), \\\n",
    "            np.asarray(dones), np.asarray(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 4)\n",
      "(128,)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "num_envs = 128\n",
    "env = VectorizedEnvWrapper(lambda: gym.make(\"CartPole-v0\"), num_envs=num_envs)\n",
    "T = 10\n",
    "observations = env.reset()\n",
    "for t in range(T):\n",
    "    actions = np.random.randint(env.action_space.n, size=num_envs)\n",
    "    observations, rewards, dones, infos = env.step(actions)  \n",
    "    for i in range(len(dones)):\n",
    "        if dones[i]:\n",
    "            observations[i] = env.reset_at(i)\n",
    "print(observations.shape)\n",
    "print(rewards.shape)\n",
    "print(dones.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[Extending OpenAI Gym environments with Wrappers and Monitors](https://hub.packtpub.com/openai-gym-environments-wrappers-and-monitors-tutorial/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
