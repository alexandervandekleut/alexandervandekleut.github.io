{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched Q-Learning with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/rl-lessons/lib/python3.6/site-packages/gym/envs/registration.py:64: UserWarning: register(timestep_limit=1) is deprecated. Use register(max_episode_steps=1) instead.\n",
      "  warnings.warn(\"register(timestep_limit={}) is deprecated. Use register(max_episode_steps={}) instead.\".format(timestep_limit, timestep_limit))\n",
      "/anaconda3/envs/rl-lessons/lib/python3.6/site-packages/gym/envs/registration.py:64: UserWarning: register(timestep_limit=1) is deprecated. Use register(max_episode_steps=1) instead.\n",
      "  warnings.warn(\"register(timestep_limit={}) is deprecated. Use register(max_episode_steps={}) instead.\".format(timestep_limit, timestep_limit))\n",
      "/anaconda3/envs/rl-lessons/lib/python3.6/site-packages/gym/envs/registration.py:64: UserWarning: register(timestep_limit=1) is deprecated. Use register(max_episode_steps=1) instead.\n",
      "  warnings.warn(\"register(timestep_limit={}) is deprecated. Use register(max_episode_steps={}) instead.\".format(timestep_limit, timestep_limit))\n",
      "/anaconda3/envs/rl-lessons/lib/python3.6/site-packages/gym/envs/registration.py:64: UserWarning: register(timestep_limit=1) is deprecated. Use register(max_episode_steps=1) instead.\n",
      "  warnings.warn(\"register(timestep_limit={}) is deprecated. Use register(max_episode_steps={}) instead.\".format(timestep_limit, timestep_limit))\n",
      "/anaconda3/envs/rl-lessons/lib/python3.6/site-packages/gym/envs/registration.py:64: UserWarning: register(timestep_limit=1) is deprecated. Use register(max_episode_steps=1) instead.\n",
      "  warnings.warn(\"register(timestep_limit={}) is deprecated. Use register(max_episode_steps={}) instead.\".format(timestep_limit, timestep_limit))\n",
      "/anaconda3/envs/rl-lessons/lib/python3.6/site-packages/gym/envs/registration.py:64: UserWarning: register(timestep_limit=1) is deprecated. Use register(max_episode_steps=1) instead.\n",
      "  warnings.warn(\"register(timestep_limit={}) is deprecated. Use register(max_episode_steps={}) instead.\".format(timestep_limit, timestep_limit))\n",
      "/anaconda3/envs/rl-lessons/lib/python3.6/site-packages/gym/envs/registration.py:64: UserWarning: register(timestep_limit=1) is deprecated. Use register(max_episode_steps=1) instead.\n",
      "  warnings.warn(\"register(timestep_limit={}) is deprecated. Use register(max_episode_steps={}) instead.\".format(timestep_limit, timestep_limit))\n",
      "/anaconda3/envs/rl-lessons/lib/python3.6/site-packages/gym/envs/registration.py:64: UserWarning: register(timestep_limit=1) is deprecated. Use register(max_episode_steps=1) instead.\n",
      "  warnings.warn(\"register(timestep_limit={}) is deprecated. Use register(max_episode_steps={}) instead.\".format(timestep_limit, timestep_limit))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import gym\n",
    "import gym_bandits\n",
    "import tensorflow as tf\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that it takes significantly longer to run the $Q$-learning agent that uses `tensorflow` than the one that runs purely using `numpy`. The overhead comes from needing to call `sess.run(...)` every time the agent chooses an action. Thus, for each timestep that the agent interacts with the environment, most of the computational cost is dominated by evaluating and updating the policy. Comparatively little time is spent stepping the environment.\n",
    "\n",
    "In the lesson on gym `Wrappers`, we learned how to make a vectorized (or 'batched') environment. The class for this is reproduced here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, make_env, num_envs=1):\n",
    "        super().__init__(make_env())\n",
    "        self.num_envs = num_envs\n",
    "        self.envs = [make_env() for env_index in range(num_envs)]\n",
    "    \n",
    "    def reset(self):\n",
    "        return np.asarray([env.reset() for env in self.envs])\n",
    "    \n",
    "    def reset_at(self, env_index):\n",
    "        return self.envs[env_index].reset()\n",
    "    \n",
    "    def step(self, actions):\n",
    "        next_states, rewards, dones, infos = [], [], [], []\n",
    "        for env, action in zip(self.envs, actions):\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            infos.append(info)\n",
    "        return np.asarray(next_states), np.asarray(rewards), \\\n",
    "            np.asarray(dones), np.asarray(infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By vectorizing the environment, we can greatly accelerate the rate at which the agent learns by collecting more training data per `sess.run(...)` call. We have to make some modifications to our earlier $Q$-learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, \n",
    "                 num_states, \n",
    "                 num_actions,\n",
    "                 num_envs,\n",
    "                 epsilon_i=1.0, \n",
    "                 epsilon_f=0.0, \n",
    "                 n_epsilon=10000, \n",
    "                 alpha=0.1, \n",
    "                 gamma = 0.99):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.epsilon = tf.get_variable(\"epsilon\", initializer=tf.constant(epsilon_i, dtype=tf.float32))\n",
    "        \n",
    "        self.Q = Q = tf.get_variable(\"Q\", initializer = tf.zeros(shape=(num_states, num_actions)))\n",
    "\n",
    "        self.s_t_ph = s_t = tf.placeholder(dtype=tf.int32, shape=(num_envs,), name=\"s_t_ph\")\n",
    "\n",
    "        p = tf.random_uniform(shape=(num_envs,), minval=0, maxval=1, dtype=tf.float32)\n",
    "        self._act = tf.zeros(shape=(num_envs,), dtype=tf.int32)\n",
    "        self.act = tf.where(p<self.epsilon, \n",
    "                           tf.random_uniform(shape=(num_envs,), minval=0, maxval=num_actions, dtype=tf.int32),\n",
    "                             tf.argmax(tf.gather(Q, indices=s_t), axis=1, output_type=tf.int32)\n",
    "                           )\n",
    "  \n",
    "\n",
    "        self.r_t_ph = r_t = tf.placeholder(dtype=tf.float32, shape=(num_envs,), name=\"r_t_ph\")\n",
    "        self.a_t_ph = a_t = tf.placeholder(dtype=tf.int32, shape=(num_envs,), name=\"a_t_ph\")\n",
    "        self.s_t_next_ph = s_t_next = tf.placeholder(dtype=tf.int32, shape=(num_envs,), name=\"s_t_next_ph\")\n",
    "\n",
    "        a_t_next = tf.where(p<self.epsilon, \n",
    "                           tf.random_uniform(shape=(num_envs,), minval=0, maxval=num_actions, dtype=tf.int32),\n",
    "                           tf.argmax(tf.gather(Q, indices=s_t_next), axis=1, output_type=tf.int32)\n",
    "                           )\n",
    "\n",
    "        TD = r_t + gamma*tf.gather_nd(Q, indices=tf.stack([s_t_next, a_t_next], axis=1))\n",
    "        self.TD = TD = tf.stop_gradient(TD)\n",
    "        self.predicted_Q = predicted_Q = tf.gather_nd(Q, indices=tf.stack([s_t, a_t], axis=1))\n",
    "        self.loss = tf.reduce_mean(0.5*(predicted_Q - TD)**2)\n",
    "        self.update = tf.train.GradientDescentOptimizer(alpha).minimize(self.loss) \n",
    "        \n",
    "        self.decay_epsilon = tf.assign(self.epsilon,\n",
    "                                      tf.maximum( \n",
    "                                          epsilon_f,\n",
    "                                          self.epsilon - (epsilon_i - epsilon_f)/n_epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent will be run using the same parameters as the one from the original $Q$-learning notebook. However, for now, this will take significantly longer than just using numpy alone.  We are making calls to `sess.run` but we are not doing a signicant amount of work, making the calls relatively expensive compared to the amount of work that they are doing. We will see later that the more work that is done in a `sess.run` call, the more efficient it becomes. Be patient as the following cell will likely take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0%\r"
     ]
    }
   ],
   "source": [
    "num_envs = 10\n",
    "env = VectorizedEnvWrapper(lambda: gym.make(\"FrozenLake-v0\"), num_envs=num_envs)\n",
    "\n",
    "T = 100000\n",
    "n_epsilon = 0.1*T\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "agent = Agent(num_states, num_actions, num_envs, epsilon_i = 1.0, epsilon_f = 0.0, alpha=0.5, n_epsilon=n_epsilon, gamma=0.95)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    s_t = env.reset()\n",
    "\n",
    "    rewards = [[] for _ in range(num_envs)]\n",
    "    episode_rewards = np.zeros(num_envs)\n",
    "\n",
    "    for t in range(T):\n",
    "        if t%1000 == 0:\n",
    "            print(f'{100*t/T}%', end='\\r')\n",
    "        a_t = sess.run(agent.act, \n",
    "                       feed_dict = {\n",
    "                            agent.s_t_ph: s_t\n",
    "        })\n",
    "        \n",
    "        s_t_next, r_t, done, info = env.step(a_t)\n",
    "\n",
    "        sess.run([agent.decay_epsilon, agent.update],\n",
    "                feed_dict = {\n",
    "                    agent.s_t_ph:s_t,\n",
    "                    agent.a_t_ph:a_t,\n",
    "                    agent.s_t_next_ph:s_t_next,\n",
    "                    agent.r_t_ph:r_t\n",
    "                })\n",
    "\n",
    "        s_t = s_t_next\n",
    "        episode_rewards += r_t\n",
    "\n",
    "        for i in range(num_envs):\n",
    "            if done[i]:\n",
    "                rewards[i].append(episode_rewards[i])\n",
    "                episode_rewards[i] = 0\n",
    "                s_t[i] = env.reset_at(i)\n",
    "window = 100\n",
    "\n",
    "min_len = len(min(rewards, key=lambda x: len(x)))\n",
    "for i in range(num_envs):\n",
    "    rewards[i] = rewards[i][:min_len]\n",
    "\n",
    "rewards = np.array(rewards).mean(axis=0)\n",
    "print(rewards.shape)\n",
    "df = pd.DataFrame(data=rewards).rolling(window=window).mean()[window-1::window]\n",
    "sns.lineplot(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
