{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Markov Reward Processes\n",
    "***\n",
    "A **Markov Reward Process**  is an extension of a Markov Process that allows us to associate rewards with states. Formally, it is a tuple $\\langle \\mathcal{S}, \\mathcal{P}, \\mathcal{R} \\rangle$ that allows us to associate with each state transition $\\langle s_t, s_{t+1} \\rangle$ some reward\n",
    "$$\n",
    "\\mathcal{R}(s_t, s_{t+1}) = \\mathbb{E}\\left[r_t \\vert s_t, s_{t+1} \\right]\n",
    "$$\n",
    "which is often simplified to being $\\mathcal{R}(s_t)$, the reward of being in a particular state $s_t$. For the purpose of this lesson and essentially all implementations, we make this simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_state_transition_matrix(num_states, scale=1):\n",
    "    \"\"\"\n",
    "    Generates a state transition probability matrix with high probability of transitioning to the same state.\n",
    "    Returns:\n",
    "        - a matrix of size num_states x num_states, where state_transition_matrix[i, j] \n",
    "    is the probability of transitioning from state i to state j.\n",
    "    Parameters:\n",
    "        - num_states: An integer representing the number of states of the markov process.\n",
    "    \"\"\"\n",
    "    P = np.eye(num_states)\n",
    "    P[np.arange(num_states)] = 1\n",
    "#     P[np.arange(num_states)-1] = 1\n",
    "#     P = np.exp(np.eye(num_states))\n",
    "#     P = np.eye(num_states) + 0.1\n",
    "    for i in range(num_states): # convert each row to a probability distribution by dividing by the total\n",
    "        P[i] /= sum(P[i])\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reward_matrix(num_states, mu=0, sigma=1):\n",
    "    \"\"\"\n",
    "    Generates a matrix corresponding to the reward of being in a certain state.\n",
    "    Returns:\n",
    "        - A vector of length num_states normally distributed around mean with standard deviation std.\n",
    "    Parameters:\n",
    "        - num_states: the number of states in the Markov reward process\n",
    "        - mu: the mean of the reward distribution\n",
    "        - sigma: the standard deviation of the reward distribution\n",
    "    \"\"\"\n",
    "    return mu + np.random.randn(num_states)*sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 4\n",
    "mu = 0\n",
    "sigma = 4\n",
    "P = generate_state_transition_matrix(num_states)\n",
    "R = generate_reward_matrix(num_states, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19709465 -5.43499524 -4.21029918 12.17190923]\n"
     ]
    }
   ],
   "source": [
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(P, R, T=100, s_0 = 0):\n",
    "    \"\"\"\n",
    "    Generates a trajectory tau for a markov reward process.\n",
    "    Returns:\n",
    "        - a sequence of T integers (states) according to a state transition table P.\n",
    "        - a sequence of T floats (rewards) according to the reward matrix R.\n",
    "    Parameters:\n",
    "        - P: A square matrix where each row is a probability distribution.\n",
    "        - R: A vector where each entry is the reward associated with the corresponding state.\n",
    "        - T: An integer representing the number of timesteps in the trajectory.\n",
    "        - s_0 (default 0): An integer representing the starting state.\n",
    "    \"\"\"\n",
    "    num_states = len(P) # the number of next states we may go to\n",
    "    s_t = s_0 # the current state is the starting state\n",
    "    tau = np.zeros(T, dtype=np.int32) # the trajectory, a fixed-length array of length T\n",
    "    tau[0] = s_t # the first state is the starting state\n",
    "    rewards = np.zeros(T) # the rewards experiences, a fixed-length array of length T\n",
    "    rewards[0] = R[s_0] # the first reward is the reward for being in the first state\n",
    "    \n",
    "    for t in range(1, T): # the length of the rest of the trajectory\n",
    "        s_t = np.random.choice(np.arange(num_states), p=P[s_t]) # randomly select a state using P, where every entry j in P[s_t] is the probability of transitioning from state s_t ot state j\n",
    "        tau[t] = s_t # add this state to our trajectory.\n",
    "        rewards[t] = R[s_t]\n",
    "        \n",
    "    return tau, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau, rewards = generate_trajectory(P, R, 100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 3 3 3 0 0 0 0 0 0 0 0 0 0 0 3 3 1 1 1 1 1 1 1 1 1 1 1 0 3 3 2 2 2 2\n",
      " 2 2 0 0 2 2 0 0 0 0 0 0 2 2 2 2 1 1 1 1 1 3 3 3 3 3 3 3 3 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 2 2 2 2 2 2 2 1 1 1 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.55187295 -4.55187295 -4.55187295 -0.16938909 -0.16938909 -0.16938909\n",
      " -4.55187295 -4.55187295 -4.55187295 -4.55187295 -4.55187295 -4.55187295\n",
      " -4.55187295 -4.55187295 -4.55187295 -4.55187295 -4.55187295 -0.16938909\n",
      " -0.16938909  4.77693661  4.77693661  4.77693661  4.77693661  4.77693661\n",
      "  4.77693661  4.77693661  4.77693661  4.77693661  4.77693661  4.77693661\n",
      " -4.55187295 -0.16938909 -0.16938909 -1.04314511 -1.04314511 -1.04314511\n",
      " -1.04314511 -1.04314511 -1.04314511 -4.55187295 -4.55187295 -1.04314511\n",
      " -1.04314511 -4.55187295 -4.55187295 -4.55187295 -4.55187295 -4.55187295\n",
      " -4.55187295 -1.04314511 -1.04314511 -1.04314511 -1.04314511  4.77693661\n",
      "  4.77693661  4.77693661  4.77693661  4.77693661 -0.16938909 -0.16938909\n",
      " -0.16938909 -0.16938909 -0.16938909 -0.16938909 -0.16938909 -0.16938909\n",
      "  4.77693661  4.77693661  4.77693661  4.77693661  4.77693661  4.77693661\n",
      " -4.55187295 -4.55187295 -4.55187295 -4.55187295 -4.55187295 -4.55187295\n",
      " -4.55187295 -4.55187295 -4.55187295 -4.55187295 -4.55187295 -4.55187295\n",
      "  4.77693661 -1.04314511 -1.04314511 -1.04314511 -1.04314511 -1.04314511\n",
      " -1.04314511 -1.04314511  4.77693661  4.77693661  4.77693661 -0.16938909\n",
      " -0.16938909 -0.16938909 -0.16938909 -0.16938909]\n"
     ]
    }
   ],
   "source": [
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Return and Discounted Return\n",
    "\n",
    "We are interested in trajectories that maximize the **return** $R_t$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_t &= r_t + r_{t+1} +  r_{t+2}+  \\dots + r_T  \\\\\n",
    "    &= \\sum_{k=t}^T r_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When $T$ is finite, we say that the trajectory has a **finite time horizon** and that the environment is **episodic** (happens in episodes).\n",
    "\n",
    "For infinite time horizons, we cannot guarantee that $R_t$ converges. As a result, we might consider discounting rewards exponentially over time in order to guarantee convergence. This line of reasoning leads us to the **discounted return** $G_t$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    G_t &= r_{t} + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots \\\\\n",
    "    &= \\sum_{k=t}^\\infty \\gamma^{k-t} r_k \n",
    "\\end{align}\n",
    "$$\n",
    "where $\\gamma$ is a discount factor between $0$ and $1$ (often close to $1$).\n",
    "\n",
    "We sometimes refer to both the discounted and undiscounted return as just \"return\" for brevity, and write $G_t$ where for some episodic environments it may be more appropriate to use $R_t$. In fact, it should not be hard to see that $R_t$ is just $G_t$ with $r_t = 0$ for $t > T$ and $\\gamma = 1$.\n",
    "\n",
    "***\n",
    "#### Value Function\n",
    "\n",
    "We can use the expected value of $G_t$ to determine the **value** of being a certain state $s_t$:\n",
    "\n",
    "$$\n",
    "V(s_t) = \\mathbb{E}\\left[ G_t \\big\\vert s_t \\right]\n",
    "$$\n",
    "\n",
    "We can decompose $V(s_t)$ into two parts: the immediate reward $r_t$ and the discounted value of being in the next state $s_{t+1}$:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    \\begin{split}\n",
    "        V(s_t) &= \\mathbb{E} \\left[ G_t \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E} \\left[ r_{t} + \\gamma r_{t+1} + \\dots + \\gamma^2 r_{t+2} \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E}\\left[r_{t} + \\gamma (r_{t+1} + \\gamma r_{t+2} + \\dots) \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E}\\left[ r_{t} + \\gamma G_{t+1} \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E} \\left[ r_{t} + \\gamma V(s_{t+1}) \\big\\vert s_t \\right] \\\\\n",
    "    \\end{split}\n",
    "\\end{align}\n",
    "\n",
    "This last form of $V(s_t)$ is known as the **Bellman Equation**.\n",
    "\n",
    "***\n",
    "#### TD(0)\n",
    "\n",
    "Say we are interesting in learning to predict $V(s_t)$. We can use a simple method called **TD(0)** to approximate $V(s_t)$ when our observation space $\\mathcal{S}$ is **discrete** (i.e., finite). We begin by initializing a vector $V$ with random values, which will serve as our initial predictions for what $V(s_t)$ is.\n",
    "\n",
    "According to the Bellman Equation, we can use $r_t + \\gamma V(s_{t+1})$ (which we call the **TD-target**) as an unbiased estimator for $V(s_t)$. Then we can modify our estimate of $V(s_t)$ to more closely match $r_t + \\gamma V(s_{t+1})$ according to a learning rate $\\alpha$ as follows:\n",
    "$$\n",
    "V(s_t) \\gets V(s_t) + \\alpha \\left( r_t + V(s_{t+1}) - V(s_t) \\right)\n",
    "$$\n",
    "\n",
    "Let's use the TD(0) algorithm to attempt to learn the value function of this Markov reward process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_value_predictions(num_states):\n",
    "    \"\"\"\n",
    "    Generates a table of predictions for the values of each state.\n",
    "    Returns:\n",
    "        - A randomly initialized vector of length num_states with values between 0 and 1.\n",
    "    Parameters:\n",
    "        - num_states: an integer corresponding to the number of states in the Markov reward process.\n",
    "    \"\"\"\n",
    "    return np.random.rand(num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_0(V, P, R, T=100, s_0=0, gamma=0.99, alpha=0.1):\n",
    "    num_states = len(P) # the number of next states we may go to\n",
    "    s_t = s_0 # the current state is the starting state\n",
    "    r_t = R[s_t]\n",
    "    \n",
    "    for t in range(1, T): # the length of the rest of the trajectory\n",
    "        s_t_next = np.random.choice(np.arange(num_states), p=P[s_t]) # randomly select a state using P, where every entry j in P[s_t] is the probability of transitioning from state s_t ot state j\n",
    "#         print('current state: {}\\t next state: {}'.format(s_t, s_t_next))\n",
    "#         print('current V: {} \\t better V: {}'.format( V[s_t], (r_t + gamma*V[s_t_next])))\n",
    "        V[s_t] = V[s_t] + alpha*(r_t + gamma*V[s_t_next] - V[s_t]) # update our value function according to the bellman equation\n",
    "        s_t = s_t_next # update our reference to s_t to point to the new state\n",
    "        r_t = R[s_t] # update our reference to r_t to point to the new reward\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = generate_value_predictions(num_states)\n",
    "V = TD_0(V, P, R, 10000, 0, 0.99, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-16.20278853  15.07039545   1.00923978   1.28018893]\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
