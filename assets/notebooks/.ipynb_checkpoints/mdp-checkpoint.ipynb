{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Markov Processes\n",
    "\n",
    "A **Markov Process** is a tuple $ \\langle \\mathcal{S}, \\mathcal{P} \\rangle $ where $\\mathcal{S}$ is a set of **states** called the **observation space** or **state space** that the agent can be in, and $ \\mathcal{P} : \\mathcal{S}^2 \\to \\left[ 0, 1 \\right]$ is a function describing the probability of transitioning from one state to another.\n",
    "$$\n",
    "\\mathcal{P}(s_t, s_{t+1}) = \\mathbb{P} \\left[s_{t+1} \\vert s_t \\right]\n",
    "$$\n",
    "A Markov Processes are used to model stochastic sequences of states $s_0, s_1, \\dots$ satisfying the **Markov property**:\n",
    "$$\n",
    "\\mathbb{P} \\left[ s_{t+1} \\vert s_t \\right] = \\mathbb{P} \\left[ s_{t+1} \\vert s_0, s_1, \\dots, s_t \\right]\n",
    "$$\n",
    "that is, the probability of transitioning from state $s_t$ to state $s_{t+1}$ is independent of previous transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_state_transition_matrix(num_states):\n",
    "    P = np.random.rand(num_states, num_states)\n",
    "    for i in range(num_states): # convert each row to a probability distribution by dividing by the total\n",
    "        P[i] /= sum(P[i])\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: \n",
      "[[0.43365075 0.43263699 0.09132303 0.04238923]\n",
      " [0.19473963 0.39793109 0.13873556 0.26859372]\n",
      " [0.17437947 0.33206503 0.29333298 0.20022252]\n",
      " [0.08424129 0.22288814 0.38442623 0.30844435]]\n"
     ]
    }
   ],
   "source": [
    "num_states = 4\n",
    "P = generate_state_transition_matrix(num_states)\n",
    "print(f'P: \\n{P}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectories\n",
    "\n",
    "A **trajectory** (denoted $\\tau$) for a Markov process is a (potentially infinite) sequence of states\n",
    "$$\n",
    "\\tau = \\langle s_0, s_1, \\dots, s_T \\rangle\n",
    "$$\n",
    "The probability of generating particular trajectories depends on the underlying dynamics of the process, which is determined by $\\mathcal{P}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(P, T=100, s_0 = 0):\n",
    "    num_states = len(P)\n",
    "    s_t = s_0\n",
    "    tau = np.zeros(T, dtype=np.int32)\n",
    "    tau[0] = s_t\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        # Choose the next state using P[s_t] as the state \n",
    "        # transition probability distribution.\n",
    "        s_t = np.random.choice(np.arange(num_states), p=P[s_t])\n",
    "        tau[t] = s_t\n",
    "        \n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau: \n",
      "[0 3 3 3 1 3 1 3 3 1 3 3 3 1 3 3 3 1 2 2 3 1 3 3 3 2 3 3 1 1 2 0 0 0 0 0 3\n",
      " 3 1 1 1 2 2 2 1 3 1 2 0 1 1 3 3 0 1 1 2 1 2 3 2 1 0 1 0 0 0 1 1 2 1 0 0 1\n",
      " 0 0 1 1 0 0 0 3 1 3 0 1 0 2 2 3 1 3 2 2 1 3 1 1 3 3]\n"
     ]
    }
   ],
   "source": [
    "T = 100\n",
    "s_0 = 0\n",
    "tau = generate_trajectory(P, T, s_0)\n",
    "print(f'tau: \\n{tau}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Markov Reward Processes\n",
    "\n",
    "A **Markov Reward Process**  is an extension of a Markov Process that allows us to associate rewards with states. Formally, it is a tuple $\\langle \\mathcal{S}, \\mathcal{P}, \\mathcal{R} \\rangle$ that allows us to associate with each state transition $\\langle s_t, s_{t+1} \\rangle$ some reward\n",
    "$$\n",
    "\\mathcal{R}(s_t, s_{t+1}) = \\mathbb{E}\\left[r_t \\vert s_t, s_{t+1} \\right]\n",
    "$$\n",
    "which is often simplified to being $\\mathcal{R}(s_t)$, the reward of being in a particular state $s_t$. For the purpose of this lesson and essentially all implementations, we make this simplification. The reward $r_t$ can be though of as a measure of how good a certain state $s_t$ is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reward_matrix(num_states, mu=0, sigma=1):\n",
    "    return mu + np.random.randn(num_states)*sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: \n",
      "[[0.55250389 0.13600516 0.11977055 0.19172041]\n",
      " [0.08087023 0.21183225 0.56513127 0.14216626]\n",
      " [0.24059129 0.1108827  0.32471353 0.32381248]\n",
      " [0.36334941 0.43350296 0.05657077 0.14657686]] \n",
      "R: \n",
      "[ 3.35937682  0.43814485  8.04725463 -5.48089562]\n"
     ]
    }
   ],
   "source": [
    "num_states = 4\n",
    "mu = 0\n",
    "sigma = 4\n",
    "P = generate_state_transition_matrix(num_states)\n",
    "R = generate_reward_matrix(num_states, mu, sigma)\n",
    "print(f'P: \\n{P} \\nR: \\n{R}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(P, R, T=100, s_0 = 0):\n",
    "    num_states = len(P)\n",
    "    s_t = s_0\n",
    "    tau = np.zeros(T, dtype=np.int32)\n",
    "    tau[0] = s_t\n",
    "    rewards = np.zeros(T)\n",
    "    rewards[0] = R[s_0]\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        s_t = np.random.choice(np.arange(num_states), p=P[s_t])\n",
    "        tau[t] = s_t\n",
    "        rewards[t] = R[s_t]\n",
    "        \n",
    "    return tau, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau: \n",
      "[0 3 0 0 0 0 0 0 0 0 0 0 1 2 3 1 3 1 2 3 1 1 2 0 1 3 1 2 0 0 0 3 0 0 3 1 0\n",
      " 1 2 3 0 3 3 0 3 1 1 2 3 0 0 2 1 1 0 0 2 2 3 3 3 1 2 0 0 3 1 2 2 3 0 3 1 2\n",
      " 2 0 1 2 2 2 0 0 0 0 0 0 0 0 0 0 3 2 2 2 0 0 0 0 3 1] \n",
      "rewards: \n",
      "[ 3.35937682 -5.48089562  3.35937682  3.35937682  3.35937682  3.35937682\n",
      "  3.35937682  3.35937682  3.35937682  3.35937682  3.35937682  3.35937682\n",
      "  0.43814485  8.04725463 -5.48089562  0.43814485 -5.48089562  0.43814485\n",
      "  8.04725463 -5.48089562  0.43814485  0.43814485  8.04725463  3.35937682\n",
      "  0.43814485 -5.48089562  0.43814485  8.04725463  3.35937682  3.35937682\n",
      "  3.35937682 -5.48089562  3.35937682  3.35937682 -5.48089562  0.43814485\n",
      "  3.35937682  0.43814485  8.04725463 -5.48089562  3.35937682 -5.48089562\n",
      " -5.48089562  3.35937682 -5.48089562  0.43814485  0.43814485  8.04725463\n",
      " -5.48089562  3.35937682  3.35937682  8.04725463  0.43814485  0.43814485\n",
      "  3.35937682  3.35937682  8.04725463  8.04725463 -5.48089562 -5.48089562\n",
      " -5.48089562  0.43814485  8.04725463  3.35937682  3.35937682 -5.48089562\n",
      "  0.43814485  8.04725463  8.04725463 -5.48089562  3.35937682 -5.48089562\n",
      "  0.43814485  8.04725463  8.04725463  3.35937682  0.43814485  8.04725463\n",
      "  8.04725463  8.04725463  3.35937682  3.35937682  3.35937682  3.35937682\n",
      "  3.35937682  3.35937682  3.35937682  3.35937682  3.35937682  3.35937682\n",
      " -5.48089562  8.04725463  8.04725463  8.04725463  3.35937682  3.35937682\n",
      "  3.35937682  3.35937682 -5.48089562  0.43814485]\n"
     ]
    }
   ],
   "source": [
    "tau, rewards = generate_trajectory(P, R, 100, 0)\n",
    "print(f'tau: \\n{tau} \\nrewards: \\n{rewards}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Return and Discounted Return\n",
    "\n",
    "If we think of an agent as living in a Markov Reward Process, and we think of the reward $r_t$ as measuring the 'goodness' of certain states $s_t$, then we are interested in trajectories that **maximize** the total rewards over a trajectory (think of it as planning a trip that visits the best tourist spots). We call the cumulative rewards over a trajectory the **return** $R_t$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_t &= r_t + r_{t+1} +  r_{t+2}+  \\dots + r_T  \\\\\n",
    "    &= \\sum_{k=t}^T r_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When $T$ is finite, we say that the trajectory has a **finite time horizon** and that the environment is **episodic** (happens in episodes).\n",
    "\n",
    "In general, we say that Markov Reward Processes always have an infinite time horizon (i.e., $T = \\infty$). Episodic environments are just a special case whereby $s_{t+1} = s_t$ for all $t>T$. \n",
    "\n",
    "If we want to find trajectories that maximize the $R_t$ over an infinite time horizon, the math can get a little hairy without a guarantee that $R_t$ converges. As a result, we might consider discounting rewards exponentially over time in order to guarantee convergence. This line of reasoning leads us to the **discounted return** $G_t$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    G_t &= r_{t} + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots \\\\\n",
    "    &= \\sum_{k=t}^\\infty \\gamma^{k-t} r_k \n",
    "\\end{align}\n",
    "$$\n",
    "where $\\gamma$ is a discount factor between $0$ and $1$ (often close to $1$).\n",
    "\n",
    "We sometimes refer to both the discounted and undiscounted return as just \"return\" for brevity, and write $G_t$ where for some episodic environments it may be more appropriate to use $R_t$. In fact, it should not be hard to see that $R_t$ is just $G_t$ with $r_t = 0$ for $t > T$ and $\\gamma = 1$.\n",
    "\n",
    "***\n",
    "### Value Function\n",
    "\n",
    "We can use the expected value of $G_t$ to determine the **value** of being a certain state $s_t$:\n",
    "\n",
    "\\begin{align}\n",
    "V(s_t) &= \\mathbb{E}\\left[ G_t \\big\\vert s_t \\right] \\\\\n",
    "        &= \\mathbb{E} \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} \\dots\\right]\n",
    "\\end{align}\n",
    "\n",
    "Essentially, the value tells us, given a Markov Reward Process with state transition dynamics $\\mathcal{P}$, what should I expect $G_t$ to be from **this state onwards**? \n",
    "\n",
    "We can decompose $V(s_t)$ into two parts: the immediate reward $r_t$ and the discounted value of being in the next state $s_{t+1}$:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    \\begin{split}\n",
    "        V(s_t) &= \\mathbb{E} \\left[ G_t \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E} \\left[ r_{t} + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E}\\left[r_{t} + \\gamma (r_{t+1} + \\gamma r_{t+2} + \\dots) \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E}\\left[ r_{t} + \\gamma G_{t+1} \\big\\vert s_t \\right] \\\\\n",
    "                &= \\mathbb{E} \\left[ r_{t} + \\gamma V(s_{t+1}) \\big\\vert s_t \\right] \\\\\n",
    "    \\end{split}\n",
    "\\end{align}\n",
    "\n",
    "This last form of $V(s_t)$ is known as the **Bellman Equation**.\n",
    "\n",
    "***\n",
    "# TD(0)\n",
    "\n",
    "Say we are interesting in learning to predict $V(s_t)$. We can use a simple method called **TD(0)** to approximate $V(s_t)$ when our observation space $\\mathcal{S}$ is **discrete** (i.e., finite). We begin by initializing a vector $V$ with zeros, which will serve as our initial predictions for what $V(s_t)$ is. Technically speaking, it can be seeded with random values and is guaranteed to converge to the right prediction given infinite data, but guessing all zeros is unbiased in terms of sign.\n",
    "\n",
    "According to the Bellman Equation, we can use \n",
    "\n",
    "$$\n",
    "r_t + \\gamma V(s_{t+1})\n",
    "$$\n",
    "\n",
    "(which we call the **TD-target**) as an unbiased estimator for $V(s_t)$. This is because we have this form of the equation:\n",
    "\n",
    "$$\n",
    "V(s_t) = \\mathbb{E} \\left[ r_{t} + \\gamma V(s_{t+1}) \\big\\vert s_t \\right]\n",
    "$$\n",
    "\n",
    "Then we can modify our estimate of $V(s_t)$ to more closely match $r_t + \\gamma V(s_{t+1})$ according to a learning rate $\\alpha$ as follows:\n",
    "\n",
    "$$\n",
    "V(s_t) \\gets V(s_t) + \\alpha \\left( r_t + \\gamma V(s_{t+1}) - V(s_t) \\right)\n",
    "$$\n",
    "\n",
    "where $r_t + \\gamma V(s_{t+1}) - V(s_t)$ is the difference between our current prediction $V(s_t)$ and the slightly better estimate: $r_t + V(s_{t+1})$. By adding this difference to our current prediction, scaled by some learning factor $\\alpha$, our estimate $V(s_t)$ slowly converges to the correct value.\n",
    "\n",
    "Let's use the TD(0) algorithm to attempt to learn the value function of this Markov reward process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_0(P, R, T=100, s_0=0, gamma=0.99, alpha=0.1):\n",
    "    num_states = len(P)\n",
    "    V = np.zeros(num_states)\n",
    "    s_t = s_0\n",
    "    r_t = R[s_t]\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        s_t_next = np.random.choice(np.arange(num_states), p=P[s_t])\n",
    "        V[s_t] = V[s_t] + alpha*(r_t + gamma*V[s_t_next] - V[s_t])\n",
    "        s_t = s_t_next\n",
    "        r_t = R[s_t]\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: \n",
      "[189.26912044 188.4107677  194.03474576 181.27714373]\n"
     ]
    }
   ],
   "source": [
    "V = TD_0(P, R, 10000, 0, 0.99, 0.1)\n",
    "print(f'V: \\n{V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Markov Decision Processes\n",
    "A **Markov Decision Process** (MDP) is an extension of a Markov Reward Process that allows state transitions to be conditional upon some action. Formally, it is a tuple $\\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R} \\rangle $ where $\\mathcal{A}$ is a set of actions available to an agent in a state $s_t$. Our reward is conditional now upon both the state $s_t$ we are in and the action $a_t$ that we took:\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(s_t, a_t) = \\mathbb{E} \\left[ r_t \\vert s_t, a_t \\right]\n",
    "$$\n",
    "\n",
    "Also, $\\mathcal{P}$ is the probability of transitioning to state $s_{t+1}$ given that the current state is $s_t$ and the current action is $a_t$:\n",
    "\n",
    "$$\n",
    "\\mathcal{P}(s_t, a_t, s_{t+1}) = \\mathbb{P} \\left[ s_{t+1} \\vert s_t, a_t \\right]\n",
    "$$\n",
    "\n",
    "Whereas in an MRP the probability of generating trajectories is dependant upon only the dynamics of the underlying Markov process, in an MDP trajectories also depend on the actions of an agent.\n",
    "\n",
    "This is an MDP with four states and two actions.\n",
    "\n",
    "![**missing image (MDP)**](images/mdp.png)\n",
    "\n",
    "In this case, $\\mathcal{P}$ is a **tensor** of shape $4 \\times 2 \\times 4$ where $\\mathcal{P}(i, j, k)$ is the probability of transitioning from state $i$ to state $k$ given action $j$. Furthermore, $\\mathcal{R}$ is a matrix of shape $4 \\times 2$ where $\\mathcal{R}(i, j)$ is the reward associated with choosing action $j$ in state $i$.\n",
    "\n",
    "***\n",
    "### Policies\n",
    "\n",
    "Our goal is to define a **policy** $\\pi$, which can be thought of as a set of rules for choosing actions based on the state. Typically, we represent $\\pi$ as a probability distribution:\n",
    "$$\n",
    "\\pi: \\mathcal{S} \\times \\mathcal{A} \\to \\left[ 0, 1 \\right]\n",
    "$$\n",
    "\n",
    "where we sample $a_t$ from the distribution\n",
    "$$\n",
    "a_t \\sim \\pi(\\cdot \\vert s_t)\n",
    "$$\n",
    "For discrete action spaces and observation spaces, we can think of this as a table of size $\\lvert \\mathcal{S} \\rvert \\times \\lvert \\mathcal{A} \\rvert$ where $\\pi_{i,j}$ is the probability of choosing action $j$ in state $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_state_transition_matrix(num_states, num_actions):\n",
    "    P = np.random.rand(num_states, num_actions, num_states)\n",
    "    for i in range(num_states):\n",
    "        for j in range(num_actions):\n",
    "            P[i, j] /= sum(P[i, j])\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reward_matrix(num_states, num_actions, mu=0, sigma=1):\n",
    "    return mu + np.random.randn(num_states, num_actions)*sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_policy(num_states, num_actions):\n",
    "    pi = np.random.rand(num_states, num_actions)\n",
    "    for i in range(num_states):\n",
    "        pi[i] /= sum(pi[i])\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: \n",
      "[[[0.17042536 0.34201973 0.10786943 0.37968548]\n",
      "  [0.16186722 0.38207381 0.16551557 0.29054341]]\n",
      "\n",
      " [[0.09679337 0.42781783 0.11451198 0.36087682]\n",
      "  [0.22995667 0.34899076 0.35566587 0.0653867 ]]\n",
      "\n",
      " [[0.4094171  0.07837153 0.28083541 0.23137597]\n",
      "  [0.14460938 0.19997512 0.29981354 0.35560195]]\n",
      "\n",
      " [[0.07871477 0.24551736 0.38380149 0.29196638]\n",
      "  [0.13551819 0.47594282 0.19870079 0.18983819]]] \n",
      "R: \n",
      "[[ 0.06807389  1.06863631]\n",
      " [-0.1527366  -0.64913176]\n",
      " [ 1.10558323  0.02134114]\n",
      " [ 2.00332665 -0.63635805]] \n",
      "pi: \n",
      "[[0.83715368 0.16284632]\n",
      " [0.8570525  0.1429475 ]\n",
      " [0.18023624 0.81976376]\n",
      " [0.05224557 0.94775443]]\n"
     ]
    }
   ],
   "source": [
    "num_states = 4\n",
    "num_actions = 2 \n",
    "\n",
    "P = generate_state_transition_matrix(num_states, num_actions)\n",
    "R = generate_reward_matrix(num_states, num_actions)\n",
    "pi = generate_policy(num_states, num_actions)\n",
    "print(f'P: \\n{P} \\nR: \\n{R} \\npi: \\n{pi}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(P, R, pi, T=100, s_0 = 0):\n",
    "    num_states = len(P)\n",
    "    num_actions = len(P[0])\n",
    "    \n",
    "    s_t = s_0\n",
    "    tau = np.zeros(T, dtype=np.int32)\n",
    "    tau[0] = s_t\n",
    "    \n",
    "    a_t = np.random.choice(np.arange(num_actions), p=pi[s_t])\n",
    "    actions = np.zeros(T, dtype=np.int32)\n",
    "    actions[0] = a_t \n",
    "    \n",
    "    rewards = np.zeros(T)\n",
    "    rewards[0] = R[s_t, a_t]\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        s_t = np.random.choice(np.arange(num_states), p=P[s_t, a_t])\n",
    "        a_t = np.random.choice(np.arange(num_actions), p=pi[s_t])\n",
    "        tau[t] = s_t\n",
    "        actions[t] = a_t\n",
    "        rewards[t] = R[s_t, a_t]\n",
    "\n",
    "    return tau, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau: \n",
      "[0 0 2 0 2 1 3 1 1 3 1 1 3 0 3 1 3 0 1 1 3 1 3 1 3 2 3 2 2 1 1 2 2 3 3 3 2\n",
      " 0 1 0 0 3 1 1 3 1 3 1 0 1 1 3 1 3 1 3 3 1 2 3 2 3 1 1 0 0 3 2 3 3 1 3 1 1\n",
      " 3 2 2 3 1 3 1 1 2 3 1 1 1 3 3 3 0 0 1 1 2 2 3 2 3 1] \n",
      "actions: \n",
      "[0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1\n",
      " 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0\n",
      " 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0] \n",
      "rewards: \n",
      "[ 0.06807389  0.06807389  1.10558323  0.06807389  0.02134114 -0.1527366\n",
      " -0.63635805 -0.1527366  -0.1527366  -0.63635805 -0.1527366  -0.1527366\n",
      " -0.63635805  0.06807389 -0.63635805 -0.1527366  -0.63635805  0.06807389\n",
      " -0.1527366  -0.1527366  -0.63635805 -0.1527366  -0.63635805 -0.1527366\n",
      " -0.63635805  0.02134114 -0.63635805  0.02134114  0.02134114 -0.1527366\n",
      " -0.1527366   0.02134114  0.02134114 -0.63635805 -0.63635805 -0.63635805\n",
      "  0.02134114  0.06807389 -0.1527366   0.06807389  0.06807389 -0.63635805\n",
      " -0.64913176 -0.1527366  -0.63635805 -0.1527366  -0.63635805 -0.64913176\n",
      "  0.06807389 -0.1527366  -0.1527366  -0.63635805 -0.1527366   2.00332665\n",
      " -0.1527366  -0.63635805 -0.63635805 -0.1527366   0.02134114 -0.63635805\n",
      "  0.02134114 -0.63635805 -0.1527366  -0.64913176  0.06807389  0.06807389\n",
      "  2.00332665  0.02134114 -0.63635805 -0.63635805 -0.1527366  -0.63635805\n",
      " -0.1527366  -0.1527366  -0.63635805  0.02134114  0.02134114 -0.63635805\n",
      " -0.1527366  -0.63635805 -0.64913176 -0.1527366   0.02134114 -0.63635805\n",
      " -0.1527366  -0.1527366  -0.1527366  -0.63635805 -0.63635805 -0.63635805\n",
      "  1.06863631  1.06863631 -0.1527366  -0.1527366   1.10558323  0.02134114\n",
      " -0.63635805  0.02134114 -0.63635805 -0.1527366 ]\n"
     ]
    }
   ],
   "source": [
    "tau, actions, rewards = generate_trajectory(P, R, pi)\n",
    "print(f'tau: \\n{tau} \\nactions: \\n{actions} \\nrewards: \\n{rewards}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1. Markov Decision Processes are mathematical models of the world that consider how the world changes based on underlying state dynamics and decisions of agents. It also measures the *goodness* of certain states by associating with each state a reward.\n",
    "2. We can use the TD(0) algorithm to learn the value function $V$ for discrete state spaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
