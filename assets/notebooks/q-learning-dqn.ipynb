{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep $Q$ networks (DQN)\n",
    "\n",
    "In previous notebooks, we have seen how we can use `tensorflow` and autodifferentiation to do tabular $Q$-learning in the context of a regression problem. While this technique is powerful for environments with small (finite) observation spaces $\\mathcal{S}$ and action spaces $\\mathcal{O}$, we run into problems when our observation space is continuous (or even just large!).\n",
    "\n",
    "Tabular $Q$-learning is only guaranteed to converge if all state-action pairs are visited infinitely many times. In practice, this generally just means a very large number to get a reasonable approximation of the $Q$-function. However, when the observation space becomes large (such as using image inputs), it is likely that we only encounter each state-action pair at most once. Thus, $Q$-learning is not guaranteed to converge.\n",
    "\n",
    "Instead, we want a technique that can estimate $Q$-values such that similar states produce similar outputs. This would allow us to learn from some state-action pairs, and then generalize to other unseen state-action pairs. By using a **differentiable function approximator**, we get this kind of behaviour. Recall that $Q$-learning is a *regression* problem, meaning any kind of regression model could work - even a linear regression. However, the most popular model used by *deep* reinforcement learning researchers is the *deep* neural network.\n",
    "\n",
    "If you are familiar with supervised learning in deep learning, you may be familiar with techniques like dropout, batch normalization, and activity regularization. So far, these kinds of techniques do not prove extremely useful in the context of reinforcement learning. Instead, fully-connected neural networks with a small number of hidden units consisting of rectified linear units tend to perform best. ([Pieter Abbeel](https://www.youtube.com/watch?v=l-mYLq6eZPY) notes that on simple problems, linear feedback control can perform well even in complex environments. Fully-connected neural networks that use ReLU activations function as multi-step piecewise linear feedback controllers, hence their success).\n",
    "\n",
    "When using neural networks, rather than passing many state-action pairs to the network and predicting a scalar $Q(s_t, a_t)$, we pass only the state $s_t$ and produce a vectorized output $\\vec{Q}(s_t)$ where each entry in the vector corresponds to the predicted $Q$-value for each action available to the agent. Note that this necessitates that $\\mathcal{A}$ is finite (and generally small), a limitation of $DQN$ that we will overcome later in the section on policy gradients.\n",
    "\n",
    "![image of deep-q-network mapping single state to multiple outputs](../images/q-network.png)\n",
    "\n",
    "In this notebook, we make use of `tensorflow`'s `keras` API to build neural networks. We also take advantage of **batched environments** to accelerate data collection. The `keras` api build neural networks that process inputs in **batches**. This means that if our observation space for a single environment has a shape $84 \\times 84 \\times 3$, then the network expects inputs of shape $B \\times 84 \\times 84 \\times 3$ where $B$ is the number of inputs in the batch.\n",
    "\n",
    "When running the tabular $Q$-learning agent in `tensorflow` in the previous notebook, runtime was considerably slower than the simply numpy-based agent. The computation time spent evaluating and updating the policy dominated the time to perform a single step/update of the agent, compared to the time spent simulating a step in the environment. Ideally, the time spent should be 50% policy evaluation and 50% environment stepping. By using batched environments, we can even this out. Furthermore, this means that we get more data per wall-clock-time, which will accelerate learning. This will be different from most tutorials which use `keras`, where a single environment is used, and inputs are manipulated to trick keras into treating them like a batch.\n",
    "\n",
    "Increasing the number of environments can stabilize training by diversifying the collected data over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size=1000000):\n",
    "        self.memory = deque(maxlen=size)\n",
    "        \n",
    "    def remember(self, s_t, a_t, r_t, s_t_next, d_t):\n",
    "        self.memory.append((s_t, a_t, r_t, s_t_next, d_t))\n",
    "        \n",
    "    def sample(self, num=32):\n",
    "        num = min(num, len(self.memory))\n",
    "        return random.sample(self.memory, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_shape, num_actions, num_envs, alpha=0.001, gamma=0.95, epsilon_i=1.0, epsilon_f=0.01, n_epsilon=0.1, hidden_sizes = []):\n",
    "        self.epsilon_i = epsilon_i\n",
    "        self.epsilon_f = epsilon_f\n",
    "        self.n_epsilon = n_epsilon\n",
    "        self.epsilon = epsilon_i\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.num_envs = num_envs\n",
    "\n",
    "        self.Q = Sequential()\n",
    "        for size in hidden_sizes:\n",
    "            self.Q.add(Dense(size, activation='relu', use_bias='false', kernel_initializer='he_uniform', dtype='float64'))\n",
    "        self.Q.add(Dense(self.num_actions, activation=\"linear\", use_bias='false', kernel_initializer='zeros', dtype='float64'))\n",
    "#         self.optimizer = tf.keras.optimizers.SGD(alpha)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(alpha)        \n",
    "\n",
    "    def act(self, s_t):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions, size=self.num_envs)\n",
    "        return np.argmax(self.Q(s_t), axis=1)\n",
    "    \n",
    "    def decay_epsilon(self, n):\n",
    "        self.epsilon = max(\n",
    "            self.epsilon_f, \n",
    "            self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f))\n",
    "\n",
    "    def update(self, s_t, a_t, r_t, s_t_next, d_t):\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_next = tf.stop_gradient(tf.reduce_max(self.Q(s_t_next), axis=1))\n",
    "            Q_pred = tf.reduce_sum(self.Q(s_t)*tf.one_hot(a_t, self.num_actions, dtype=tf.float64), axis=1)\n",
    "            loss = tf.reduce_mean(0.5*(r_t + (1-d_t)*self.gamma*Q_next - Q_pred)**2)\n",
    "        grads = tape.gradient(loss, self.Q.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteToBoxWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete), \\\n",
    "            \"Should only be used to wrap Discrete envs.\"\n",
    "        self.n = self.observation_space.n\n",
    "        self.observation_space = gym.spaces.Box(0, 1, (self.n,))\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        new_obs = np.zeros(self.n)\n",
    "        new_obs[obs] = 1\n",
    "        return new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, make_env, num_envs=1):\n",
    "        super().__init__(make_env())\n",
    "        self.num_envs = num_envs\n",
    "        self.envs = [make_env() for env_index in range(num_envs)]\n",
    "    \n",
    "    def reset(self):\n",
    "        return np.asarray([env.reset() for env in self.envs])\n",
    "    \n",
    "    def reset_at(self, env_index):\n",
    "        return self.envs[env_index].reset()\n",
    "    \n",
    "    def step(self, actions):\n",
    "        next_states, rewards, dones, infos = [], [], [], []\n",
    "        for env, action in zip(self.envs, actions):\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            infos.append(info)\n",
    "        return np.asarray(next_states), np.asarray(rewards), \\\n",
    "            np.asarray(dones), np.asarray(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, window=100):\n",
    "    sns.lineplot(\n",
    "        data=data.rolling(window=window).mean()[window-1::window]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name, T=20000, num_envs=32, batch_size=32, hidden_sizes=[24, 24], alpha=0.001, gamma=0.95):\n",
    "    env = VectorizedEnvWrapper(lambda: gym.make(env_name), num_envs)\n",
    "    state_shape = env.observation_space.shape\n",
    "    num_actions = env.action_space.n\n",
    "    agent = Agent(state_shape, num_actions, num_envs, alpha=alpha, hidden_sizes=hidden_sizes, gamma=gamma)\n",
    "    rewards = []\n",
    "    buffer = ReplayBuffer()\n",
    "    episode_rewards = 0\n",
    "    s_t = env.reset()\n",
    "    for t in range(T):\n",
    "        a_t = agent.act(s_t)\n",
    "        s_t_next, r_t, d_t, info = env.step(a_t)\n",
    "        buffer.remember(s_t, a_t, r_t, s_t_next, d_t)\n",
    "        s_t = s_t_next\n",
    "        for batch in buffer.sample(batch_size):\n",
    "            agent.update(*batch)\n",
    "        agent.decay_epsilon(t/T)\n",
    "        episode_rewards += r_t\n",
    "\n",
    "        for i in range(env.num_envs):\n",
    "            if d_t[i]:\n",
    "                rewards.append(episode_rewards[i])\n",
    "                episode_rewards[i] = 0\n",
    "                s_t[i] = env.reset_at(i)\n",
    "            \n",
    "    plot(pd.DataFrame(rewards), window=10)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0612 19:29:48.104817 4657370560 deprecation.py:323] From /anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "train(\"CartPole-v0\", T=20000, num_envs=32, batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
