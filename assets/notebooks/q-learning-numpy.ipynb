{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Q-learning\n",
    "\n",
    "***\n",
    "### Value Function\n",
    "Recall the definition of the **value function** $V(s_t)$:\n",
    "\n",
    "$$\n",
    "V(s_t) = \\mathbb{E} \\left[ G_t \\vert s_t \\right]\n",
    "$$\n",
    "\n",
    "In an MDP, our expected discounted return $G_t$ depends on the trajectories $\\tau$ that we generate. Whereas in a Markov process or Markov reward process this depends only on the underlying state dynamics, in an MDP, trajectories also depend on the actions that we choose. Since these actions are determined by our **policy** $\\pi$, we write the following:\n",
    "\n",
    "$$\n",
    "V^\\pi (s_t) = \\mathbb{E}_\\pi \\left[ G_t \\vert s_t \\right]\n",
    "$$\n",
    "\n",
    "that is, given that we are currently in state $s_t$, what value should we expect our discounted return $G_t$ to be, given that we generate trajectories according to our policy $\\pi$?\n",
    "\n",
    "***\n",
    "### Action-Value function\n",
    "We can define an analogous definition for the **action-value function**, which extends the notion of the value function to account for choosing a *specific action* $a_t$ in a state $s_t$, rather than just choosing the one suggested by the policy:\n",
    "\n",
    "$$\n",
    "Q^\\pi (s_t, a_t) = \\mathbb{E}_\\pi \\left[ G_t \\vert s_t, a_t \\right]\n",
    "$$\n",
    "\n",
    "that is, given that we are currently in state $s_t$, taking action $a_t$, what value should we expect our discounted return $G_t$ to be? We can see the relationship between $V^\\pi (s_t)$ and $Q^\\pi (s_t, a_t)$:\n",
    "\n",
    "$$\n",
    "V^\\pi (s_t) = \\mathbb{E}_{a_t \\sim \\pi} \\left[ Q^\\pi (s_t, a_t) \\vert s_t \\right]\n",
    "$$\n",
    "\n",
    "where $a_t \\sim \\pi$ means 'over actions $a_t$ selected according to a probability distribution $\\pi$'.\n",
    "\n",
    "We also have a Bellman equation for the state-value function:\n",
    "$$\n",
    "Q^\\pi (s_t, a_t) = \\mathbb{E}_\\pi \\left[ r_t + \\gamma Q^\\pi (s_{t+1}, a_{t+1}) \\vert s_t, a_t \\right]\n",
    "$$\n",
    "\n",
    "***\n",
    "### $Q$-learning\n",
    "$Q$-learning is an algorithm analogous to the TD(0) algorithm we've described before. In TD(0), we have a table $V$ containing predictions for $V^\\pi (s_t)$ for each state $s_t$, updating our predictions as follows:\n",
    "\n",
    "$$\n",
    "V (s_t) \\gets V (s_t) + \\alpha \\left( r_t + \\gamma V (s_{t+1}) - V (s_t) \\right)\n",
    "$$\n",
    "\n",
    "In $Q$-learning, we have a similar learning rule, except that our table now contains values for any state-action pair $(s_t, a_t)$.\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha \\left( r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "The advantage of $Q$-learning is that we have an estimate, for each action, of the expected discounted return we will get from taking that action. Recall the reward hypothesis:\n",
    "\n",
    "> Every action of a rational agent can be thought of as seeking to maximize some cumulative scalar reward signal.\n",
    "\n",
    "Consider being in some state $s_t$, and for each action $a_t$ we compute $Q(s_t, a_t)$. These values are our guesses for the discounted return we will get as a result of choosing each action. If we choose the action that maximizes $Q(s_t, a_t)$, then our agent is maximizing the discounted return. This defines a **greedy policy**\n",
    "\n",
    "$$\n",
    "\\pi(a_t \\vert s_t) = \\begin{cases} 1, & a_t = \\arg \\max_{a_t} Q(s_t, a_t) \\\\ 0, & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "Knowing this, we can estimate the $Q$ value for the next state-action pair $(s_{t+1}, a_{t+1})$ by assuming the agent chooses the action that maximizes that $Q$ value. This gives us the following update rule:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha \\left( r_t + \\gamma \\max_{a_{t+1}} \\left( Q(s_{t+1}, a_{t+1}) \\right) - Q(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "***\n",
    "### Modified Bellman Equation and Terminal States\n",
    "Recall that some environments are **episodic**. This means that there must be some states $s_T$ where, regardless of what action is chosen, the next state is always $s_T$ and the reward is always $0$. The `gym` library defines an environment api with a `step` function that returns $s_{t+1}, r_t, d_t,$ info given an action $a_t$. Here, $d_t$ is a boolean flag denoting whether or not $s_{t+1}$ is terminal.\n",
    "\n",
    "By the definition of $Q(s_t, a_t)$, we can see that $r_t + \\gamma Q(s_{t+1}, a_{t+1})$ is an unbiased estimate for $Q(s_t, a_t)$. This is what gives us the Bellman equation update rule. If we know that a state is terminal, then we have an even better estimate: if $s_t$ is terminal, then $G_t = r_t$, and our update rule is just\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha \\left( r_t - Q(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "If we interpret the truthiness of $d_t$ as an integer (i.e., $d_t = 1$ if `True` and $0$ if `False`), then we can define a modified update rule that takes terminal states into account:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha \\left( r_t + (1-d_t)\\gamma \\max_{a_{t+1}} \\left( Q(s_{t+1}, a_{t+1}) \\right) - Q(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "***\n",
    "### Exploration-Exploitation\n",
    "\n",
    "Our agent can quickly become stuck in a suboptimal policy by always choosing the same action in the same state. Recall that we use the greedy policy to choose actions. If we choose a certain action in a certain state, it's because that action has the maximal $Q$ value associated with it. If we always follow the greedy policy, we will generally tend to continue to choose this action since we don't get the opportunity to explore (and thus update our predictions for) the other actions. This problem is known as the **exploration-exploitation tradeoff**: should we try to learn more about the $Q$ function by exploring (and thus finding a better policy) or should we exploit what we know about the environment to try to maximize the returns that we know about?\n",
    "\n",
    "### $\\epsilon$-greedy policies\n",
    "\n",
    "The simplest approach to solving the exploration-exploitation tradeoff is to randomly choose an action $a_t \\sim \\mathcal{A}$ rather than from the policy $a_t \\sim \\pi$. We choose a random action with a probability $\\epsilon$. At the beginning of training, the agent should not follow its policy, since its estimates for $Q(s_t, a_t)$ are poor. Instead, the agent should focus on gathering as much data as possible to improve its guesses, and should do this by exploring. This means that initially, $\\epsilon$ should be high (close to 1). Over time, as our predictions for $Q(s_t, a_t)$ improve, we can trust our prediction and thus our greedy policy more, and thus $\\epsilon$ should be low (close to 0).\n",
    "\n",
    "We define $\\epsilon_i$ to be the initial value for $\\epsilon$ and $\\epsilon_f$ to be the final value. We decay $\\epsilon$ using either linear or exponential decay. Linear decay means we subtract a constant from $\\epsilon$ at every time step, and exponential decay means we multiply $\\epsilon$ by some decay factor less than 1. Here we use linear decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, \n",
    "                 epsilon_i=1.0, \n",
    "                 epsilon_f=0.0, \n",
    "                 n_epsilon=0.1, \n",
    "                 alpha=0.5, \n",
    "                 gamma = 0.95):\n",
    "        \n",
    "        \"\"\"\n",
    "        num_states: the number of states in the discrete state space\n",
    "        num_actions: the number of actions in the discrete action space\n",
    "        epsilon_i: the initial value for epsilon\n",
    "        epsilon_f: the final value for epsilon\n",
    "        n_epsilon: a float between 0 and 1 determining at which point\n",
    "        during training epsilon should have decayed from epsilon_i to\n",
    "        epsilon_f\n",
    "        alpha: the learning rate\n",
    "        gamma: the decay rate\n",
    "        \"\"\"\n",
    "        \n",
    "        self.epsilon_i = epsilon_i\n",
    "        self.epsilon_f = epsilon_f\n",
    "        self.epsilon = epsilon_i\n",
    "        self.n_epsilon = n_epsilon\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "    def decay_epsilon(self, n):\n",
    "        \"\"\"\n",
    "        Decays the agent's exploration rate according to n, which is a\n",
    "        float between 0 and 1 describing how far along training is, \n",
    "        with 0 meaning 'just started' and 1 meaning 'done'.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(\n",
    "            self.epsilon_f, \n",
    "            self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f))\n",
    "    \n",
    "    def act(self, s_t):\n",
    "        \"\"\"\n",
    "        Epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        return np.argmax(self.Q[s_t])\n",
    "    \n",
    "    def update(self, s_t, a_t, r_t, s_t_next, d_t):\n",
    "        \"\"\"\n",
    "        Uses the q-learning update rule to update the agent's predictions\n",
    "        for Q(s_t, a_t).\n",
    "        \"\"\"\n",
    "        Q_next = np.max(self.Q[s_t_next])\n",
    "        self.Q[s_t, a_t] = self.Q[s_t, a_t] + \\\n",
    "        self.alpha*(r_t + (1-d_t)*self.gamma*Q_next - \\\n",
    "        self.Q[s_t, a_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, window=100):\n",
    "    \"\"\"\n",
    "    Given a pandas dataframe 'data', plots a rolling mean of the data\n",
    "    using a window size of 'window'.\n",
    "    \"\"\"\n",
    "    sns.lineplot(\n",
    "        data=data.rolling(window=window).mean()[window-1::window]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name,\n",
    "         T=100000, \n",
    "          alpha=0.8, \n",
    "          gamma=0.95, \n",
    "          epsilon_i = 1.0, \n",
    "          epsilon_f = 0.0, \n",
    "          n_epsilon = 0.1):\n",
    "    env = gym.make(env_name)\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    agent = Agent(num_states, num_actions, \n",
    "                  alpha=alpha, \n",
    "                  gamma=gamma, \n",
    "                  epsilon_i=epsilon_i, \n",
    "                  epsilon_f=epsilon_f, \n",
    "                  n_epsilon = n_epsilon)\n",
    "\n",
    "    rewards = []\n",
    "    episode_rewards = 0\n",
    "    \n",
    "    s_t = env.reset()\n",
    "    \n",
    "    for t in range(T):\n",
    "        a_t = agent.act(s_t)\n",
    "        s_t_next, r_t, d_t, info = env.step(a_t)\n",
    "        agent.update(s_t, a_t, r_t, s_t_next, d_t)\n",
    "        agent.decay_epsilon(t/T)\n",
    "        s_t = s_t_next\n",
    "        episode_rewards += r_t\n",
    "        \n",
    "        if d_t:\n",
    "            rewards.append(episode_rewards)\n",
    "            episode_rewards = 0\n",
    "            s_t = env.reset()\n",
    "            \n",
    "    plot(pd.DataFrame(rewards))\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Agent at 0x1a18692320>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(\"FrozenLake-v0\", T=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
